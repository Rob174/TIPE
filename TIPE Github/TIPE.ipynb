{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZyU4JIfpjyY"
   },
   "source": [
    "# TIPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Autoencodeurs - Apprentissage non supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par charger les noms des fichiers en mémoire : <br>\n",
    "On sépare en 3 sous-lots de paires image label<br>\n",
    "* Lot d'entrainement\n",
    "* Lot de test\n",
    "* Lot de validation\n",
    "<br>\n",
    "\n",
    "\n",
    "Pour économiser la mémoire du pc la fonction retournant le batch d'entrainement chargera effectivement les images en mémoire et libèrera la place mémoire précédement occupée (cf cellule plus bas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Vérification GPU\")\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print()\n",
    "print()\n",
    "from google.colab import files\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "from PIL import Image\n",
    "\n",
    "# Ajoute images\n",
    "entrainement = 60/100\n",
    "test = 20/100\n",
    "# validation = le reste : 20%\n",
    "tailleImage = 399\n",
    "\n",
    "images = []\n",
    "noise = []\n",
    "for file in uploaded.keys():\n",
    "    if file.endswith(\".jpg\"):\n",
    "      if file[0] == \"c\":\n",
    "        images.append(file)\n",
    "      else:\n",
    "        noise.append(file)\n",
    "indice = np.arange(len(images))\n",
    "np.random.shuffle(indice)\n",
    "train_dataset_label = []\n",
    "train_dataset_item = []\n",
    "test_dataset_label = []\n",
    "test_dataset_item = []\n",
    "validation_dataset_label = []\n",
    "validation_dataset_item = []\n",
    "print(len(images),\" images disponibles\")\n",
    "print(\"On partage de 0 à \",str(int(entrainement*len(images))),\" pour l'entrainement\")\n",
    "print(\"On partage de \",str(int(entrainement*len(images))+1),\" à \",str(int((1-test)*len(images))),\" pour la validation\")\n",
    "print(\"On partage de \",int((1-test)*len(images)+1),\" à \",str(len(images)),\" pour le test\")\n",
    "\n",
    "afficherIndices = False\n",
    "afficheIndiceImage = False #Affiche indice des images mélangées, sinon l'indice i \n",
    "for i in range(len(images)):\n",
    "    if i <= int(entrainement*len(images)):\n",
    "        train_dataset_label.append(np.array(Image.open(images[indice[i]]),np.uint8))\n",
    "        train_dataset_item.append(np.array(Image.open(noise[indice[i]]),np.uint8))\n",
    "        if afficherIndices == True:\n",
    "            print(\"Indice \",str(indice[i]),\" associé à train\") if afficheIndiceImage == True else print(\"Indice \",str(i),\" associé à train\")\n",
    "        \n",
    "    elif i > int((entrainement+test)*len(images)):\n",
    "        i_test = i - int((entrainement+test)*len(images))\n",
    "        test_dataset_label.append(np.array(Image.open(images[indice[i]]),np.uint8))\n",
    "        test_dataset_item.append(np.array(Image.open(noise[indice[i]]),np.uint8))\n",
    "        if afficherIndices == True:\n",
    "            print(\"Indice \",str(indice[i]),\" associé à test\") if afficheIndiceImage == True else print(\"Indice \",str(i),\" associé à test\")\n",
    "        \n",
    "    else:\n",
    "        i_validation = i - int((entrainement)*len(images))\n",
    "        validation_dataset_label.append(np.array(Image.open(images[indice[i]]),np.uint8))\n",
    "        validation_dataset_item.append(np.array(Image.open(noise[indice[i]]),np.uint8))\n",
    "        if afficherIndices == True:\n",
    "            print(\"Indice \",str(indice[i]),\" associé à validation\") if afficheIndiceImage == True else print(\"Indice \",str(i),\" associé à validation\")\n",
    "        \n",
    "print(\"Data stored in memory !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérification GPU\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7094546054700314070\n",
      "]\n",
      "\n",
      "\n",
      "489  images disponibles\n",
      "On partage de 0 à  293  pour l'entrainement\n",
      "On partage de  294  à  391  pour la validation\n",
      "On partage de  392  à  489  pour le test\n",
      "Path stored in memory !\n"
     ]
    }
   ],
   "source": [
    "print(\"Vérification GPU\")\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print()\n",
    "print()\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "from PIL import Image\n",
    "\n",
    "os.chdir(\"D:/Photos\")\n",
    "# Ajoute images\n",
    "entrainement = 60/100\n",
    "test = 20/100\n",
    "# validation = le reste : 20%\n",
    "tailleImage = 399\n",
    "\n",
    "images = []\n",
    "noise = []\n",
    "\n",
    "i = 1\n",
    "while os.path.isfile(\"clean/\"+str(i)+\".jpg\"):\n",
    "    images.append(\"clean/\"+str(i)+\".jpg\")\n",
    "    i += 1\n",
    "\n",
    "i = 1\n",
    "while os.path.isfile(\"noise/\"+str(i)+\".jpg\"):\n",
    "    noise.append(\"noise/\"+str(i)+\".jpg\")\n",
    "    i += 1\n",
    "    \n",
    "indice = np.arange(len(images))\n",
    "np.random.shuffle(indice)\n",
    "\n",
    "train_dataset_label = [images[i] for i in indice[:int(len(indice)*entrainement)+1]]\n",
    "train_dataset_item = [noise[i] for i in indice[:int(len(indice)*entrainement)+1]]\n",
    "\n",
    "test_dataset_label = [images[i] for i in indice[int(len(indice)*entrainement):int(len(indice)*entrainement)+int(len(indice)*test)+1]]\n",
    "test_dataset_item = [noise[i] for i in indice[int(len(indice)*entrainement):int(len(indice)*entrainement)+int(len(indice)*test)+1]]\n",
    "\n",
    "validation_dataset_label = [images[i] for i in indice[int(len(indice)*entrainement)+int(len(indice)*test):]]\n",
    "validation_dataset_item = [noise[i] for i in indice[int(len(indice)*entrainement)+int(len(indice)*test):]]\n",
    "\n",
    "print(len(images),\" images disponibles\")\n",
    "print(\"On partage de 0 à \",str(int(entrainement*len(images))),\" pour l'entrainement\")\n",
    "print(\"On partage de \",str(int(entrainement*len(images))+1),\" à \",str(int((1-test)*len(images))),\" pour la validation\")\n",
    "print(\"On partage de \",int((1-test)*len(images)+1),\" à \",str(len(images)),\" pour le test\")\n",
    "print(\"Path stored in memory !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    print(\"Size of {}\".format(len(data)))\n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    data_shuffle = [np.array(Image.open(data[i]),np.uint8) for i in idx]\n",
    "    labels_shuffle = [np.array(Image.open(labels[i]),np.uint8) for i in idx]\n",
    "    print()\n",
    "\n",
    "    return np.array(data_shuffle), np.array(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis la construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "n_batches_per_epoch = 3\n",
    "batch_size = 7\n",
    "\n",
    "height = 399\n",
    "width = 399\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "Size of 294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "\n",
    "    conv1 = tf.layers.conv2d(X, filters=5, kernel_size=15,\n",
    "                         strides=1, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=10, kernel_size=15,\n",
    "                         strides=1, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "    conv3 = tf.layers.conv2d(conv2, filters=7, kernel_size=15,\n",
    "                         strides=1, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv3\")\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=7, kernel_size=5,\n",
    "                         strides=1, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv4\")\n",
    "    output = tf.layers.conv2d(conv4, filters=3, kernel_size=2,\n",
    "                         strides=1, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv7\")\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.square(output - y))\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    import os\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup avant modification architecture autoencodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "nom = 'Essai7T'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}{}/\".format(root_logdir,nom,now)\n",
    "\n",
    "noyaux = [20, 17, 15, 12, 10, 7, 5, 3, 5, 7, 10, 12, 15, 17, 20]\n",
    "filtre = [3, 5, 7, 10, 12, 15, 17, 20, 17, 15, 12, 10, 7, 5, 3]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "\n",
    "    conv1 = tf.layers.conv2d(X, filters=filtre[0], kernel_size=noyaux[0],\n",
    "                         strides=1, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=filtre[1], kernel_size=noyaux[1],\n",
    "                             strides=1, padding='SAME',\n",
    "                             activation=tf.nn.relu, name='conv2')\n",
    "    conv3 = tf.layers.conv2d(conv2, filters=filtre[2], kernel_size=noyaux[2],\n",
    "                             strides=1, padding='SAME',\n",
    "                             activation=tf.nn.relu, name='conv3')\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=filtre[3], kernel_size=noyaux[3],\n",
    "                             strides=1, padding='SAME',\n",
    "                             activation=tf.nn.relu, name='conv4')\n",
    "    conv5 = tf.layers.conv2d(conv4, filters=filtre[4], kernel_size=noyaux[4],\n",
    "                             strides=1, padding='SAME',\n",
    "                             activation=tf.nn.relu, name='conv5')\n",
    "    conv6 = tf.layers.conv2d(conv5, filters=filtre[5], kernel_size=noyaux[5],\n",
    "                             strides=1, padding='SAME',\n",
    "                             activation=tf.nn.relu, name='conv6')\n",
    "    conv7 = tf.layers.conv2d(conv6, filters=filtre[6], kernel_size=noyaux[6],\n",
    "                             strides=1, padding='SAME',\n",
    "                             activation=tf.nn.relu, name='conv7')\n",
    "    conv8 = tf.layers.conv2d(conv7, filters=filtre[7], kernel_size=noyaux[7],\n",
    "                             strides=1, padding='SAME',\n",
    "                             activation=tf.nn.relu, name='conv8')\n",
    "    conv9 = tf.layers.conv2d(conv8, filters=filtre[8], kernel_size=noyaux[8],\n",
    "                             strides=1, padding='SAME',\n",
    "                             activation=tf.nn.relu, name='conv9')\n",
    "    conv10 = tf.layers.conv2d(conv9, filters=filtre[9], kernel_size=noyaux[9],\n",
    "                              strides=1, padding='SAME',\n",
    "                              activation=tf.nn.relu, name='conv10')\n",
    "    conv11 = tf.layers.conv2d(conv10, filters=filtre[10], kernel_size=noyaux[10],\n",
    "                              strides=1, padding='SAME',\n",
    "                              activation=tf.nn.relu, name='conv11')\n",
    "    conv12 = tf.layers.conv2d(conv11, filters=filtre[11], kernel_size=noyaux[11],\n",
    "                              strides=1, padding='SAME',\n",
    "                              activation=tf.nn.relu, name='conv12')\n",
    "    conv13 = tf.layers.conv2d(conv12, filters=filtre[12], kernel_size=noyaux[12],\n",
    "                              strides=1, padding='SAME',\n",
    "                              activation=tf.nn.relu, name='conv13')\n",
    "    conv14 = tf.layers.conv2d(conv13, filters=filtre[13], kernel_size=noyaux[13],\n",
    "                              strides=1, padding='SAME',\n",
    "                              activation=tf.nn.relu, name='conv14')\n",
    "    output = tf.layers.conv2d(conv14, filters=filtre[14], kernel_size=noyaux[14],\n",
    "                              strides=1, padding='SAME',\n",
    "                              activation=tf.nn.relu, name='conv15')\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.square(output - X))\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "        loss_saver = tf.summary.scalar(\"Loss\",loss)\n",
    "        summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "    with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "        \n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op, feed_dict={X: y_batch, y: X_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver.eval(feed_dict={X: y_batch, y: X_batch})\n",
    "                  summary_writer.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train))\n",
    "        summary_writer.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      \n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratégie de réglage des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Implémenté| Essai | Fonctionnement | Nombre de couches | Types de couches | Nb de filtres | Taille du noyau | Choix | Résultats |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|Oui| 1| Autoencodeur sur image | 5 | Uniquement convolution | Aléatoire | Cst à 15 puis décroissant jusqu'à 2 | Nombre de batch pour avoir réglièrement la 'précision' ; Se base // image de référence |`tf.square(output - y)` descend jusqu'à 64 mais fluctue toujours à 2100 à epooque 99 |\n",
    "|Oui| 2| Autoencodeur sur image | 5 | Uniquement convolution | Croissant de 3 à 15 | Décroissant de 15 à 3 |Monotonie inverse filtre/noyau <b>Mais besoin même D -> Rupture à la fin </b>; Se base // image <b>bruitée</b> |`tf.square(output - X)` reste stable de l'epoch 36 à 93 à ~200 |\n",
    "|Oui| 3| Autoencodeur sur image | 5 | Uniquement convolution | Croissant de 3 à 31 | Décroissant de 15 à 3 |Donner une plus gde variation du nb de filtres |Arrêté au bout d'une 1/2 h sans dépasser epoch 0 ; après chargement progressif des images fonctionne, le + près|\n",
    "|Oui| 4 | Autoencodeur sur image | 5 | Uniquement convolution | Croissant de 3 à 31 | Décroissant de 15 à 3 |Prendre le meilleur modèle et le tester avec `tf.square(output - y)` | `tf.square(output - y)` Ne converge pas du tout|\n",
    "|Oui| 5 | Autoencodeur sur image | 15 | Uniquement convolution | Croissant de 3 à 20 | Décroissant de 20 à 3 |Réduire les noyaux de convo pr plus efficacité et augmenter le nb de couches| `tf.square(output - y)` Ne converge pas du tout|\n",
    "|Oui| 6 | Autoencodeur sur image | 15 | Uniquement convolution | Symétrique de 3 à 20 | Symétrique de 3 à 20 |Utiliser la symétrie préconisée ds livre et la tester avec `tf.square(output - y)` | `tf.square(output - y)` ne converge pas du tout (Pas arrivé jusqu'à la fin)|\n",
    "|Oui| 7 | Autoencodeur sur image | 5 | Uniquement convolution | Croissant de 3 à 31 | Décroissant de 15 à 3 |Prendre le meilleur modèle et le tester avec `tf.square(output - y)` | `tf.square(output - y)` Ne converge pas du tout|\n",
    "|Oui| 7T | Autoencodeur sur image | 5 | Uniquement convolution | Croissante de 3 à 15 |Décroissante de 15 à 3  |Couche de max_pooling|Erreur plus basse mais `tf.square(output - X)` avec cette choix les images non bruitées n'apprend pas réellement |\n",
    "|Oui| 8 | Autoencodeur sur image | 5 | Convolution et couche intégralement connectée au milieu | Symétrique de 3 à 20 | Symétrique de 3 à 20 |Couche de max_pooling + Intégralement connectée|Erreur plus basse mais `tf.square(output - X)` avec cette choix les images non bruitées n'apprend pas réellement |\n",
    "|Oui| 9 | DNN sur image | 5 | Couche intégralement connectée | - | - |-|-|-|-|-|Intégralement connectée|- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement de perspective : le modèle n'apprend pas puisque globalement les étoiles ne sont pas détectées. Le bruit est détecté. Il s'agirait alors d'extraire le bruit des images et de le soustraire à l'image bruitée.<br>\n",
    "Ou bien recentrer les images sur les galaxies bruitées/ débruitées pour :<br>\n",
    "<ul>\n",
    "    <li>Déterminer si on a un objet</li>\n",
    "    <li>Corriger en fonction</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Implémenté| Essai | Fonctionnement | Nombre de couches | Types de couches | Nb de filtres | Taille du noyau| Choix | Résultats |\n",
    "| --- | --- | ---| ---| ---| ---| ---| --- | --- |\n",
    "|Oui| 9 | Autoencodeur sur image | 5 | Couche intégralement connectée | - | - |Que couche dense pour reproduire image débruitée|Converge et même surajuste|\n",
    "|Oui| 10 | Autoencodeur sur image | 5 | Couche intégralement connectée | - | - |Couche dense pour reproduire à partir des images bruitées les images débruitées|Ne converge pas vraiment|\n",
    "|Oui| 11 | Autoencodeur sur image | 5 | Couche intégralement connectée | - | - |Même architecture mais en laissant apprendre plus longtemps|Toujours pas|\n",
    "|Oui| 12 | Autoencodeur sur image | 5 | Couche intégralement connectée | - | - |Plus de couches en revenant à 300 iterations (suffisait en essai 11|-|\n",
    "|Oui| 13 | Autoencodeur sur image | 5 | Uniquement convolution | Constant à 3 | - | Symétrique de 15 à 3 |Entrainement couche par couche | `tf.square(output - y)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Oui| 13 | Autoencodeur sur image | 5 | Uniquement convolution | Constant à 3 | Symétrique de 15 à 3 | Relu |7| 3|99|-|-|---| Entrainement couche par couche | `tf.square(output - y)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai sur les tailles de noyau de convolution sur 100 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Implémenté| Essai | Fonctionnement | Nb de filtres | Taille du noyau| Choix | Résultats |\n",
    "| --- | --- | ---| ---| ---| ---| ---|\n",
    "|Oui| C1 | Autoencodeur sur image | 3 | 2 |-|-|\n",
    "|Oui| C2 | Autoencodeur sur image | 3 | 4 |-|-|\n",
    "|Oui| C3 | Autoencodeur sur image | 3 | 6 |-|-|\n",
    "|Oui| C4 | Autoencodeur sur image | 3 | 8 |-|-|\n",
    "|Oui| C5 | Autoencodeur sur image | 3 | 10 |-|-|\n",
    "|Oui| C6 | Autoencodeur sur image | 3 | 12 |-|-|\n",
    "|Oui| C7 | Autoencodeur sur image | 3 | 14 |-|-|\n",
    "|Oui| C8 | Autoencodeur sur image | 3 | 16 |-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que plus le noyau est petit, plus la couche apprend. A l'inverse plus le noyau est grand, moins le système apprend. Néanmoins, les deux aboutissent à une même précision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En combinant les deux 'brutalement' (sans entrainement couche par couche) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Implémenté| Essai | Fonctionnement | Nombre de couches | Nb de filtres | Taille du noyau| Choix | Résultats |\n",
      "| --- | --- | ---| ---| ---| ---| --- | --- |\n",
      "|Oui| Cdeux0 | Autoencodeur sur image | 3 | 2, 2 |-|-|\n",
      "|Oui| Cdeux1 | Autoencodeur sur image | 3 | 2, 4 |-|-|\n",
      "|Oui| Cdeux2 | Autoencodeur sur image | 3 | 2, 6 |-|-|\n",
      "|Oui| Cdeux3 | Autoencodeur sur image | 3 | 2, 8 |-|-|\n",
      "|Oui| Cdeux4 | Autoencodeur sur image | 3 | 2, 10 |-|-|\n",
      "|Oui| Cdeux5 | Autoencodeur sur image | 3 | 2, 12 |-|-|\n",
      "|Oui| Cdeux6 | Autoencodeur sur image | 3 | 2, 14 |-|-|\n",
      "|Oui| Cdeux7 | Autoencodeur sur image | 3 | 2, 16 |-|-|\n",
      "|Oui| Cdeux8 | Autoencodeur sur image | 3 | 4, 2 |-|-|\n",
      "|Oui| Cdeux9 | Autoencodeur sur image | 3 | 4, 4 |-|-|\n",
      "|Oui| Cdeux10 | Autoencodeur sur image | 3 | 4, 6 |-|-|\n",
      "|Oui| Cdeux11 | Autoencodeur sur image | 3 | 4, 8 |-|-|\n",
      "|Oui| Cdeux12 | Autoencodeur sur image | 3 | 4, 10 |-|-|\n",
      "|Oui| Cdeux13 | Autoencodeur sur image | 3 | 4, 12 |-|-|\n",
      "|Oui| Cdeux14 | Autoencodeur sur image | 3 | 4, 14 |-|-|\n",
      "|Oui| Cdeux15 | Autoencodeur sur image | 3 | 4, 16 |-|-|\n",
      "|Oui| Cdeux16 | Autoencodeur sur image | 3 | 6, 2 |-|-|\n",
      "|Oui| Cdeux17 | Autoencodeur sur image | 3 | 6, 4 |-|-|\n",
      "|Oui| Cdeux18 | Autoencodeur sur image | 3 | 6, 6 |-|-|\n",
      "|Oui| Cdeux19 | Autoencodeur sur image | 3 | 6, 8 |-|-|\n",
      "|Oui| Cdeux20 | Autoencodeur sur image | 3 | 6, 10 |-|-|\n",
      "|Oui| Cdeux21 | Autoencodeur sur image | 3 | 6, 12 |-|-|\n",
      "|Oui| Cdeux22 | Autoencodeur sur image | 3 | 6, 14 |-|-|\n",
      "|Oui| Cdeux23 | Autoencodeur sur image | 3 | 6, 16 |-|-|\n",
      "|Oui| Cdeux24 | Autoencodeur sur image | 3 | 8, 2 |-|-|\n",
      "|Oui| Cdeux25 | Autoencodeur sur image | 3 | 8, 4 |-|-|\n",
      "|Oui| Cdeux26 | Autoencodeur sur image | 3 | 8, 6 |-|-|\n",
      "|Oui| Cdeux27 | Autoencodeur sur image | 3 | 8, 8 |-|-|\n",
      "|Oui| Cdeux28 | Autoencodeur sur image | 3 | 8, 10 |-|-|\n",
      "|Oui| Cdeux29 | Autoencodeur sur image | 3 | 8, 12 |-|-|\n",
      "|Oui| Cdeux30 | Autoencodeur sur image | 3 | 8, 14 |-|-|\n",
      "|Oui| Cdeux31 | Autoencodeur sur image | 3 | 8, 16 |-|-|\n",
      "|Oui| Cdeux32 | Autoencodeur sur image | 3 | 10, 2 |-|-|\n",
      "|Oui| Cdeux33 | Autoencodeur sur image | 3 | 10, 4 |-|-|\n",
      "|Oui| Cdeux34 | Autoencodeur sur image | 3 | 10, 6 |-|-|\n",
      "|Oui| Cdeux35 | Autoencodeur sur image | 3 | 10, 8 |-|-|\n",
      "|Oui| Cdeux36 | Autoencodeur sur image | 3 | 10, 10 |-|-|\n",
      "|Oui| Cdeux37 | Autoencodeur sur image | 3 | 10, 12 |-|-|\n",
      "|Oui| Cdeux38 | Autoencodeur sur image | 3 | 10, 14 |-|-|\n",
      "|Oui| Cdeux39 | Autoencodeur sur image | 3 | 10, 16 |-|-|\n",
      "|Oui| Cdeux40 | Autoencodeur sur image | 3 | 12, 2 |-|-|\n",
      "|Oui| Cdeux41 | Autoencodeur sur image | 3 | 12, 4 |-|-|\n",
      "|Oui| Cdeux42 | Autoencodeur sur image | 3 | 12, 6 |-|-|\n",
      "|Oui| Cdeux43 | Autoencodeur sur image | 3 | 12, 8 |-|-|\n",
      "|Oui| Cdeux44 | Autoencodeur sur image | 3 | 12, 10 |-|-|\n",
      "|Oui| Cdeux45 | Autoencodeur sur image | 3 | 12, 12 |-|-|\n",
      "|Oui| Cdeux46 | Autoencodeur sur image | 3 | 12, 14 |-|-|\n",
      "|Oui| Cdeux47 | Autoencodeur sur image | 3 | 12, 16 |-|-|\n",
      "|Oui| Cdeux48 | Autoencodeur sur image | 3 | 14, 2 |-|-|\n",
      "|Oui| Cdeux49 | Autoencodeur sur image | 3 | 14, 4 |-|-|\n",
      "|Oui| Cdeux50 | Autoencodeur sur image | 3 | 14, 6 |-|-|\n",
      "|Oui| Cdeux51 | Autoencodeur sur image | 3 | 14, 8 |-|-|\n",
      "|Oui| Cdeux52 | Autoencodeur sur image | 3 | 14, 10 |-|-|\n",
      "|Oui| Cdeux53 | Autoencodeur sur image | 3 | 14, 12 |-|-|\n",
      "|Oui| Cdeux54 | Autoencodeur sur image | 3 | 14, 14 |-|-|\n",
      "|Oui| Cdeux55 | Autoencodeur sur image | 3 | 14, 16 |-|-|\n",
      "|Oui| Cdeux56 | Autoencodeur sur image | 3 | 16, 2 |-|-|\n",
      "|Oui| Cdeux57 | Autoencodeur sur image | 3 | 16, 4 |-|-|\n",
      "|Oui| Cdeux58 | Autoencodeur sur image | 3 | 16, 6 |-|-|\n",
      "|Oui| Cdeux59 | Autoencodeur sur image | 3 | 16, 8 |-|-|\n",
      "|Oui| Cdeux60 | Autoencodeur sur image | 3 | 16, 10 |-|-|\n",
      "|Oui| Cdeux61 | Autoencodeur sur image | 3 | 16, 12 |-|-|\n",
      "|Oui| Cdeux62 | Autoencodeur sur image | 3 | 16, 14 |-|-|\n",
      "|Oui| Cdeux63 | Autoencodeur sur image | 3 | 16, 16 |-|-|\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"|Implémenté| Essai | Fonctionnement | Nombre de couches | Nb de filtres | Taille du noyau| Choix | Résultats |\n",
    "| --- | --- | ---| ---| ---| ---| --- | --- |\"\"\")\n",
    "i = 0\n",
    "for c1 in range(1,9):\n",
    "    for c2 in range(1,9):\n",
    "        print(\"\"\"|Oui| Cdeux\"\"\"+str(i)+\"\"\" | Autoencodeur sur image | 3 | \"\"\"+str(c1*2)+\"\"\", \"\"\"+str(c2*2)+\"\"\" |-|-|\"\"\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "|Implémenté| Essai | Fonctionnement | Nombre de couches | Nb de filtres | Taille du noyau| Choix | Résultats |\n",
    "| --- | --- | ---| ---| ---| ---| --- | --- |\n",
    "|Oui| Cdeux0 | Autoencodeur sur image | 3 | 2, 2 |-|-|\n",
    "|Oui| Cdeux1 | Autoencodeur sur image | 3 | 2, 4 |-|-|\n",
    "|Oui| Cdeux2 | Autoencodeur sur image | 3 | 2, 6 |-|-|\n",
    "|Oui| Cdeux3 | Autoencodeur sur image | 3 | 2, 8 |-|-|\n",
    "|Oui| Cdeux4 | Autoencodeur sur image | 3 | 2, 10 |-|-|\n",
    "|Oui| Cdeux5 | Autoencodeur sur image | 3 | 2, 12 |-|-|\n",
    "|Oui| Cdeux6 | Autoencodeur sur image | 3 | 2, 14 |-|-|\n",
    "|Oui| Cdeux7 | Autoencodeur sur image | 3 | 2, 16 |-|-|\n",
    "|Oui| Cdeux8 | Autoencodeur sur image | 3 | 4, 2 |-|-|\n",
    "|Oui| Cdeux9 | Autoencodeur sur image | 3 | 4, 4 |-|-|\n",
    "|Oui| Cdeux10 | Autoencodeur sur image | 3 | 4, 6 |-|-|\n",
    "|Oui| Cdeux11 | Autoencodeur sur image | 3 | 4, 8 |-|-|\n",
    "|Oui| Cdeux12 | Autoencodeur sur image | 3 | 4, 10 |-|-|\n",
    "|Oui| Cdeux13 | Autoencodeur sur image | 3 | 4, 12 |-|-|\n",
    "|Oui| Cdeux14 | Autoencodeur sur image | 3 | 4, 14 |-|-|\n",
    "|Oui| Cdeux15 | Autoencodeur sur image | 3 | 4, 16 |-|-|\n",
    "|Oui| Cdeux16 | Autoencodeur sur image | 3 | 6, 2 |-|-|\n",
    "|Oui| Cdeux17 | Autoencodeur sur image | 3 | 6, 4 |-|-|\n",
    "|Oui| Cdeux18 | Autoencodeur sur image | 3 | 6, 6 |-|-|\n",
    "|Oui| Cdeux19 | Autoencodeur sur image | 3 | 6, 8 |-|-|\n",
    "|Oui| Cdeux20 | Autoencodeur sur image | 3 | 6, 10 |-|-|\n",
    "|Oui| Cdeux21 | Autoencodeur sur image | 3 | 6, 12 |-|-|\n",
    "|Oui| Cdeux22 | Autoencodeur sur image | 3 | 6, 14 |-|-|\n",
    "|Oui| Cdeux23 | Autoencodeur sur image | 3 | 6, 16 |-|-|\n",
    "|Oui| Cdeux24 | Autoencodeur sur image | 3 | 8, 2 |-|-|\n",
    "|Oui| Cdeux25 | Autoencodeur sur image | 3 | 8, 4 |-|-|\n",
    "|Oui| Cdeux26 | Autoencodeur sur image | 3 | 8, 6 |-|-|\n",
    "|Oui| Cdeux27 | Autoencodeur sur image | 3 | 8, 8 |-|-|\n",
    "|Oui| Cdeux28 | Autoencodeur sur image | 3 | 8, 10 |-|-|\n",
    "|Oui| Cdeux29 | Autoencodeur sur image | 3 | 8, 12 |-|-|\n",
    "|Oui| Cdeux30 | Autoencodeur sur image | 3 | 8, 14 |-|-|\n",
    "|Oui| Cdeux31 | Autoencodeur sur image | 3 | 8, 16 |-|-|\n",
    "|Oui| Cdeux32 | Autoencodeur sur image | 3 | 10, 2 |-|-|\n",
    "|Oui| Cdeux33 | Autoencodeur sur image | 3 | 10, 4 |-|-|\n",
    "|Oui| Cdeux34 | Autoencodeur sur image | 3 | 10, 6 |-|-|\n",
    "|Oui| Cdeux35 | Autoencodeur sur image | 3 | 10, 8 |-|-|\n",
    "|Oui| Cdeux36 | Autoencodeur sur image | 3 | 10, 10 |-|-|\n",
    "|Oui| Cdeux37 | Autoencodeur sur image | 3 | 10, 12 |-|-|\n",
    "|Oui| Cdeux38 | Autoencodeur sur image | 3 | 10, 14 |-|-|\n",
    "|Oui| Cdeux39 | Autoencodeur sur image | 3 | 10, 16 |-|-|\n",
    "|Oui| Cdeux40 | Autoencodeur sur image | 3 | 12, 2 |-|-|\n",
    "|Oui| Cdeux41 | Autoencodeur sur image | 3 | 12, 4 |-|-|\n",
    "|Oui| Cdeux42 | Autoencodeur sur image | 3 | 12, 6 |-|-|\n",
    "|Oui| Cdeux43 | Autoencodeur sur image | 3 | 12, 8 |-|-|\n",
    "|Oui| Cdeux44 | Autoencodeur sur image | 3 | 12, 10 |-|-|\n",
    "|Oui| Cdeux45 | Autoencodeur sur image | 3 | 12, 12 |-|-|\n",
    "|Oui| Cdeux46 | Autoencodeur sur image | 3 | 12, 14 |-|-|\n",
    "|Oui| Cdeux47 | Autoencodeur sur image | 3 | 12, 16 |-|-|\n",
    "|Oui| Cdeux48 | Autoencodeur sur image | 3 | 14, 2 |-|-|\n",
    "|Oui| Cdeux49 | Autoencodeur sur image | 3 | 14, 4 |-|-|\n",
    "|Oui| Cdeux50 | Autoencodeur sur image | 3 | 14, 6 |-|-|\n",
    "|Oui| Cdeux51 | Autoencodeur sur image | 3 | 14, 8 |-|-|\n",
    "|Oui| Cdeux52 | Autoencodeur sur image | 3 | 14, 10 |-|-|\n",
    "|Oui| Cdeux53 | Autoencodeur sur image | 3 | 14, 12 |-|-|\n",
    "|Oui| Cdeux54 | Autoencodeur sur image | 3 | 14, 14 |-|-|\n",
    "|Oui| Cdeux55 | Autoencodeur sur image | 3 | 14, 16 |-|-|\n",
    "|Oui| Cdeux56 | Autoencodeur sur image | 3 | 16, 2 |-|-|\n",
    "|Oui| Cdeux57 | Autoencodeur sur image | 3 | 16, 4 |-|-|\n",
    "|Oui| Cdeux58 | Autoencodeur sur image | 3 | 16, 6 |-|-|\n",
    "|Oui| Cdeux59 | Autoencodeur sur image | 3 | 16, 8 |-|-|\n",
    "|Oui| Cdeux60 | Autoencodeur sur image | 3 | 16, 10 |-|-|\n",
    "|Oui| Cdeux61 | Autoencodeur sur image | 3 | 16, 12 |-|-|\n",
    "|Oui| Cdeux62 | Autoencodeur sur image | 3 | 16, 14 |-|-|\n",
    "|Oui| Cdeux63 | Autoencodeur sur image | 3 | 16, 16 |-|-|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 0\n",
    "c1,c2 = 2,2\n",
    "while c1 <= 16:\n",
    "  c2 = 2\n",
    "  while c2 <= 16:\n",
    "    for p in (2,3):\n",
    "      print(\"Essai \"+str(i)+\" pour conv0 avec \" + str(c1) + \" et conv1 avec \" + str(c2) + \"et pour couche de pooling \"+str(p))\n",
    "      print(str(i/127)+\"% réalisé\")\n",
    "      nom = 'EssaiCdeuxPooling'+str(i)\n",
    "\n",
    "      tf.reset_default_graph()\n",
    "\n",
    "      root_logdir = \"tf_logs\"\n",
    "      logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "      with tf.Graph().as_default():\n",
    "          X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "          y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "          print(X.get_shape())\n",
    "\n",
    "\n",
    "          conv0 = tf.layers.conv2d(X, filters=3, kernel_size=c1,\n",
    "                  strides=1, padding='SAME',\n",
    " \t \t \t            activation=tf.nn.relu, name='conv0')\n",
    "          size0 = conv0.get_shape()\n",
    "          print('Shape of conv0 : '+str(size0))\n",
    "          maxPool0 = tf.layers.max_pooling2d(conv0, pool_size=(p,p), strides=(2,2), padding='same')\n",
    "          print('Shape of maxPool0 : '+str(maxPool0.get_shape()))\n",
    "\n",
    "          conv1 = tf.layers.conv2d(maxPool0, filters=3, kernel_size=c2,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv1')\n",
    "          size1 = conv1.get_shape()\n",
    "          print('Shape of conv1 : '+str(size1))\n",
    "          conv2 = tf.layers.conv2d(conv1, filters=3, kernel_size=c2,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv2')\n",
    "          print('Shape of conv2 : '+str(conv2.get_shape()))\n",
    "          resize0 = tf.image.resize_images(conv2, size=size0[1:3], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "          print('Shape of resize0 : '+str(resize0.get_shape()))\n",
    "          conv3 = tf.layers.conv2d(resize0, filters=3, kernel_size=c1,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv3')\n",
    "          print('Shape of conv3 : '+str(conv3.get_shape()))\n",
    "\n",
    "\n",
    "          with tf.name_scope('Optimizer'):\n",
    "              optimizer = tf.train.AdamOptimizer()\n",
    "          with tf.name_scope(\"phase0\"):\n",
    "              loss0 = tf.reduce_mean(tf.square(conv3 - y))\n",
    "              training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "          with tf.name_scope(\"init\"):\n",
    "              init = tf.global_variables_initializer()\n",
    "          with tf.name_scope(\"enregistrement\"):\n",
    "              summary_writer = tbc.get_writer()\n",
    "              loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "              kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
    "              bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
    "              kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
    "              bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "              \n",
    "              kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/kernel')[0]\n",
    "              bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/bias')[0]\n",
    "              kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/kernel')[0]\n",
    "              bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/bias')[0]\n",
    "              \n",
    "              kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "              bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "              kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "              bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "              kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
    "              bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
    "              kernel_saver3 = tf.summary.histogram(\"Kernel3\",kernel2)\n",
    "              bias_saver3 = tf.summary.histogram(\"Bias3\",bias2)\n",
    "              \n",
    "              print(os.path.split(X.name))\n",
    "              summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "              saver = tf.train.Saver()\n",
    "\n",
    "          with tf.name_scope('Avant-Apres_'+nom):\n",
    "              orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "              fin = tf.summary.image('output',tf.cast(conv1,dtype=tf.uint8))\n",
    "\n",
    "          n_epochs = 100\n",
    "          n_batches_per_epoch = 3\n",
    "          Lloss = []\n",
    "          def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "              global Lloss\n",
    "              if len(Lloss) < duree:\n",
    "                  Lloss.append(lastLoss)\n",
    "                  return False\n",
    "              else:\n",
    "                  Lloss = Lloss[1:duree] + [lastLoss]\n",
    "                  L = Lloss[:]\n",
    "                  L.sort(reverse=True)\n",
    "                  if L[0] <= seuil:\n",
    "                    if decroissance == True:\n",
    "                      return True if L == Lloss else False\n",
    "                    else:\n",
    "                      return True\n",
    "                  return False\n",
    "\n",
    "          import os\n",
    "          with tf.Session() as sess:\n",
    "              init.run()\n",
    "              for epoch in range(n_epochs):\n",
    "                  print(\"epoch : \",epoch)\n",
    "                  for iteration in range(n_batches_per_epoch):\n",
    "                      X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                      sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                      if iteration % 5 == 0:\n",
    "                        summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel3,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias3,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "                  if arret(loss_train0,100,6) == True:\n",
    "                    break\n",
    "              summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "              summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "              saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      summary_writer0.close()\n",
    "    c2 += 2\n",
    "    i += 1\n",
    "  c1 += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globalement à la fin le système reste imprécis et n'apprend plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après une repécision sur la théorie des couches de convolution il apparait que ces couches permettent de rééchantillonner les données, d'en extraire les caractéristique et d'en réduire la dimensionnalité. Néanmoins, pour cela une couche de pooling est nécessaire. On reprend donc la démarche entamée en essai 8 sans la couche intégralement connectée tout d'abord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme cette piste expliquerait les résultats, et étant donné les résultats sur une seule couche peut prometteurs, l'entraienement de 2 couche par couche est laissé de côté."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>On fait varier les noyaux entre 2 et 16, avec 3 filtre par couche, pour 2 couches tout en faisant un pooling allant de 2 à 3 et un pas unitaire. On fait utilise le symétrique pour la reconstruction.</b></br>\n",
    "Donc 2 étapes : </br>\n",
    "<ol>\n",
    "    <li>Encodage : réduction de l'image</li>\n",
    "    <li>Décodage : reconstruction de l'image désirée</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On aura des couches du type : (voir cellules cachées du dessous pour le détail des essais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Implémenté| Essai | Fonctionnement | Taille du noyau 1 | Pooling | Taille du noyau 2 | Choix | Résultats |\n",
    "| --- | --- | ---| ---| ---| ---| --- | --- |\n",
    "|Oui| CdeuxPooling0 | Autoencodeur sur image | 2| 2 |2|-|-|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Implémenté| Essai | Fonctionnement | Taille du noyau 1 | Pooling | Taille du noyau 2 | Choix | Résultats |\n",
      "| --- | --- | ---| ---| ---| ---| --- | --- |\n",
      "|Oui| CdeuxPooling0 | Autoencodeur sur image | 2| 2 |2|-|-|\n",
      "|Oui| CdeuxPooling1 | Autoencodeur sur image | 2| 3 |2|-|-|\n",
      "|Oui| CdeuxPooling2 | Autoencodeur sur image | 2| 2 |4|-|-|\n",
      "|Oui| CdeuxPooling3 | Autoencodeur sur image | 2| 3 |4|-|-|\n",
      "|Oui| CdeuxPooling4 | Autoencodeur sur image | 2| 2 |6|-|-|\n",
      "|Oui| CdeuxPooling5 | Autoencodeur sur image | 2| 3 |6|-|-|\n",
      "|Oui| CdeuxPooling6 | Autoencodeur sur image | 2| 2 |8|-|-|\n",
      "|Oui| CdeuxPooling7 | Autoencodeur sur image | 2| 3 |8|-|-|\n",
      "|Oui| CdeuxPooling8 | Autoencodeur sur image | 2| 2 |10|-|-|\n",
      "|Oui| CdeuxPooling9 | Autoencodeur sur image | 2| 3 |10|-|-|\n",
      "|Oui| CdeuxPooling10 | Autoencodeur sur image | 2| 2 |12|-|-|\n",
      "|Oui| CdeuxPooling11 | Autoencodeur sur image | 2| 3 |12|-|-|\n",
      "|Oui| CdeuxPooling12 | Autoencodeur sur image | 2| 2 |14|-|-|\n",
      "|Oui| CdeuxPooling13 | Autoencodeur sur image | 2| 3 |14|-|-|\n",
      "|Oui| CdeuxPooling14 | Autoencodeur sur image | 2| 2 |16|-|-|\n",
      "|Oui| CdeuxPooling15 | Autoencodeur sur image | 2| 3 |16|-|-|\n",
      "|Oui| CdeuxPooling16 | Autoencodeur sur image | 4| 2 |2|-|-|\n",
      "|Oui| CdeuxPooling17 | Autoencodeur sur image | 4| 3 |2|-|-|\n",
      "|Oui| CdeuxPooling18 | Autoencodeur sur image | 4| 2 |4|-|-|\n",
      "|Oui| CdeuxPooling19 | Autoencodeur sur image | 4| 3 |4|-|-|\n",
      "|Oui| CdeuxPooling20 | Autoencodeur sur image | 4| 2 |6|-|-|\n",
      "|Oui| CdeuxPooling21 | Autoencodeur sur image | 4| 3 |6|-|-|\n",
      "|Oui| CdeuxPooling22 | Autoencodeur sur image | 4| 2 |8|-|-|\n",
      "|Oui| CdeuxPooling23 | Autoencodeur sur image | 4| 3 |8|-|-|\n",
      "|Oui| CdeuxPooling24 | Autoencodeur sur image | 4| 2 |10|-|-|\n",
      "|Oui| CdeuxPooling25 | Autoencodeur sur image | 4| 3 |10|-|-|\n",
      "|Oui| CdeuxPooling26 | Autoencodeur sur image | 4| 2 |12|-|-|\n",
      "|Oui| CdeuxPooling27 | Autoencodeur sur image | 4| 3 |12|-|-|\n",
      "|Oui| CdeuxPooling28 | Autoencodeur sur image | 4| 2 |14|-|-|\n",
      "|Oui| CdeuxPooling29 | Autoencodeur sur image | 4| 3 |14|-|-|\n",
      "|Oui| CdeuxPooling30 | Autoencodeur sur image | 4| 2 |16|-|-|\n",
      "|Oui| CdeuxPooling31 | Autoencodeur sur image | 4| 3 |16|-|-|\n",
      "|Oui| CdeuxPooling32 | Autoencodeur sur image | 6| 2 |2|-|-|\n",
      "|Oui| CdeuxPooling33 | Autoencodeur sur image | 6| 3 |2|-|-|\n",
      "|Oui| CdeuxPooling34 | Autoencodeur sur image | 6| 2 |4|-|-|\n",
      "|Oui| CdeuxPooling35 | Autoencodeur sur image | 6| 3 |4|-|-|\n",
      "|Oui| CdeuxPooling36 | Autoencodeur sur image | 6| 2 |6|-|-|\n",
      "|Oui| CdeuxPooling37 | Autoencodeur sur image | 6| 3 |6|-|-|\n",
      "|Oui| CdeuxPooling38 | Autoencodeur sur image | 6| 2 |8|-|-|\n",
      "|Oui| CdeuxPooling39 | Autoencodeur sur image | 6| 3 |8|-|-|\n",
      "|Oui| CdeuxPooling40 | Autoencodeur sur image | 6| 2 |10|-|-|\n",
      "|Oui| CdeuxPooling41 | Autoencodeur sur image | 6| 3 |10|-|-|\n",
      "|Oui| CdeuxPooling42 | Autoencodeur sur image | 6| 2 |12|-|-|\n",
      "|Oui| CdeuxPooling43 | Autoencodeur sur image | 6| 3 |12|-|-|\n",
      "|Oui| CdeuxPooling44 | Autoencodeur sur image | 6| 2 |14|-|-|\n",
      "|Oui| CdeuxPooling45 | Autoencodeur sur image | 6| 3 |14|-|-|\n",
      "|Oui| CdeuxPooling46 | Autoencodeur sur image | 6| 2 |16|-|-|\n",
      "|Oui| CdeuxPooling47 | Autoencodeur sur image | 6| 3 |16|-|-|\n",
      "|Oui| CdeuxPooling48 | Autoencodeur sur image | 8| 2 |2|-|-|\n",
      "|Oui| CdeuxPooling49 | Autoencodeur sur image | 8| 3 |2|-|-|\n",
      "|Oui| CdeuxPooling50 | Autoencodeur sur image | 8| 2 |4|-|-|\n",
      "|Oui| CdeuxPooling51 | Autoencodeur sur image | 8| 3 |4|-|-|\n",
      "|Oui| CdeuxPooling52 | Autoencodeur sur image | 8| 2 |6|-|-|\n",
      "|Oui| CdeuxPooling53 | Autoencodeur sur image | 8| 3 |6|-|-|\n",
      "|Oui| CdeuxPooling54 | Autoencodeur sur image | 8| 2 |8|-|-|\n",
      "|Oui| CdeuxPooling55 | Autoencodeur sur image | 8| 3 |8|-|-|\n",
      "|Oui| CdeuxPooling56 | Autoencodeur sur image | 8| 2 |10|-|-|\n",
      "|Oui| CdeuxPooling57 | Autoencodeur sur image | 8| 3 |10|-|-|\n",
      "|Oui| CdeuxPooling58 | Autoencodeur sur image | 8| 2 |12|-|-|\n",
      "|Oui| CdeuxPooling59 | Autoencodeur sur image | 8| 3 |12|-|-|\n",
      "|Oui| CdeuxPooling60 | Autoencodeur sur image | 8| 2 |14|-|-|\n",
      "|Oui| CdeuxPooling61 | Autoencodeur sur image | 8| 3 |14|-|-|\n",
      "|Oui| CdeuxPooling62 | Autoencodeur sur image | 8| 2 |16|-|-|\n",
      "|Oui| CdeuxPooling63 | Autoencodeur sur image | 8| 3 |16|-|-|\n",
      "|Oui| CdeuxPooling64 | Autoencodeur sur image | 10| 2 |2|-|-|\n",
      "|Oui| CdeuxPooling65 | Autoencodeur sur image | 10| 3 |2|-|-|\n",
      "|Oui| CdeuxPooling66 | Autoencodeur sur image | 10| 2 |4|-|-|\n",
      "|Oui| CdeuxPooling67 | Autoencodeur sur image | 10| 3 |4|-|-|\n",
      "|Oui| CdeuxPooling68 | Autoencodeur sur image | 10| 2 |6|-|-|\n",
      "|Oui| CdeuxPooling69 | Autoencodeur sur image | 10| 3 |6|-|-|\n",
      "|Oui| CdeuxPooling70 | Autoencodeur sur image | 10| 2 |8|-|-|\n",
      "|Oui| CdeuxPooling71 | Autoencodeur sur image | 10| 3 |8|-|-|\n",
      "|Oui| CdeuxPooling72 | Autoencodeur sur image | 10| 2 |10|-|-|\n",
      "|Oui| CdeuxPooling73 | Autoencodeur sur image | 10| 3 |10|-|-|\n",
      "|Oui| CdeuxPooling74 | Autoencodeur sur image | 10| 2 |12|-|-|\n",
      "|Oui| CdeuxPooling75 | Autoencodeur sur image | 10| 3 |12|-|-|\n",
      "|Oui| CdeuxPooling76 | Autoencodeur sur image | 10| 2 |14|-|-|\n",
      "|Oui| CdeuxPooling77 | Autoencodeur sur image | 10| 3 |14|-|-|\n",
      "|Oui| CdeuxPooling78 | Autoencodeur sur image | 10| 2 |16|-|-|\n",
      "|Oui| CdeuxPooling79 | Autoencodeur sur image | 10| 3 |16|-|-|\n",
      "|Oui| CdeuxPooling80 | Autoencodeur sur image | 12| 2 |2|-|-|\n",
      "|Oui| CdeuxPooling81 | Autoencodeur sur image | 12| 3 |2|-|-|\n",
      "|Oui| CdeuxPooling82 | Autoencodeur sur image | 12| 2 |4|-|-|\n",
      "|Oui| CdeuxPooling83 | Autoencodeur sur image | 12| 3 |4|-|-|\n",
      "|Oui| CdeuxPooling84 | Autoencodeur sur image | 12| 2 |6|-|-|\n",
      "|Oui| CdeuxPooling85 | Autoencodeur sur image | 12| 3 |6|-|-|\n",
      "|Oui| CdeuxPooling86 | Autoencodeur sur image | 12| 2 |8|-|-|\n",
      "|Oui| CdeuxPooling87 | Autoencodeur sur image | 12| 3 |8|-|-|\n",
      "|Oui| CdeuxPooling88 | Autoencodeur sur image | 12| 2 |10|-|-|\n",
      "|Oui| CdeuxPooling89 | Autoencodeur sur image | 12| 3 |10|-|-|\n",
      "|Oui| CdeuxPooling90 | Autoencodeur sur image | 12| 2 |12|-|-|\n",
      "|Oui| CdeuxPooling91 | Autoencodeur sur image | 12| 3 |12|-|-|\n",
      "|Oui| CdeuxPooling92 | Autoencodeur sur image | 12| 2 |14|-|-|\n",
      "|Oui| CdeuxPooling93 | Autoencodeur sur image | 12| 3 |14|-|-|\n",
      "|Oui| CdeuxPooling94 | Autoencodeur sur image | 12| 2 |16|-|-|\n",
      "|Oui| CdeuxPooling95 | Autoencodeur sur image | 12| 3 |16|-|-|\n",
      "|Oui| CdeuxPooling96 | Autoencodeur sur image | 14| 2 |2|-|-|\n",
      "|Oui| CdeuxPooling97 | Autoencodeur sur image | 14| 3 |2|-|-|\n",
      "|Oui| CdeuxPooling98 | Autoencodeur sur image | 14| 2 |4|-|-|\n",
      "|Oui| CdeuxPooling99 | Autoencodeur sur image | 14| 3 |4|-|-|\n",
      "|Oui| CdeuxPooling100 | Autoencodeur sur image | 14| 2 |6|-|-|\n",
      "|Oui| CdeuxPooling101 | Autoencodeur sur image | 14| 3 |6|-|-|\n",
      "|Oui| CdeuxPooling102 | Autoencodeur sur image | 14| 2 |8|-|-|\n",
      "|Oui| CdeuxPooling103 | Autoencodeur sur image | 14| 3 |8|-|-|\n",
      "|Oui| CdeuxPooling104 | Autoencodeur sur image | 14| 2 |10|-|-|\n",
      "|Oui| CdeuxPooling105 | Autoencodeur sur image | 14| 3 |10|-|-|\n",
      "|Oui| CdeuxPooling106 | Autoencodeur sur image | 14| 2 |12|-|-|\n",
      "|Oui| CdeuxPooling107 | Autoencodeur sur image | 14| 3 |12|-|-|\n",
      "|Oui| CdeuxPooling108 | Autoencodeur sur image | 14| 2 |14|-|-|\n",
      "|Oui| CdeuxPooling109 | Autoencodeur sur image | 14| 3 |14|-|-|\n",
      "|Oui| CdeuxPooling110 | Autoencodeur sur image | 14| 2 |16|-|-|\n",
      "|Oui| CdeuxPooling111 | Autoencodeur sur image | 14| 3 |16|-|-|\n",
      "|Oui| CdeuxPooling112 | Autoencodeur sur image | 16| 2 |2|-|-|\n",
      "|Oui| CdeuxPooling113 | Autoencodeur sur image | 16| 3 |2|-|-|\n",
      "|Oui| CdeuxPooling114 | Autoencodeur sur image | 16| 2 |4|-|-|\n",
      "|Oui| CdeuxPooling115 | Autoencodeur sur image | 16| 3 |4|-|-|\n",
      "|Oui| CdeuxPooling116 | Autoencodeur sur image | 16| 2 |6|-|-|\n",
      "|Oui| CdeuxPooling117 | Autoencodeur sur image | 16| 3 |6|-|-|\n",
      "|Oui| CdeuxPooling118 | Autoencodeur sur image | 16| 2 |8|-|-|\n",
      "|Oui| CdeuxPooling119 | Autoencodeur sur image | 16| 3 |8|-|-|\n",
      "|Oui| CdeuxPooling120 | Autoencodeur sur image | 16| 2 |10|-|-|\n",
      "|Oui| CdeuxPooling121 | Autoencodeur sur image | 16| 3 |10|-|-|\n",
      "|Oui| CdeuxPooling122 | Autoencodeur sur image | 16| 2 |12|-|-|\n",
      "|Oui| CdeuxPooling123 | Autoencodeur sur image | 16| 3 |12|-|-|\n",
      "|Oui| CdeuxPooling124 | Autoencodeur sur image | 16| 2 |14|-|-|\n",
      "|Oui| CdeuxPooling125 | Autoencodeur sur image | 16| 3 |14|-|-|\n",
      "|Oui| CdeuxPooling126 | Autoencodeur sur image | 16| 2 |16|-|-|\n",
      "|Oui| CdeuxPooling127 | Autoencodeur sur image | 16| 3 |16|-|-|\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"|Implémenté| Essai | Fonctionnement | Taille du noyau 1 | Pooling | Taille du noyau 2 | Choix | Résultats |\n",
    "| --- | --- | ---| ---| ---| ---| --- | --- |\"\"\")\n",
    "i = 0\n",
    "for c1 in range(1,9):\n",
    "    for c2 in range(1,9):\n",
    "        for p in (2,3):\n",
    "            print(\"\"\"|Oui| CdeuxPooling\"\"\"+str(i)+\"\"\" | Autoencodeur sur image | \"\"\"+str(c1*2)+\"\"\"| \"\"\"+str(p)+\"\"\" |\"\"\"+str(c2*2)+\"\"\"|-|-|\"\"\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "|Implémenté| Essai | Fonctionnement | Taille du noyau 1 | Pooling | Taille du noyau 2 | Choix | Résultats |\n",
    "| --- | --- | ---| ---| ---| ---| --- | --- |\n",
    "|Oui| CdeuxPooling0 | Autoencodeur sur image | 2| 2 |2|-|-|\n",
    "|Oui| CdeuxPooling1 | Autoencodeur sur image | 2| 3 |2|-|-|\n",
    "|Oui| CdeuxPooling2 | Autoencodeur sur image | 2| 2 |4|-|-|\n",
    "|Oui| CdeuxPooling3 | Autoencodeur sur image | 2| 3 |4|-|-|\n",
    "|Oui| CdeuxPooling4 | Autoencodeur sur image | 2| 2 |6|-|-|\n",
    "|Oui| CdeuxPooling5 | Autoencodeur sur image | 2| 3 |6|-|-|\n",
    "|Oui| CdeuxPooling6 | Autoencodeur sur image | 2| 2 |8|-|-|\n",
    "|Oui| CdeuxPooling7 | Autoencodeur sur image | 2| 3 |8|-|-|\n",
    "|Oui| CdeuxPooling8 | Autoencodeur sur image | 2| 2 |10|-|-|\n",
    "|Oui| CdeuxPooling9 | Autoencodeur sur image | 2| 3 |10|-|-|\n",
    "|Oui| CdeuxPooling10 | Autoencodeur sur image | 2| 2 |12|-|-|\n",
    "|Oui| CdeuxPooling11 | Autoencodeur sur image | 2| 3 |12|-|-|\n",
    "|Oui| CdeuxPooling12 | Autoencodeur sur image | 2| 2 |14|-|-|\n",
    "|Oui| CdeuxPooling13 | Autoencodeur sur image | 2| 3 |14|-|-|\n",
    "|Oui| CdeuxPooling14 | Autoencodeur sur image | 2| 2 |16|-|-|\n",
    "|Oui| CdeuxPooling15 | Autoencodeur sur image | 2| 3 |16|-|-|\n",
    "|Oui| CdeuxPooling16 | Autoencodeur sur image | 4| 2 |2|-|-|\n",
    "|Oui| CdeuxPooling17 | Autoencodeur sur image | 4| 3 |2|-|-|\n",
    "|Oui| CdeuxPooling18 | Autoencodeur sur image | 4| 2 |4|-|-|\n",
    "|Oui| CdeuxPooling19 | Autoencodeur sur image | 4| 3 |4|-|-|\n",
    "|Oui| CdeuxPooling20 | Autoencodeur sur image | 4| 2 |6|-|-|\n",
    "|Oui| CdeuxPooling21 | Autoencodeur sur image | 4| 3 |6|-|-|\n",
    "|Oui| CdeuxPooling22 | Autoencodeur sur image | 4| 2 |8|-|-|\n",
    "|Oui| CdeuxPooling23 | Autoencodeur sur image | 4| 3 |8|-|-|\n",
    "|Oui| CdeuxPooling24 | Autoencodeur sur image | 4| 2 |10|-|-|\n",
    "|Oui| CdeuxPooling25 | Autoencodeur sur image | 4| 3 |10|-|-|\n",
    "|Oui| CdeuxPooling26 | Autoencodeur sur image | 4| 2 |12|-|-|\n",
    "|Oui| CdeuxPooling27 | Autoencodeur sur image | 4| 3 |12|-|-|\n",
    "|Oui| CdeuxPooling28 | Autoencodeur sur image | 4| 2 |14|-|-|\n",
    "|Oui| CdeuxPooling29 | Autoencodeur sur image | 4| 3 |14|-|-|\n",
    "|Oui| CdeuxPooling30 | Autoencodeur sur image | 4| 2 |16|-|-|\n",
    "|Oui| CdeuxPooling31 | Autoencodeur sur image | 4| 3 |16|-|-|\n",
    "|Oui| CdeuxPooling32 | Autoencodeur sur image | 6| 2 |2|-|-|\n",
    "|Oui| CdeuxPooling33 | Autoencodeur sur image | 6| 3 |2|-|-|\n",
    "|Oui| CdeuxPooling34 | Autoencodeur sur image | 6| 2 |4|-|-|\n",
    "|Oui| CdeuxPooling35 | Autoencodeur sur image | 6| 3 |4|-|-|\n",
    "|Oui| CdeuxPooling36 | Autoencodeur sur image | 6| 2 |6|-|-|\n",
    "|Oui| CdeuxPooling37 | Autoencodeur sur image | 6| 3 |6|-|-|\n",
    "|Oui| CdeuxPooling38 | Autoencodeur sur image | 6| 2 |8|-|-|\n",
    "|Oui| CdeuxPooling39 | Autoencodeur sur image | 6| 3 |8|-|-|\n",
    "|Oui| CdeuxPooling40 | Autoencodeur sur image | 6| 2 |10|-|-|\n",
    "|Oui| CdeuxPooling41 | Autoencodeur sur image | 6| 3 |10|-|-|\n",
    "|Oui| CdeuxPooling42 | Autoencodeur sur image | 6| 2 |12|-|-|\n",
    "|Oui| CdeuxPooling43 | Autoencodeur sur image | 6| 3 |12|-|-|\n",
    "|Oui| CdeuxPooling44 | Autoencodeur sur image | 6| 2 |14|-|-|\n",
    "|Oui| CdeuxPooling45 | Autoencodeur sur image | 6| 3 |14|-|-|\n",
    "|Oui| CdeuxPooling46 | Autoencodeur sur image | 6| 2 |16|-|-|\n",
    "|Oui| CdeuxPooling47 | Autoencodeur sur image | 6| 3 |16|-|-|\n",
    "|Oui| CdeuxPooling48 | Autoencodeur sur image | 8| 2 |2|-|-|\n",
    "|Oui| CdeuxPooling49 | Autoencodeur sur image | 8| 3 |2|-|-|\n",
    "|Oui| CdeuxPooling50 | Autoencodeur sur image | 8| 2 |4|-|-|\n",
    "|Oui| CdeuxPooling51 | Autoencodeur sur image | 8| 3 |4|-|-|\n",
    "|Oui| CdeuxPooling52 | Autoencodeur sur image | 8| 2 |6|-|-|\n",
    "|Oui| CdeuxPooling53 | Autoencodeur sur image | 8| 3 |6|-|-|\n",
    "|Oui| CdeuxPooling54 | Autoencodeur sur image | 8| 2 |8|-|-|\n",
    "|Oui| CdeuxPooling55 | Autoencodeur sur image | 8| 3 |8|-|-|\n",
    "|Oui| CdeuxPooling56 | Autoencodeur sur image | 8| 2 |10|-|-|\n",
    "|Oui| CdeuxPooling57 | Autoencodeur sur image | 8| 3 |10|-|-|\n",
    "|Oui| CdeuxPooling58 | Autoencodeur sur image | 8| 2 |12|-|-|\n",
    "|Oui| CdeuxPooling59 | Autoencodeur sur image | 8| 3 |12|-|-|\n",
    "|Oui| CdeuxPooling60 | Autoencodeur sur image | 8| 2 |14|-|-|\n",
    "|Oui| CdeuxPooling61 | Autoencodeur sur image | 8| 3 |14|-|-|\n",
    "|Oui| CdeuxPooling62 | Autoencodeur sur image | 8| 2 |16|-|-|\n",
    "|Oui| CdeuxPooling63 | Autoencodeur sur image | 8| 3 |16|-|-|\n",
    "|Oui| CdeuxPooling64 | Autoencodeur sur image | 10| 2 |2|-|-|\n",
    "|Oui| CdeuxPooling65 | Autoencodeur sur image | 10| 3 |2|-|-|\n",
    "|Oui| CdeuxPooling66 | Autoencodeur sur image | 10| 2 |4|-|-|\n",
    "|Oui| CdeuxPooling67 | Autoencodeur sur image | 10| 3 |4|-|-|\n",
    "|Oui| CdeuxPooling68 | Autoencodeur sur image | 10| 2 |6|-|-|\n",
    "|Oui| CdeuxPooling69 | Autoencodeur sur image | 10| 3 |6|-|-|\n",
    "|Oui| CdeuxPooling70 | Autoencodeur sur image | 10| 2 |8|-|-|\n",
    "|Oui| CdeuxPooling71 | Autoencodeur sur image | 10| 3 |8|-|-|\n",
    "|Oui| CdeuxPooling72 | Autoencodeur sur image | 10| 2 |10|-|-|\n",
    "|Oui| CdeuxPooling73 | Autoencodeur sur image | 10| 3 |10|-|-|\n",
    "|Oui| CdeuxPooling74 | Autoencodeur sur image | 10| 2 |12|-|-|\n",
    "|Oui| CdeuxPooling75 | Autoencodeur sur image | 10| 3 |12|-|-|\n",
    "|Oui| CdeuxPooling76 | Autoencodeur sur image | 10| 2 |14|-|-|\n",
    "|Oui| CdeuxPooling77 | Autoencodeur sur image | 10| 3 |14|-|-|\n",
    "|Oui| CdeuxPooling78 | Autoencodeur sur image | 10| 2 |16|-|-|\n",
    "|Oui| CdeuxPooling79 | Autoencodeur sur image | 10| 3 |16|-|-|\n",
    "|Oui| CdeuxPooling80 | Autoencodeur sur image | 12| 2 |2|-|-|\n",
    "|Oui| CdeuxPooling81 | Autoencodeur sur image | 12| 3 |2|-|-|\n",
    "|Oui| CdeuxPooling82 | Autoencodeur sur image | 12| 2 |4|-|-|\n",
    "|Oui| CdeuxPooling83 | Autoencodeur sur image | 12| 3 |4|-|-|\n",
    "|Oui| CdeuxPooling84 | Autoencodeur sur image | 12| 2 |6|-|-|\n",
    "|Oui| CdeuxPooling85 | Autoencodeur sur image | 12| 3 |6|-|-|\n",
    "|Oui| CdeuxPooling86 | Autoencodeur sur image | 12| 2 |8|-|-|\n",
    "|Oui| CdeuxPooling87 | Autoencodeur sur image | 12| 3 |8|-|-|\n",
    "|Oui| CdeuxPooling88 | Autoencodeur sur image | 12| 2 |10|-|-|\n",
    "|Oui| CdeuxPooling89 | Autoencodeur sur image | 12| 3 |10|-|-|\n",
    "|Oui| CdeuxPooling90 | Autoencodeur sur image | 12| 2 |12|-|-|\n",
    "|Oui| CdeuxPooling91 | Autoencodeur sur image | 12| 3 |12|-|-|\n",
    "|Oui| CdeuxPooling92 | Autoencodeur sur image | 12| 2 |14|-|-|\n",
    "|Oui| CdeuxPooling93 | Autoencodeur sur image | 12| 3 |14|-|-|\n",
    "|Oui| CdeuxPooling94 | Autoencodeur sur image | 12| 2 |16|-|-|\n",
    "|Oui| CdeuxPooling95 | Autoencodeur sur image | 12| 3 |16|-|-|\n",
    "|Oui| CdeuxPooling96 | Autoencodeur sur image | 14| 2 |2|-|-|\n",
    "|Oui| CdeuxPooling97 | Autoencodeur sur image | 14| 3 |2|-|-|\n",
    "|Oui| CdeuxPooling98 | Autoencodeur sur image | 14| 2 |4|-|-|\n",
    "|Oui| CdeuxPooling99 | Autoencodeur sur image | 14| 3 |4|-|-|\n",
    "|Oui| CdeuxPooling100 | Autoencodeur sur image | 14| 2 |6|-|-|\n",
    "|Oui| CdeuxPooling101 | Autoencodeur sur image | 14| 3 |6|-|-|\n",
    "|Oui| CdeuxPooling102 | Autoencodeur sur image | 14| 2 |8|-|-|\n",
    "|Oui| CdeuxPooling103 | Autoencodeur sur image | 14| 3 |8|-|-|\n",
    "|Oui| CdeuxPooling104 | Autoencodeur sur image | 14| 2 |10|-|-|\n",
    "|Oui| CdeuxPooling105 | Autoencodeur sur image | 14| 3 |10|-|-|\n",
    "|Oui| CdeuxPooling106 | Autoencodeur sur image | 14| 2 |12|-|-|\n",
    "|Oui| CdeuxPooling107 | Autoencodeur sur image | 14| 3 |12|-|-|\n",
    "|Oui| CdeuxPooling108 | Autoencodeur sur image | 14| 2 |14|-|-|\n",
    "|Oui| CdeuxPooling109 | Autoencodeur sur image | 14| 3 |14|-|-|\n",
    "|Oui| CdeuxPooling110 | Autoencodeur sur image | 14| 2 |16|-|-|\n",
    "|Oui| CdeuxPooling111 | Autoencodeur sur image | 14| 3 |16|-|-|\n",
    "|Oui| CdeuxPooling112 | Autoencodeur sur image | 16| 2 |2|-|-|\n",
    "|Oui| CdeuxPooling113 | Autoencodeur sur image | 16| 3 |2|-|-|\n",
    "|Oui| CdeuxPooling114 | Autoencodeur sur image | 16| 2 |4|-|-|\n",
    "|Oui| CdeuxPooling115 | Autoencodeur sur image | 16| 3 |4|-|-|\n",
    "|Oui| CdeuxPooling116 | Autoencodeur sur image | 16| 2 |6|-|-|\n",
    "|Oui| CdeuxPooling117 | Autoencodeur sur image | 16| 3 |6|-|-|\n",
    "|Oui| CdeuxPooling118 | Autoencodeur sur image | 16| 2 |8|-|-|\n",
    "|Oui| CdeuxPooling119 | Autoencodeur sur image | 16| 3 |8|-|-|\n",
    "|Oui| CdeuxPooling120 | Autoencodeur sur image | 16| 2 |10|-|-|\n",
    "|Oui| CdeuxPooling121 | Autoencodeur sur image | 16| 3 |10|-|-|\n",
    "|Oui| CdeuxPooling122 | Autoencodeur sur image | 16| 2 |12|-|-|\n",
    "|Oui| CdeuxPooling123 | Autoencodeur sur image | 16| 3 |12|-|-|\n",
    "|Oui| CdeuxPooling124 | Autoencodeur sur image | 16| 2 |14|-|-|\n",
    "|Oui| CdeuxPooling125 | Autoencodeur sur image | 16| 3 |14|-|-|\n",
    "|Oui| CdeuxPooling126 | Autoencodeur sur image | 16| 2 |16|-|-|\n",
    "|Oui| CdeuxPooling127 | Autoencodeur sur image | 16| 3 |16|-|-|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 0\n",
    "c1,c2 = 2,2\n",
    "while c1 <= 16:\n",
    "  c2 = 2\n",
    "  while c2 <= 16:\n",
    "      print(\"Essai \"+str(i)+\" pour conv0 avec \" + str(c1) + \" et conv1 avec \" + str(c2) + \" et sans couche de pooling\")\n",
    "      print(str(i/127)+\"% réalisé\")\n",
    "      nom = 'EssaiCdeux'+str(i)+'avecC0_'+str(c1)+'_C1_'+str(c2)\n",
    "\n",
    "      tf.reset_default_graph()\n",
    "\n",
    "      root_logdir = \"tf_logs\"\n",
    "      logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "      with tf.Graph().as_default():\n",
    "          X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "          y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "          print(X.get_shape())\n",
    "\n",
    "\n",
    "          conv0 = tf.layers.conv2d(X, filters=3, kernel_size=c1,\n",
    "                  strides=1, padding='SAME',\n",
    " \t \t \t            activation=tf.nn.relu, name='conv0')\n",
    "          size0 = conv0.get_shape()\n",
    "          print('Shape of conv0 : '+str(size0))\n",
    "\n",
    "          conv1 = tf.layers.conv2d(conv0, filters=3, kernel_size=c2,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv1')\n",
    "          size1 = conv1.get_shape()\n",
    "          print('Shape of conv1 : '+str(size1))\n",
    "\n",
    "\n",
    "          with tf.name_scope('Optimizer'):\n",
    "              optimizer = tf.train.AdamOptimizer()\n",
    "          with tf.name_scope(\"phase0\"):\n",
    "              loss0 = tf.reduce_mean(tf.square(conv1 - y))\n",
    "              training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "          with tf.name_scope(\"init\"):\n",
    "              init = tf.global_variables_initializer()\n",
    "          with tf.name_scope(\"enregistrement\"):\n",
    "              loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "              kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
    "              bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
    "              kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
    "              bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "              \n",
    "              kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "              bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "              kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "              bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "              \n",
    "              print(os.path.split(X.name))\n",
    "              summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "              saver = tf.train.Saver()\n",
    "\n",
    "          with tf.name_scope('Avant-Apres_'+nom):\n",
    "              orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "              fin = tf.summary.image('output',tf.cast(conv1,dtype=tf.uint8))\n",
    "\n",
    "          n_epochs = 100\n",
    "          n_batches_per_epoch = 3\n",
    "          Lloss = []\n",
    "          def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "              global Lloss\n",
    "              if len(Lloss) < duree:\n",
    "                  Lloss.append(lastLoss)\n",
    "                  return False\n",
    "              else:\n",
    "                  Lloss = Lloss[1:duree] + [lastLoss]\n",
    "                  L = Lloss[:]\n",
    "                  L.sort(reverse=True)\n",
    "                  if L[0] <= seuil:\n",
    "                    if decroissance == True:\n",
    "                      return True if L == Lloss else False\n",
    "                    else:\n",
    "                      return True\n",
    "                  return False\n",
    "\n",
    "          import os\n",
    "          with tf.Session() as sess:\n",
    "              init.run()\n",
    "              for epoch in range(n_epochs):\n",
    "                  print(\"epoch : \",epoch)\n",
    "                  for iteration in range(n_batches_per_epoch):\n",
    "                      X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                      sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                      if iteration % 5 == 0:\n",
    "                        summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "                  if arret(loss_train0,100,6) == True:\n",
    "                    break\n",
    "              summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "              summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "              saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      summary_writer0.close()\n",
    "      c2 += 2\n",
    "      i += 1\n",
    "  c1 += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 12\n",
    "c1,c2 = 4,12\n",
    "while c1 <= 16:\n",
    "  c2 = 2\n",
    "  while c2 <= 16:\n",
    "    for p in (2,3):\n",
    "      print(\"Essai \"+str(i)+\" pour conv0 avec \" + str(c1) + \" et conv1 avec \" + str(c2) + \"et pour couche de pooling \"+str(p))\n",
    "      print(str(i/127)+\"% réalisé\")\n",
    "      nom = 'EssaiCdeuxPooling'+str(i)\n",
    "\n",
    "      tf.reset_default_graph()\n",
    "\n",
    "      root_logdir = \"tf_logs\"\n",
    "      logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "      with tf.Graph().as_default():\n",
    "          X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "          y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "          print(X.get_shape())\n",
    "\n",
    "\n",
    "          conv0 = tf.layers.conv2d(X, filters=3, kernel_size=c1,\n",
    "                  strides=1, padding='SAME',\n",
    " \t \t \t            activation=tf.nn.relu, name='conv0')\n",
    "          size0 = conv0.get_shape()\n",
    "          print('Shape of conv0 : '+str(size0))\n",
    "          maxPool0 = tf.layers.max_pooling2d(conv0, pool_size=(p,p), strides=(2,2), padding='same')\n",
    "          print('Shape of maxPool0 : '+str(maxPool0.get_shape()))\n",
    "\n",
    "          conv1 = tf.layers.conv2d(maxPool0, filters=3, kernel_size=c2,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv1')\n",
    "          size1 = conv1.get_shape()\n",
    "          print('Shape of conv1 : '+str(size1))\n",
    "          conv2 = tf.layers.conv2d(conv1, filters=3, kernel_size=c2,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv2')\n",
    "          print('Shape of conv2 : '+str(conv2.get_shape()))\n",
    "          resize0 = tf.image.resize_images(conv2, size=size0[1:3], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "          print('Shape of resize0 : '+str(resize0.get_shape()))\n",
    "          conv3 = tf.layers.conv2d(resize0, filters=3, kernel_size=c1,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv3')\n",
    "          print('Shape of conv3 : '+str(conv3.get_shape()))\n",
    "\n",
    "\n",
    "          with tf.name_scope('Optimizer'):\n",
    "              optimizer = tf.train.AdamOptimizer()\n",
    "          with tf.name_scope(\"phase0\"):\n",
    "              loss0 = tf.reduce_mean(tf.square(conv3 - y))\n",
    "              training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "          with tf.name_scope(\"init\"):\n",
    "              init = tf.global_variables_initializer()\n",
    "          with tf.name_scope(\"enregistrement\"):\n",
    "              loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "              kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
    "              bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
    "              kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
    "              bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "              \n",
    "              kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/kernel')[0]\n",
    "              bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/bias')[0]\n",
    "              kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/kernel')[0]\n",
    "              bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/bias')[0]\n",
    "              \n",
    "              kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "              bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "              kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "              bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "              kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
    "              bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
    "              kernel_saver3 = tf.summary.histogram(\"Kernel3\",kernel2)\n",
    "              bias_saver3 = tf.summary.histogram(\"Bias3\",bias2)\n",
    "              \n",
    "              print(os.path.split(X.name))\n",
    "              summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "              saver = tf.train.Saver()\n",
    "\n",
    "          with tf.name_scope('Avant-Apres_'+nom):\n",
    "              orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "              fin = tf.summary.image('output',tf.cast(conv1,dtype=tf.uint8))\n",
    "\n",
    "          n_epochs = 100\n",
    "          n_batches_per_epoch = 3\n",
    "          Lloss = []\n",
    "          def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "              global Lloss\n",
    "              if len(Lloss) < duree:\n",
    "                  Lloss.append(lastLoss)\n",
    "                  return False\n",
    "              else:\n",
    "                  Lloss = Lloss[1:duree] + [lastLoss]\n",
    "                  L = Lloss[:]\n",
    "                  L.sort(reverse=True)\n",
    "                  if L[0] <= seuil:\n",
    "                    if decroissance == True:\n",
    "                      return True if L == Lloss else False\n",
    "                    else:\n",
    "                      return True\n",
    "                  return False\n",
    "\n",
    "          import os\n",
    "          with tf.Session() as sess:\n",
    "              init.run()\n",
    "              for epoch in range(n_epochs):\n",
    "                  print(\"epoch : \",epoch)\n",
    "                  for iteration in range(n_batches_per_epoch):\n",
    "                      X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                      sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                      if iteration % 5 == 0:\n",
    "                        summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel3,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias3,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "                  if arret(loss_train0,100,6) == True:\n",
    "                    break\n",
    "              summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "              summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "              saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      summary_writer0.close()\n",
    "    c2 += 2\n",
    "    i += 1\n",
    "  c1 += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globalement sauf changement (actuellement l'entrainement en est à l'essai 9), le système reste très imprécis et très instable.</br>\n",
    "On ajoute une couche dense au milieu avec les couche de max_pooling déjà : même tableau que ci-dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 0\n",
    "c1,c2 = 2,2\n",
    "while c1 <= 16:\n",
    "  c2 = 2\n",
    "  while c2 <= 16:\n",
    "    for p in (2,3):\n",
    "      print(\"Essai \"+str(i)+\" pour conv0 avec \" + str(c1) + \" et conv1 avec \" + str(c2) + \"et pour couche de pooling \"+str(p))\n",
    "      print(str(i/127)+\"% réalisé\")\n",
    "      nom = 'EssaiCdeuxPoolingDense'+str(i)+'avecC0_'+str(c1)+'_C1_'+str(c2)+\"_p_\"+str(p)\n",
    "\n",
    "      tf.reset_default_graph()\n",
    "\n",
    "      root_logdir = \"tf_logs\"\n",
    "      logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "      with tf.Graph().as_default():\n",
    "          X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "          y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "          print(X.get_shape())\n",
    "\n",
    "\n",
    "          conv0 = tf.layers.conv2d(X, filters=3, kernel_size=c1,\n",
    "                  strides=1, padding='SAME',\n",
    " \t \t \t            activation=tf.nn.relu, name='conv0')\n",
    "          size0 = conv0.get_shape()\n",
    "          print('Shape of conv0 : '+str(size0))\n",
    "          maxPool0 = tf.layers.max_pooling2d(conv0, pool_size=(p,p), strides=(2,2), padding='same')\n",
    "          print('Shape of maxPool0 : '+str(maxPool0.get_shape()))\n",
    "\n",
    "          conv1 = tf.layers.conv2d(maxPool0, filters=3, kernel_size=c2,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv1')\n",
    "          size1 = conv1.get_shape()\n",
    "          print('Shape of conv1 : '+str(size1))\n",
    "          dense0 = tf.layers.dense(conv1,3)\n",
    "          print('Shape of dense0 : '+str(dense0.get_shape()))\n",
    "          conv2 = tf.layers.conv2d(dense0, filters=3, kernel_size=c2,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv2')\n",
    "          print('Shape of conv2 : '+str(conv2.get_shape()))\n",
    "          resize0 = tf.image.resize_images(conv2, size=size0[1:3], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "          print('Shape of resize0 : '+str(resize0.get_shape()))\n",
    "          conv3 = tf.layers.conv2d(resize0, filters=3, kernel_size=c1,\n",
    "                 strides=1, padding='SAME',\n",
    "                 activation=tf.nn.relu, name='conv3')\n",
    "          print('Shape of conv3 : '+str(conv3.get_shape()))\n",
    "\n",
    "\n",
    "          with tf.name_scope('Optimizer'):\n",
    "              optimizer = tf.train.AdamOptimizer()\n",
    "          with tf.name_scope(\"phase0\"):\n",
    "              loss0 = tf.reduce_mean(tf.square(conv3 - y))\n",
    "              training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "          with tf.name_scope(\"init\"):\n",
    "              init = tf.global_variables_initializer()\n",
    "          with tf.name_scope(\"enregistrement\"):\n",
    "              loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "              kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
    "              bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
    "              kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
    "              bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "              \n",
    "              kernelD = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/kernel')\n",
    "              biasD = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/bias')\n",
    "              \n",
    "              \n",
    "              kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/kernel')[0]\n",
    "              bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/bias')[0]\n",
    "              kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/kernel')[0]\n",
    "              bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/bias')[0]\n",
    "              \n",
    "              kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "              bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "              kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "              bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "              kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
    "              bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
    "              kernel_saver3 = tf.summary.histogram(\"Kernel3\",kernel2)\n",
    "              bias_saver3 = tf.summary.histogram(\"Bias3\",bias2)\n",
    "              kernel_saverD = tf.summary.histogram(\"Kernel3\",kernelD)\n",
    "              bias_saverD = tf.summary.histogram(\"Bias3\",biasD)\n",
    "              \n",
    "              print(os.path.split(X.name))\n",
    "              summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "              saver = tf.train.Saver()\n",
    "\n",
    "          with tf.name_scope('Avant-Apres_'+nom):\n",
    "              orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "              fin = tf.summary.image('output',tf.cast(conv1,dtype=tf.uint8))\n",
    "\n",
    "          n_epochs = 100\n",
    "          n_batches_per_epoch = 3\n",
    "          Lloss = []\n",
    "          def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "              global Lloss\n",
    "              if len(Lloss) < duree:\n",
    "                  Lloss.append(lastLoss)\n",
    "                  return False\n",
    "              else:\n",
    "                  Lloss = Lloss[1:duree] + [lastLoss]\n",
    "                  L = Lloss[:]\n",
    "                  L.sort(reverse=True)\n",
    "                  if L[0] <= seuil:\n",
    "                    if decroissance == True:\n",
    "                      return True if L == Lloss else False\n",
    "                    else:\n",
    "                      return True\n",
    "                  return False\n",
    "\n",
    "          import os\n",
    "          with tf.Session() as sess:\n",
    "              init.run()\n",
    "              for epoch in range(n_epochs):\n",
    "                  print(\"epoch : \",epoch)\n",
    "                  for iteration in range(n_batches_per_epoch):\n",
    "                      X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                      sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                      if iteration % 5 == 0:\n",
    "                        summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strKernelD = kernel_saverD.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_strBiasD = bias_saverD.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernel3,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBias3,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strKernelD,epoch*n_batches_per_epoch+iteration)\n",
    "                        summary_writer0.add_summary(summary_strBiasD,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "                  if arret(loss_train0,100,6) == True:\n",
    "                    break\n",
    "              summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "              summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "              saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      summary_writer0.close()\n",
    "    c2 += 2\n",
    "    i += 1\n",
    "  c1 += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant essayons de voir l'impact du nombre de couches sur la précision. On met une taille de noyaux croissant de 2 à 15. On met le nombre de filtre à 3 fixe pour le moment. \n",
    "Sans réduction de taille et couche intégralement connectée. </br>\n",
    "On fait varier le nombre de couches de 2 à 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Essai | Taille des noyaux |\n",
      "| --- | ---|\n",
      "| CtestCouches0 | [ 2 15] |\n",
      "| CtestCouches1 | [ 2  8 15] |\n",
      "| CtestCouches2 | [ 2  6 10 15] |\n",
      "| CtestCouches3 | [ 2  5  8 11 15] |\n",
      "| CtestCouches4 | [ 2  4  7  9 12 15] |\n",
      "| CtestCouches5 | [ 2  4  6  8 10 12 15] |\n",
      "| CtestCouches6 | [ 2  3  5  7  9 11 13 15] |\n",
      "| CtestCouches7 | [ 2  3  5  6  8 10 11 13 15] |\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"|Essai | Taille des noyaux |\n",
    "| --- | ---|\"\"\")\n",
    "i = 0\n",
    "import numpy as np\n",
    "for nb in range(2,10):\n",
    "    tailles = np.linspace(2,15,nb,dtype=np.int)\n",
    "    print(\"\"\"| CtestCouches\"\"\"+str(i)+\"\"\" | \"\"\"+str(tailles)+\"\"\" |\"\"\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Essai | Taille des noyaux |\n",
    "| --- | ---|\n",
    "| CtestCouches0 | [ 2 15] |\n",
    "| CtestCouches1 | [ 2  8 15] |\n",
    "| CtestCouches2 | [ 2  6 10 15] |\n",
    "| CtestCouches3 | [ 2  5  8 11 15] |\n",
    "| CtestCouches4 | [ 2  4  7  9 12 15] |\n",
    "| CtestCouches5 | [ 2  4  6  8 10 12 15] |\n",
    "| CtestCouches6 | [ 2  3  5  7  9 11 13 15] |\n",
    "| CtestCouches7 | [ 2  3  5  6  8 10 11 13 15] |\n",
    "| CtestCouches8 | [ 2  2  2  2  2  2  2  2  2] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "conv0 = tf.layers.conv2d(X, filters=3, kernel_size=2,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv0')\n",
      "print('Shape of conv0 : '+str(conv0.get_shape()))\n",
      "conv1 = tf.layers.conv2d(conv0, filters=3, kernel_size=6,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv1')\n",
      "print('Shape of conv1 : '+str(conv1.get_shape()))\n",
      "conv2 = tf.layers.conv2d(conv1, filters=3, kernel_size=10,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv2')\n",
      "print('Shape of conv2 : '+str(conv2.get_shape()))\n",
      "output = tf.layers.conv2d(conv2, filters=3, kernel_size=15,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv3')\n",
      "print('Shape of output : '+str(output.get_shape()))\n",
      "\n",
      "with tf.name_scope('Optimizer'):\n",
      "  optimizer = tf.train.AdamOptimizer()\n",
      "with tf.name_scope(\"phase0\"):\n",
      "  loss0 = tf.reduce_mean(tf.square(conv3 - y))\n",
      "  training_op0 = optimizer.minimize(loss0)\n",
      "\n",
      "with tf.name_scope(\"init\"):\n",
      "  init = tf.global_variables_initializer()\n",
      "with tf.name_scope(\"enregistrement\"):\n",
      "    loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
      "    kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
      "    bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
      "    kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
      "    bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
      "    kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/kernel')[0]\n",
      "    bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/bias')[0]\n",
      "    kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/kernel')[0]\n",
      "    bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/bias')[0]\n",
      "\n",
      "\n",
      "    kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
      "    bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
      "    kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
      "    bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
      "    kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
      "    bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
      "    kernel_saver3 = tf.summary.histogram(\"Kernel3\",kernel3)\n",
      "    bias_saver3 = tf.summary.histogram(\"Bias3\",bias3)\n",
      "\n",
      "\n",
      "    print(os.path.split(X.name))\n",
      "      summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
      "      saver = tf.train.Saver()\n",
      "\n",
      "    with tf.name_scope('Avant-Apres_'+nom):\n",
      "      orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
      "      fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
      "\n",
      "    n_epochs = 100\n",
      "    n_batches_per_epoch = 3\n",
      "    Lloss = []\n",
      "    def arret(lastLoss, seuil, duree, decroissance = False):\n",
      "      global Lloss\n",
      "      if len(Lloss) < duree:\n",
      "          Lloss.append(lastLoss)\n",
      "          return False\n",
      "      else:\n",
      "          Lloss = Lloss[1:duree] + [lastLoss]\n",
      "          L = Lloss[:]\n",
      "          L.sort(reverse=True)\n",
      "          if L[0] <= seuil:\n",
      "            if decroissance == True:\n",
      "              return True if L == Lloss else False\n",
      "            else:\n",
      "              return True\n",
      "          return False\n",
      "\n",
      "    import os\n",
      "    with tf.Session() as sess:\n",
      "      init.run()\n",
      "      for epoch in range(n_epochs):\n",
      "          print(\"epoch : \",epoch)\n",
      "          for iteration in range(n_batches_per_epoch):\n",
      "              X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
      "              sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
      "              if iteration % 5 == 0:\n",
      "                summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
      "\n",
      "                summary_strKernel0 = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_strBias0 = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_writer0.add_summary(summary_strKernel0,epoch*n_batches_per_epoch+iteration)\n",
      "                summary_writer0.add_summary(summary_strBias0,epoch*n_batches_per_epoch+iteration)\n",
      "                summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
      "                summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
      "                summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
      "                summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
      "                summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                summary_writer0.add_summary(summary_strKernel3,epoch*n_batches_per_epoch+iteration)\n",
      "                summary_writer0.add_summary(summary_strBias3,epoch*n_batches_per_epoch+iteration)\n",
      "          loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "          print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
      "          if arret(loss_train0,100,6) == True:\n",
      "              break\n",
      "      summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
      "      summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
      "      saver.save(sess, 'model/model'+nom+'.ckpt')\n"
     ]
    }
   ],
   "source": [
    "nb = 4\n",
    "nbResize = 5\n",
    "plage = 2,15\n",
    "plageResize = 3,15\n",
    "inputSize = 399\n",
    "import numpy as np\n",
    "chaineTf = ''\n",
    "seprateur = '///'\n",
    "def convTf(indice,filtre,noyau,chaineTf,first=False,last=False):\n",
    "    chaineTf += seprateur\n",
    "    global lastName\n",
    "    if last == True:\n",
    "        chaineTf += 'output'\n",
    "#         lastName = 'output'\n",
    "    else:\n",
    "        chaineTf += \"conv\"+str(indice)\n",
    "    chaineTf += \" = tf.layers.conv2d(\"\n",
    "    if first == True:\n",
    "        chaineTf += \"X\"\n",
    "    else:\n",
    "        chaineTf += lastName\n",
    "    chaineTf+= \", filters=\"+str(filtre)+\", kernel_size=\"+str(noyau)+\",\\n \\t \\t \\t strides=1, padding='SAME',\\n \\t \\t \\t activation=tf.nn.relu, name='conv\"+str(indice)+\"')\"\n",
    "    lastName = \"conv\"+str(indice)\n",
    "    if last == True:\n",
    "        lastName = 'output'\n",
    "    chaineTf += seprateur+\"print('Shape of \"+lastName+\" : '+str(\"+lastName+\".get_shape()))\"\n",
    "    return chaineTf\n",
    "    \n",
    "# filtre = np.linspace(plage[0],plage[1],nb//2+1 if nb % 2 == 1 else nb//2,dtype=np.int)\n",
    "# noyaux = np.linspace(plage[1],plage[0],nb//2+1 if nb % 2 == 1 else nb//2,dtype=np.int)\n",
    "# filtre = np.concatenate((filtre,np.flip(filtre,axis=0) if nb % 2 == 0 else np.flip(filtre[:-1],axis=0)),axis=0).tolist()\n",
    "# noyaux = np.concatenate((noyaux,np.flip(noyaux,axis=0) if nb % 2 == 0 else np.flip(noyaux[:-1],axis=0)),axis=0).tolist()\n",
    "filtre = np.linspace(3,3,nb,dtype=np.int)\n",
    "noyaux = np.linspace(plage[0],plage[1],nb,dtype=np.int)\n",
    "layers = [3 for i in range(15)]\n",
    "print()\n",
    "nom = \"\"\n",
    "last = ''\n",
    "lastName = 'X'\n",
    "currentSize = inputSize\n",
    "chaineTf = convTf(0,filtre[0],noyaux[0],chaineTf,first=True)\n",
    "\n",
    "for i in range(1,len(filtre)-1):\n",
    "    chaineTf = convTf(i,filtre[i],noyaux[i],chaineTf)\n",
    "chaineTf = convTf(len(filtre)-1,filtre[-1],noyaux[-1],chaineTf,last=True)\n",
    "chaineTf = chaineTf.split(seprateur)\n",
    "for elem in chaineTf:\n",
    "    print(elem)\n",
    "print(\"\"\"\n",
    "with tf.name_scope('Optimizer'):\n",
    "  optimizer = tf.train.AdamOptimizer()\n",
    "with tf.name_scope(\"phase0\"):\n",
    "  loss0 = tf.reduce_mean(tf.square(conv3 - y))\n",
    "  training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "with tf.name_scope(\"init\"):\n",
    "  init = tf.global_variables_initializer()\n",
    "with tf.name_scope(\"enregistrement\"):\n",
    "    loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\"\"\",end='')\n",
    "for i in range(len(chaineTf)//2-1):\n",
    "    print(\"\"\"\n",
    "    kernel\"\"\"+str(i)+\"\"\" = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv\"\"\"+str(i)+\"\"\"/kernel')[0]\n",
    "    bias\"\"\"+str(i)+\"\"\" = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv\"\"\"+str(i)+\"\"\"/bias')[0]\"\"\",end='')\n",
    "\n",
    "print(\"\"\"\n",
    "    kernel\"\"\"+str(len(chaineTf)//2-1)+\"\"\" = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/kernel')[0]\n",
    "    bias\"\"\"+str(len(chaineTf)//2-1)+\"\"\" = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/bias')[0]\"\"\")\n",
    "print()\n",
    "for i in range(len(chaineTf)//2):\n",
    "    print(\"\"\"\n",
    "    kernel_saver\"\"\"+str(i)+\"\"\" = tf.summary.histogram(\"Kernel\"\"\"+str(i)+\"\"\"\",kernel\"\"\"+str(i)+\"\"\")\n",
    "    bias_saver\"\"\"+str(i)+\"\"\" = tf.summary.histogram(\"Bias\"\"\"+str(i)+\"\"\"\",bias\"\"\"+str(i)+\"\"\")\"\"\",end='')\n",
    "    \n",
    "print()\n",
    "print(\"\"\"\n",
    "\n",
    "    print(os.path.split(X.name))\n",
    "      summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "    with tf.name_scope('Avant-Apres_'+nom):\n",
    "      orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "      fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    Lloss = []\n",
    "    def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "      global Lloss\n",
    "      if len(Lloss) < duree:\n",
    "          Lloss.append(lastLoss)\n",
    "          return False\n",
    "      else:\n",
    "          Lloss = Lloss[1:duree] + [lastLoss]\n",
    "          L = Lloss[:]\n",
    "          L.sort(reverse=True)\n",
    "          if L[0] <= seuil:\n",
    "            if decroissance == True:\n",
    "              return True if L == Lloss else False\n",
    "            else:\n",
    "              return True\n",
    "          return False\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "      init.run()\n",
    "      for epoch in range(n_epochs):\n",
    "          print(\"epoch : \",epoch)\n",
    "          for iteration in range(n_batches_per_epoch):\n",
    "              X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "              sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "              if iteration % 5 == 0:\n",
    "                summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\"\"\")\n",
    "for i in range(len(chaineTf)//2):\n",
    "    print(\"\"\"\n",
    "                summary_strKernel\"\"\"+str(i)+\"\"\" = kernel_saver\"\"\"+str(i)+\"\"\".eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                summary_strBias\"\"\"+str(i)+\"\"\" = bias_saver\"\"\"+str(i)+\"\"\".eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                summary_writer0.add_summary(summary_strKernel\"\"\"+str(i)+\"\"\",epoch*n_batches_per_epoch+iteration)\n",
    "                summary_writer0.add_summary(summary_strBias\"\"\"+str(i)+\"\"\",epoch*n_batches_per_epoch+iteration)\"\"\",end='')\n",
    "print(\"\"\"\n",
    "          loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "          print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "          if arret(loss_train0,100,6) == True:\n",
    "              break\n",
    "      summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "      summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "      saver.save(sess, 'model/model'+nom+'.ckpt')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 8\n",
    "print(\"Essai avec 8 couches\")\n",
    "nom = 'CtestCouchesFiltres_10filtres_'+str(i)+'_'+str(2+i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    conv0 = tf.layers.conv2d(X, filters=3, kernel_size=2,\n",
    " \t \t \t strides=1, padding='SAME',\n",
    " \t \t \t activation=tf.nn.relu, name='conv0')\n",
    "    print('Shape of conv0 : '+str(conv0.get_shape()))\n",
    "    conv1 = tf.layers.conv2d(conv0, filters=3, kernel_size=2,\n",
    "       strides=1, padding='SAME',\n",
    "       activation=tf.nn.relu, name='conv1')\n",
    "    print('Shape of conv1 : '+str(conv1.get_shape()))\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=9, kernel_size=2,\n",
    " \t \t \t strides=1, padding='SAME',\n",
    " \t \t \t activation=tf.nn.relu, name='conv2')\n",
    "    print('Shape of conv2 : '+str(conv2.get_shape()))\n",
    "    conv3 = tf.layers.conv2d(conv2, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv3')\n",
    "    print('Shape of conv3 : '+str(conv3.get_shape()))\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv4')\n",
    "    print('Shape of conv4 : '+str(conv4.get_shape()))\n",
    "    conv5 = tf.layers.conv2d(conv4, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv5')\n",
    "    print('Shape of conv5 : '+str(conv5.get_shape()))\n",
    "    conv6 = tf.layers.conv2d(conv5, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv6')\n",
    "    print('Shape of conv6 : '+str(conv6.get_shape()))\n",
    "    conv7 = tf.layers.conv2d(conv6, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv7')\n",
    "    print('Shape of conv7 : '+str(conv7.get_shape()))\n",
    "    output = tf.layers.conv2d(conv7, filters=3, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='output')\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    \n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "      optimizer = tf.train.AdamOptimizer()\n",
    "    with tf.name_scope(\"phase0\"):\n",
    "      loss0 = tf.reduce_mean(tf.square(output - y))\n",
    "      training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "      init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "      loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "      kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
    "      bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
    "      kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
    "      bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "      kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/kernel')[0]\n",
    "      bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/bias')[0]\n",
    "      kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/kernel')[0]\n",
    "      bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/bias')[0]\n",
    "      kernel4 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv4/kernel')[0]\n",
    "      bias4 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv4/bias')[0]\n",
    "      kernel5 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv5/kernel')[0]\n",
    "      bias5 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv5/bias')[0]\n",
    "      kernel6 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv6/kernel')[0]\n",
    "      bias6 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv6/bias')[0]\n",
    "      kernel7 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv7/kernel')[0]\n",
    "      bias7 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv7/bias')[0]\n",
    "      kernel8 = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/kernel')[0]\n",
    "      bias8 = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/bias')[0]\n",
    "\n",
    "\n",
    "      kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "      bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "      kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "      bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "      kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
    "      bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
    "      kernel_saver3 = tf.summary.histogram(\"Kernel3\",kernel3)\n",
    "      bias_saver3 = tf.summary.histogram(\"Bias3\",bias3)\n",
    "      kernel_saver4 = tf.summary.histogram(\"Kernel4\",kernel4)\n",
    "      bias_saver4 = tf.summary.histogram(\"Bias4\",bias4)\n",
    "      kernel_saver5 = tf.summary.histogram(\"Kernel5\",kernel5)\n",
    "      bias_saver5 = tf.summary.histogram(\"Bias5\",bias5)\n",
    "      kernel_saver6 = tf.summary.histogram(\"Kernel6\",kernel6)\n",
    "      bias_saver6 = tf.summary.histogram(\"Bias6\",bias6)\n",
    "      kernel_saver7 = tf.summary.histogram(\"Kernel3\",kernel7)\n",
    "      bias_saver7 = tf.summary.histogram(\"Bias3\",bias7)\n",
    "      kernel_saver8 = tf.summary.histogram(\"Kernel8\",kernel8)\n",
    "      bias_saver8 = tf.summary.histogram(\"Bias8\",bias8)\n",
    "\n",
    "      print(os.path.split(X.name))\n",
    "      summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "\n",
    "      n_epochs = 100\n",
    "      n_batches_per_epoch = 3\n",
    "      Lloss = []\n",
    "      def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "        global Lloss\n",
    "        if len(Lloss) < duree:\n",
    "            Lloss.append(lastLoss)\n",
    "            return False\n",
    "        else:\n",
    "            Lloss = Lloss[1:duree] + [lastLoss]\n",
    "            L = Lloss[:]\n",
    "            L.sort(reverse=True)\n",
    "            if L[0] <= seuil:\n",
    "              if decroissance == True:\n",
    "                return True if L == Lloss else False\n",
    "              else:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "      import os\n",
    "      with tf.Session() as sess:\n",
    "          init.run()\n",
    "          for epoch in range(n_epochs):\n",
    "              print(\"epoch : \",epoch)\n",
    "              for iteration in range(n_batches_per_epoch):\n",
    "                  X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                  sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                  if iteration % 5 == 0:\n",
    "                    summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                    summary_strKernel0 = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias0 = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel0,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias0,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel3,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias3,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel4 = kernel_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias4 = bias_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel4,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias4,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel5 = kernel_saver5.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias5 = bias_saver5.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel5,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias5,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel6 = kernel_saver6.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias6 = bias_saver6.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel6,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias6,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel7 = kernel_saver7.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias7 = bias_saver7.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel7,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias7,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel8 = kernel_saver8.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias8 = bias_saver8.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel8,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias8,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "              loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "              print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "              if arret(loss_train0,100,6) == True:\n",
    "                  break\n",
    "          summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "          summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "          saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme cela ne change toujours rien, le nombre de filtre fixé à trois est peut-être la cause de cette imprécision subsistante. Donc test avec plus de filtres au milieu en sachant qu'il faut impérativement 3 filtres au début et à la fin pour correspondre aux 3 canaux RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 8\n",
    "print(\"Essai avec 8 couches\")\n",
    "nom = 'CtestCouchesFiltres_10filtres_'+str(i)+'_'+str(2+i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    conv0 = tf.layers.conv2d(X, filters=3, kernel_size=2,\n",
    " \t \t \t strides=1, padding='SAME',\n",
    " \t \t \t activation=tf.nn.relu, name='conv0')\n",
    "    print('Shape of conv0 : '+str(conv0.get_shape()))\n",
    "    conv1 = tf.layers.conv2d(conv0, filters=3, kernel_size=2,\n",
    "       strides=1, padding='SAME',\n",
    "       activation=tf.nn.relu, name='conv1')\n",
    "    print('Shape of conv1 : '+str(conv1.get_shape()))\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=9, kernel_size=2,\n",
    " \t \t \t strides=1, padding='SAME',\n",
    " \t \t \t activation=tf.nn.relu, name='conv2')\n",
    "    print('Shape of conv2 : '+str(conv2.get_shape()))\n",
    "    conv3 = tf.layers.conv2d(conv2, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv3')\n",
    "    print('Shape of conv3 : '+str(conv3.get_shape()))\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv4')\n",
    "    print('Shape of conv4 : '+str(conv4.get_shape()))\n",
    "    conv5 = tf.layers.conv2d(conv4, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv5')\n",
    "    print('Shape of conv5 : '+str(conv5.get_shape()))\n",
    "    conv6 = tf.layers.conv2d(conv5, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv6')\n",
    "    print('Shape of conv6 : '+str(conv6.get_shape()))\n",
    "    conv7 = tf.layers.conv2d(conv6, filters=9, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv7')\n",
    "    print('Shape of conv7 : '+str(conv7.get_shape()))\n",
    "    output = tf.layers.conv2d(conv7, filters=3, kernel_size=2,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='output')\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    \n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "      optimizer = tf.train.AdamOptimizer()\n",
    "    with tf.name_scope(\"phase0\"):\n",
    "      loss0 = tf.reduce_mean(tf.square(output - y))\n",
    "      training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "      init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "      loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "      kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
    "      bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
    "      kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
    "      bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "      kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/kernel')[0]\n",
    "      bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/bias')[0]\n",
    "      kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/kernel')[0]\n",
    "      bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/bias')[0]\n",
    "      kernel4 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv4/kernel')[0]\n",
    "      bias4 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv4/bias')[0]\n",
    "      kernel5 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv5/kernel')[0]\n",
    "      bias5 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv5/bias')[0]\n",
    "      kernel6 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv6/kernel')[0]\n",
    "      bias6 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv6/bias')[0]\n",
    "      kernel7 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv7/kernel')[0]\n",
    "      bias7 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv7/bias')[0]\n",
    "      kernel8 = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/kernel')[0]\n",
    "      bias8 = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/bias')[0]\n",
    "\n",
    "\n",
    "      kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "      bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "      kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "      bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "      kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
    "      bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
    "      kernel_saver3 = tf.summary.histogram(\"Kernel3\",kernel3)\n",
    "      bias_saver3 = tf.summary.histogram(\"Bias3\",bias3)\n",
    "      kernel_saver4 = tf.summary.histogram(\"Kernel4\",kernel4)\n",
    "      bias_saver4 = tf.summary.histogram(\"Bias4\",bias4)\n",
    "      kernel_saver5 = tf.summary.histogram(\"Kernel5\",kernel5)\n",
    "      bias_saver5 = tf.summary.histogram(\"Bias5\",bias5)\n",
    "      kernel_saver6 = tf.summary.histogram(\"Kernel6\",kernel6)\n",
    "      bias_saver6 = tf.summary.histogram(\"Bias6\",bias6)\n",
    "      kernel_saver7 = tf.summary.histogram(\"Kernel3\",kernel7)\n",
    "      bias_saver7 = tf.summary.histogram(\"Bias3\",bias7)\n",
    "      kernel_saver8 = tf.summary.histogram(\"Kernel8\",kernel8)\n",
    "      bias_saver8 = tf.summary.histogram(\"Bias8\",bias8)\n",
    "\n",
    "      print(os.path.split(X.name))\n",
    "      summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "\n",
    "      n_epochs = 100\n",
    "      n_batches_per_epoch = 3\n",
    "      Lloss = []\n",
    "      def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "        global Lloss\n",
    "        if len(Lloss) < duree:\n",
    "            Lloss.append(lastLoss)\n",
    "            return False\n",
    "        else:\n",
    "            Lloss = Lloss[1:duree] + [lastLoss]\n",
    "            L = Lloss[:]\n",
    "            L.sort(reverse=True)\n",
    "            if L[0] <= seuil:\n",
    "              if decroissance == True:\n",
    "                return True if L == Lloss else False\n",
    "              else:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "      import os\n",
    "      with tf.Session() as sess:\n",
    "          init.run()\n",
    "          for epoch in range(n_epochs):\n",
    "              print(\"epoch : \",epoch)\n",
    "              for iteration in range(n_batches_per_epoch):\n",
    "                  X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                  sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                  if iteration % 5 == 0:\n",
    "                    summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                    summary_strKernel0 = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias0 = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel0,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias0,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel3,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias3,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel4 = kernel_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias4 = bias_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel4,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias4,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel5 = kernel_saver5.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias5 = bias_saver5.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel5,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias5,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel6 = kernel_saver6.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias6 = bias_saver6.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel6,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias6,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel7 = kernel_saver7.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias7 = bias_saver7.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel7,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias7,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "                    summary_strKernel8 = kernel_saver8.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_strBias8 = bias_saver8.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    summary_writer0.add_summary(summary_strKernel8,epoch*n_batches_per_epoch+iteration)\n",
    "                    summary_writer0.add_summary(summary_strBias8,epoch*n_batches_per_epoch+iteration)\n",
    "                    \n",
    "              loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "              print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "              if arret(loss_train0,100,6) == True:\n",
    "                  break\n",
    "          summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "          summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "          saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour visualiser l'impact des couches denses seules, on ajoute prograssivement des couches denses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 2\n",
    "print(\"Essai avec \"+str(i)+\" couches\")\n",
    "nom = 'Cdense_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    \n",
    "    dense0 = tf.layers.dense(X,3,name='dense0')\n",
    "    print('Shape of dense0 : '+str(dense0.get_shape()))\n",
    "    output = tf.layers.dense(dense0,3,name='dense1')\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    \n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "      optimizer = tf.train.AdamOptimizer()\n",
    "    with tf.name_scope(\"phase0\"):\n",
    "      loss0 = tf.reduce_mean(tf.square(output - y))\n",
    "      training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "      init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "      loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "      kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/kernel')[0]\n",
    "      bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/bias')[0]\n",
    "      kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense1/kernel')[0]\n",
    "      bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense1/bias')[0]\n",
    "\n",
    "\n",
    "      kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "      bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "      kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "      bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "\n",
    "      print(os.path.split(X.name))\n",
    "      summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    Lloss = []\n",
    "    def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "      global Lloss\n",
    "      if len(Lloss) < duree:\n",
    "          Lloss.append(lastLoss)\n",
    "          return False\n",
    "      else:\n",
    "          Lloss = Lloss[1:duree] + [lastLoss]\n",
    "          L = Lloss[:]\n",
    "          L.sort(reverse=True)\n",
    "          if L[0] <= seuil:\n",
    "            if decroissance == True:\n",
    "              return True if L == Lloss else False\n",
    "            else:\n",
    "              return True\n",
    "          return False\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel0 = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias0 = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel0,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias0,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "            loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "            if arret(loss_train0,100,6) == True:\n",
    "                break\n",
    "        summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 couches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : erreur ds le code à la première exécution (pas pris la bonne couche en paramètre de la suivante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 3\n",
    "print(\"Essai avec \"+str(i)+\" couches\")\n",
    "nom = 'Cdense_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    \n",
    "    dense0 = tf.layers.dense(X,3,name='dense0')\n",
    "    print('Shape of dense0 : '+str(dense0.get_shape()))\n",
    "    dense1 = tf.layers.dense(dense0,3,name='dense1')\n",
    "    print('Shape of dense1 : '+str(dense1.get_shape()))\n",
    "    output = tf.layers.dense(dense1,3,name='dense2')\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    \n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "      optimizer = tf.train.AdamOptimizer()\n",
    "    with tf.name_scope(\"phase0\"):\n",
    "      loss0 = tf.reduce_mean(tf.square(output - y))\n",
    "      training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "      init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "      loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "      kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/kernel')[0]\n",
    "      bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/bias')[0]\n",
    "      kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense1/kernel')[0]\n",
    "      bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense1/bias')[0]\n",
    "      kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense2/kernel')[0]\n",
    "      bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense2/bias')[0]\n",
    "\n",
    "\n",
    "      kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "      bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "      kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "      bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "      kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
    "      bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
    "\n",
    "      print(os.path.split(X.name))\n",
    "      summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    Lloss = []\n",
    "    def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "      global Lloss\n",
    "      if len(Lloss) < duree:\n",
    "          Lloss.append(lastLoss)\n",
    "          return False\n",
    "      else:\n",
    "          Lloss = Lloss[1:duree] + [lastLoss]\n",
    "          L = Lloss[:]\n",
    "          L.sort(reverse=True)\n",
    "          if L[0] <= seuil:\n",
    "            if decroissance == True:\n",
    "              return True if L == Lloss else False\n",
    "            else:\n",
    "              return True\n",
    "          return False\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel0 = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias0 = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel0,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias0,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "                  \n",
    "                  summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "            loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "            if arret(loss_train0,100,6) == True:\n",
    "                break\n",
    "        summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 4\n",
    "print(\"Essai avec \"+str(i)+\" couches\")\n",
    "nom = 'Cdense_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    \n",
    "    dense0 = tf.layers.dense(X,3,name='dense0')\n",
    "    print('Shape of dense0 : '+str(dense0.get_shape()))\n",
    "    dense1 = tf.layers.dense(dense0,3,name='dense1')\n",
    "    print('Shape of dense1 : '+str(dense1.get_shape()))\n",
    "    dense2 = tf.layers.dense(dense1,3,name='dense2')\n",
    "    print('Shape of dense2 : '+str(dense2.get_shape()))\n",
    "    output = tf.layers.dense(dense2,3,name='dense3')\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    \n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "      optimizer = tf.train.AdamOptimizer()\n",
    "    with tf.name_scope(\"phase0\"):\n",
    "      loss0 = tf.reduce_mean(tf.square(output - y))\n",
    "      training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "      init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "      loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "      kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/kernel')[0]\n",
    "      bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/bias')[0]\n",
    "      kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense1/kernel')[0]\n",
    "      bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense1/bias')[0]\n",
    "      kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense2/kernel')[0]\n",
    "      bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense2/bias')[0]\n",
    "      kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense3/kernel')[0]\n",
    "      bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense3/bias')[0]\n",
    "\n",
    "\n",
    "      kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "      bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "      kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "      bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "      kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
    "      bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
    "      kernel_saver3 = tf.summary.histogram(\"Kernel3\",kernel3)\n",
    "      bias_saver3 = tf.summary.histogram(\"Bias3\",bias3)\n",
    "\n",
    "      print(os.path.split(X.name))\n",
    "      summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    Lloss = []\n",
    "    def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "      global Lloss\n",
    "      if len(Lloss) < duree:\n",
    "          Lloss.append(lastLoss)\n",
    "          return False\n",
    "      else:\n",
    "          Lloss = Lloss[1:duree] + [lastLoss]\n",
    "          L = Lloss[:]\n",
    "          L.sort(reverse=True)\n",
    "          if L[0] <= seuil:\n",
    "            if decroissance == True:\n",
    "              return True if L == Lloss else False\n",
    "            else:\n",
    "              return True\n",
    "          return False\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel0 = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias0 = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel0,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias0,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "                  \n",
    "                  summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
    "                  \n",
    "                  summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel3,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias3,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "            loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "            if arret(loss_train0,100,6) == True:\n",
    "                break\n",
    "        summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme cela semble ne rien changer : même les couches denses la précision ne reste pas de façon stable en-dessous de 500 cela doit provenir de la base de donnée.</br> \n",
    "Le problème doit provenir d'un lot pas assez diversifié et des images 'trop statique' : les images sont déjà réalisées, et en nombre pas forcément assez improtant. De plus, les réglages photoshop ont été peut-être trop poussés. Pour tenter d'en améliorer la qualité, plusieurs solution: </br>\n",
    "<ol>\n",
    "    <li>Rotation de l'image de départ puis crop dans l'image résultant</li>\n",
    "    <li>Réduction dimensions de l'image</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solution générale serait d'appliquer cela directement dans le notebook colab. On fera ces modifications aléatoirement et on comptera sur cela pour assurer la différence entre les diférents lots (entrainement, vérification et test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des images sources (noms de fichiers numéros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "noises = []\n",
    "\n",
    "i = 1\n",
    "while os.path.isfile(\"Images_source/clean/\"+str(i)+\".jpg\"):\n",
    "    images.append(\"Images_source/clean/\"+str(i)+\".jpg\")\n",
    "    i += 1\n",
    "\n",
    "i = 1\n",
    "while os.path.isfile(\"Images_source/noise/\"+str(i)+\".jpg\"):\n",
    "    noises.append(\"Images_source/noise/\"+str(i)+\".jpg\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée une fonction qui renvoie l'image créée en :</br>\n",
    "<ol>\n",
    "    <li>Tirant une image aléatoirement une image dans le dossier des images source</li>\n",
    "    <li>Appliquant :</br>\n",
    "    <ol>\n",
    "    <li>Rotation libre</li>\n",
    "    <li>Modification de taille aléatoire mais de façon à tjrs obtenir une image de dimensions attendues en sortie (image carrée tjrs) dépendant de la modification de rotation</li>\n",
    "    </ol>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def next_batch(batch_size, noises, images,tailleAttendue):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    print(\"Size of {}\".format(len(noises)))\n",
    "    noiseTensor = []\n",
    "    imageTensor = []\n",
    "    for i in range(batch_size):\n",
    "      choix = np.random.randint(0,len(images))#choix aléatoire de l'image\n",
    "      image = cv2.imread(images[choix],1)#Ouvre en rgb l'image nettoyée\n",
    "      noise = cv2.imread(noises[choix],1)#Ouvre en rgb l'image bruitée\n",
    "      angle = np.random.randint(0,90)\n",
    "      taille = np.random.randint(int(tailleAttendue*2**0.5), image.shape[0]+1)# Racine de deux pour pouvoir toujours récupérer la taille désirée\n",
    "\n",
    "      resizedImage = cv2.resize(image,(taille,taille))\n",
    "      resizedNoise = cv2.resize(noise,(taille,taille))\n",
    "      \n",
    "      rows,cols = resizedImage.shape[:2]\n",
    "      \n",
    "      centre = (np.random.randint(taille//2,rows-taille//2) if taille//2<rows-taille//2 else taille//2,np.random.randint(taille//2,cols-taille//2) if taille//2<cols-taille//2 else taille//2)\n",
    "      M = cv2.getRotationMatrix2D(centre,angle,1)\n",
    "\n",
    "      rotatedImage = cv2.warpAffine(resizedImage,M,(cols,rows))\n",
    "      rotatedNoise = cv2.warpAffine(resizedNoise,M,(cols,rows))\n",
    "\n",
    "      resultImage = rotatedImage[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
    "      resultNoise = rotatedNoise[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
    "\n",
    "      noiseTensor.append(np.array(resultNoise,np.uint8))\n",
    "      imageTensor.append(np.array(resultImage,np.uint8))\n",
    "    return np.array(noiseTensor), np.array(imageTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fait le nombre on peut ajouter des couches en augmentant le nombre de neurones au milieu du réseau. Cela permet de mieux faire converger le modèle : `EssaiLotRemaniePlusNeurones`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essai avec 5 couches\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-770e3c0fecc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'EssaiLotRemaniePlusNeurones'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mroot_logdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tf_logs\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 5\n",
    "print(\"Essai avec \"+str(i)+\" couches\")\n",
    "nom = 'EssaiLotRemaniePlusNeurones'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    \n",
    "    dense0 = tf.layers.dense(X,3,name='dense0')\n",
    "    print('Shape of dense0 : '+str(dense0.get_shape()))\n",
    "    dense1 = tf.layers.dense(dense0,3,name='dense1')\n",
    "    print('Shape of dense1 : '+str(dense1.get_shape()))\n",
    "    dense2 = tf.layers.dense(dense1,100,name='dense2')\n",
    "    print('Shape of dense2 : '+str(dense2.get_shape()))\n",
    "    dense3 = tf.layers.dense(dense2,100,name='dense3')\n",
    "    print('Shape of dense3 : '+str(dense3.get_shape()))\n",
    "    output = tf.layers.dense(dense3,3,name='dense4')\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    \n",
    "\n",
    "    with tf.name_scope('Optimizer'):\n",
    "      optimizer = tf.train.AdamOptimizer()\n",
    "    with tf.name_scope(\"phase0\"):\n",
    "      loss0 = tf.reduce_mean(tf.square(output - y))\n",
    "      training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "      init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "      loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "      kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/kernel')[0]\n",
    "      bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense0/bias')[0]\n",
    "      kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense1/kernel')[0]\n",
    "      bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense1/bias')[0]\n",
    "      kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense2/kernel')[0]\n",
    "      bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense2/bias')[0]\n",
    "      kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense3/kernel')[0]\n",
    "      bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense3/bias')[0]\n",
    "      kernel4 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense4/kernel')[0]\n",
    "      bias4 = tf.get_collection(tf.GraphKeys.VARIABLES, 'dense4/bias')[0]\n",
    "\n",
    "\n",
    "      kernel_saver0 = tf.summary.histogram(\"Kernel0\",kernel0)\n",
    "      bias_saver0 = tf.summary.histogram(\"Bias0\",bias0)\n",
    "      kernel_saver1 = tf.summary.histogram(\"Kernel1\",kernel1)\n",
    "      bias_saver1 = tf.summary.histogram(\"Bias1\",bias1)\n",
    "      kernel_saver2 = tf.summary.histogram(\"Kernel2\",kernel2)\n",
    "      bias_saver2 = tf.summary.histogram(\"Bias2\",bias2)\n",
    "      kernel_saver3 = tf.summary.histogram(\"Kernel3\",kernel3)\n",
    "      bias_saver3 = tf.summary.histogram(\"Bias3\",bias3)\n",
    "      kernel_saver4 = tf.summary.histogram(\"Kernel4\",kernel4)\n",
    "      bias_saver4 = tf.summary.histogram(\"Bias4\",bias4)\n",
    "\n",
    "      print(os.path.split(X.name))\n",
    "      summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    Lloss = []\n",
    "    def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "      global Lloss\n",
    "      if len(Lloss) < duree:\n",
    "          Lloss.append(lastLoss)\n",
    "          return False\n",
    "      else:\n",
    "          Lloss = Lloss[1:duree] + [lastLoss]\n",
    "          L = Lloss[:]\n",
    "          L.sort(reverse=True)\n",
    "          if L[0] <= seuil:\n",
    "            if decroissance == True:\n",
    "              return True if L == Lloss else False\n",
    "            else:\n",
    "              return True\n",
    "          return False\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, noises,images,height)\n",
    "                sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel0 = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias0 = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel0,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias0,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "                  \n",
    "                  summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel2,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias2,epoch*n_batches_per_epoch+iteration)\n",
    "                  \n",
    "                  summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel3,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias3,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                  summary_strKernel4 = kernel_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_strBias4 = bias_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_strKernel4,epoch*n_batches_per_epoch+iteration)\n",
    "                  summary_writer0.add_summary(summary_strBias4,epoch*n_batches_per_epoch+iteration)\n",
    "                  \n",
    "            loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "            if arret(loss_train0,100,6) == True:\n",
    "                break\n",
    "        summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les <b>couches de convolution</b></br>\n",
    "Après avoir visionné cette vidéo <a href='https://www.youtube.com/watch?v=YRhxdVk_sIs&index=7&list=PLSbuiZgaaqsKQf2zuihSpALqH5xQd8b4c&t=0s'>Convolutional Neural Networks (CNNs) explained</a> il apparait que le redimensionnement peut poser des problèmes par rapport à l'apprentissage : en effet, le cercle de diamètre différent de chaque étoile variera suivant la taille de la région de l'image considérée, d'autant plus si on la redimensionne aléatoirement (NB : quelques images de départ ont déjà été redimensionnées pour obtenir des zones exploitables). </br>\n",
    "Cela permettra néanmoins d'avoir un réseau plus polyvalent et moins sensible aux artéfacts de retouche manuelle. Il serait alors nécessaire d'afficher les caractéritiques apprises par chaque couche pour déterminer ce qui est détecté. </br>\n",
    "De plus, à la suite de cette vidéo, il apparait que les filtres correspondent aux nombre de formes détectables par la couche de convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de lancer la réalisation de ce modèle, d'après les articles <a href='https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f'>GANs from Scratch 1: A deep introduction</a> et <a href='https://towardsdatascience.com/generative-adversarial-networks-using-tensorflow-c8f4518406df'>Generative Adversarial Networks using Tensorflow</a>, il apparait que le type de réseau de neurone que je cherche à réaliser s'appelle un <a href='https://fr.wikipedia.org/wiki/R%C3%A9seaux_antagonistes_g%C3%A9n%C3%A9ratifs'>Réseaux antagonistes génératifs</a>. D'après <a href='https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/gan.py'>cet exemple</a> qui illustre le code générale pour réaliser ce type de réseau --> réalisation du nouveau code. Chercher également à clarifier le code et la création du nécessaire pour l'analyse des couches grâce à un ensemble de fonctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, la version `GAN_essai_1`, la version 0 ayant servie uniquement à mettre en place le système de fonction, à faire correctement la conversion entre différents types d'objets pour permettre l'enregistrement de toutes les variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter que dans ce type de model, la fonction de coût utilise un log ce qui pose des problèmes de divergences ce qui a nécessité un plancher minimum pour ne pas atteindre 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def next_batch(batch_size, noises, images,tailleAttendue):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "#     print(\"Size of {}\".format(len(noises)))\n",
    "    noiseTensor = []\n",
    "    imageTensor = []\n",
    "#     import matplotlib.pyplot as plt\n",
    "    for i in range(batch_size):\n",
    "      choix = np.random.randint(0,len(images))#choix aléatoire de l'image\n",
    "      image = cv2.imread(images[choix],1)#Ouvre en rgb l'image nettoyée\n",
    "      noise = cv2.imread(noises[choix],1)#Ouvre en rgb l'image bruitée\n",
    "      angle = np.random.randint(0,90)\n",
    "      taille = np.random.randint(int(tailleAttendue*2**0.5), image.shape[0]+1)# Racine de deux pour pouvoir toujours récupérer la taille désirée\n",
    "\n",
    "      resizedImage = cv2.resize(image,(taille,taille))\n",
    "      resizedNoise = cv2.resize(noise,(taille,taille))\n",
    "      \n",
    "      rows,cols = resizedImage.shape[:2]\n",
    "#       print('CentreRow : ')\n",
    "#       print((taille//2,rows-taille//2))\n",
    "#       print('CentreCols : ')\n",
    "#       print((taille//2,cols-taille//2))\n",
    "#       tmp = taille//2\n",
    "      \n",
    "      centre = (np.random.randint(taille//2,rows-taille//2) if taille//2<rows-taille//2 else taille//2,np.random.randint(taille//2,cols-taille//2) if taille//2<cols-taille//2 else taille//2)\n",
    "      M = cv2.getRotationMatrix2D(centre,angle,1)\n",
    "\n",
    "      rotatedImage = cv2.warpAffine(resizedImage,M,(cols,rows))\n",
    "      rotatedNoise = cv2.warpAffine(resizedNoise,M,(cols,rows))\n",
    "#       print('Rotation : '+str(rotatedImage.shape))\n",
    "\n",
    "      resultImage = rotatedImage[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
    "      resultNoise = rotatedNoise[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
    "#       print('Crop : '+str(resultImage.shape))\n",
    "#       plt.imshow(resultImage)\n",
    "\n",
    "#       noiseTensor.append(np.array(resultNoise,np.uint8))\n",
    "#       imageTensor.append(np.array(resultImage,np.uint8))\n",
    "#       print(imageTensor)\n",
    "#     plt.show()\n",
    "    return np.array(noiseTensor,np.uint8), np.array(imageTensor,np.uint8)\n",
    "# next_batch(3,noises,images,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 1\n",
    "nom = 'GAN_essai_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "    print(\"Taille de l'entrée : \"+str(X.get_shape()))\n",
    "    def denseLayer(input,nbNeurones,nom,start=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if start==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    def generator(input,neurones):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'denseGenerator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'denseGenerator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'denseGenerator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(input,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"disc_input\")\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generator(gen_input,[100,100,100])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminator(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,[100,100,100],'Faux')\n",
    "    \n",
    "    gen_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_faux,1e-10,10)))\n",
    "    disc_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_vrai,1e-10,10)) + tf.log(tf.clip_by_value(1. - disc_faux,1e-10,10)))#Attention peut tendre vers -\n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    disc_vars = disc_vars1+disc_vars2\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        init.run()\n",
    "        for i in range(1,251):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,_,gl,dl = sess.run([train_gen,train_disc,gen_loss,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          if i % 5 == 0:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour analyser au mieux les valeurs de coût affichées, revoir la vidéo <a href='https://youtu.be/JikwRMXJzk0?list=PLSbuiZgaaqsKQf2zuihSpALqH5xQd8b4c'>Les réseaux adversariaux (GAN) | Intelligence artificielle 49</a>. En effet les fonctions de coût pour les `discriminator` et le `generator` sont générées à partir de log mais de façons différentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_faux,1e-10,10)))\n",
    "disc_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_vrai,1e-10,10)) + tf.log(tf.clip_by_value(1. - disc_faux,1e-10,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question à traiter également :</br>\n",
    "(En sachant que le générateur a à l'origine pour rôle de créer des vrais/fausses images à l'origine (revoir la vidéo pour préciser ce rôle))</br>\n",
    "<b>Le générateur doit-il sortir une image en pseudo-rgb ?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regarder aussi : <a href='https://youtu.be/dXB-KQYkzNU'>vidéo sur la normalisation du batch</a> (discutttée en fin de la vidéo <a href='https://youtu.be/JikwRMXJzk0?list=PLSbuiZgaaqsKQf2zuihSpALqH5xQd8b4c'>Les réseaux adversariaux (GAN) | Intelligence artificielle 49</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code modifié :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def next_batch(batch_size, noises, images,tailleAttendue):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "#     print(\"Size of {}\".format(len(noises)))\n",
    "    noiseTensor = []\n",
    "    imageTensor = []\n",
    "#     import matplotlib.pyplot as plt\n",
    "    for i in range(batch_size):\n",
    "      choix = np.random.randint(0,len(images))#choix aléatoire de l'image\n",
    "      image = cv2.imread(images[choix],1)#Ouvre en rgb l'image nettoyée\n",
    "      noise = cv2.imread(noises[choix],1)#Ouvre en rgb l'image bruitée\n",
    "      angle = np.random.randint(0,90)\n",
    "      taille = np.random.randint(int(tailleAttendue*2**0.5), image.shape[0]+1)# Racine de deux pour pouvoir toujours récupérer la taille désirée\n",
    "\n",
    "      resizedImage = cv2.resize(image,(taille,taille))\n",
    "      resizedNoise = cv2.resize(noise,(taille,taille))\n",
    "      \n",
    "      rows,cols = resizedImage.shape[:2]\n",
    "#       print('CentreRow : ')\n",
    "#       print((taille//2,rows-taille//2))\n",
    "#       print('CentreCols : ')\n",
    "#       print((taille//2,cols-taille//2))\n",
    "#       tmp = taille//2\n",
    "      \n",
    "      centre = (np.random.randint(taille//2,rows-taille//2) if taille//2<rows-taille//2 else taille//2,np.random.randint(taille//2,cols-taille//2) if taille//2<cols-taille//2 else taille//2)\n",
    "      M = cv2.getRotationMatrix2D(centre,angle,1)\n",
    "\n",
    "      rotatedImage = cv2.warpAffine(resizedImage,M,(cols,rows))\n",
    "      rotatedNoise = cv2.warpAffine(resizedNoise,M,(cols,rows))\n",
    "#       print('Rotation : '+str(rotatedImage.shape))\n",
    "\n",
    "      resultImage = rotatedImage[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
    "      resultNoise = rotatedNoise[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
    "#       print('Crop : '+str(resultImage.shape))\n",
    "#       plt.imshow(resultImage)\n",
    "\n",
    "      noiseTensor.append(np.array(resultNoise,np.uint8))\n",
    "      imageTensor.append(np.array(resultImage,np.uint8))\n",
    "#       print(imageTensor)\n",
    "#     plt.show()\n",
    "    noiseTensor,imageTensor = np.array(noiseTensor,np.uint8), np.array(imageTensor,np.uint8)\n",
    "    return noiseTensor, imageTensor\n",
    "# next_batch(3,noises,images,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 1\n",
    "nom = 'GAN_essai_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "    def denseLayer(input,nbNeurones,nom,start=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if start==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    def generator(input,neurones):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'denseGenerator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'denseGenerator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'denseGenerator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(input,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"disc_input\")\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generator(gen_input,[100,100,100])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminator(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,[100,100,100],'Faux')\n",
    "    \n",
    "    gen_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_faux,1e-10,10)))\n",
    "    disc_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_vrai,1e-10,10)) + tf.log(tf.clip_by_value(1. - disc_faux,1e-10,10)))#Attention peut tendre vers -\n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    disc_vars = disc_vars1+disc_vars2\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        init.run()\n",
    "        for i in range(1,251):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,_,gl,dl = sess.run([train_gen,train_disc,gen_loss,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          if i % 5 == 0:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat : problèmes d'enregistrement : seule l'entrée sortie du générateur est affichée. On note également qu'entre 2 essais (un où l'enregistrement a échoué (pour une raison inconnue)) si la fonction de coût du générateur arrive à la valeur limitée pour éviter de tendre vers 0, le générateur reste bloqué à ce stade et n'apprend plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 0\n",
    "nom = 'GAN_essai_test'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "    def denseLayer(input,nbNeurones,nom,start=False,generator=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if start==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    def generator(input,neurones):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'denseGenerator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'denseGenerator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'denseGenerator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(input,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"disc_input\")\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generator(gen_input,[100,100,100])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminator(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,[100,100,100],'Faux')\n",
    "    \n",
    "    gen_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_faux,1e-10,10)))\n",
    "    disc_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_vrai,1e-10,10)) + tf.log(tf.clip_by_value(1. - disc_faux,1e-10,10)))#Attention peut tendre vers -\n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    disc_vars = disc_vars1+disc_vars2\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        init.run()\n",
    "        for i in range(1,6):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,_,gl,dl = sess.run([train_gen,train_disc,gen_loss,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          if i % 5 == 0:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_vrai_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous affiche bien les images mais donne des résultats très colorés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après la vidéo <a href='https://youtu.be/0VPQHbMvGzg'>Generative Adversarial Networks (LIVE)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb moyen de sélectionner les variables à entrainer : <a href='https://youtu.be/0VPQHbMvGzg?t=3147'>ici</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminateur : Encodeur</br>\n",
    "<ul>\n",
    "    <li>Doit déterminer si la probabilité que l'image en entrée soit une vraie image ou une fausse</li>\n",
    "    <li>Opération d'encodage de l'information :\n",
    "        <ul style=\"list-style-type:&rarr\">\n",
    "            <li>Tester la présence d'un objet</li>\n",
    "            <li>Pas besoin de le savoir en fonction de la position de l'objet</li>\n",
    "            <li>Convolution</li>\n",
    "            <li>Finit par couche intégralement connectée car après qu'on ait détecter les caractéristiques, il faut les mettre en relation</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générateur : Décodeur\n",
    "<ul>\n",
    "    <li>Part normalement d'une gaussienne</li>\n",
    "    <li>Fait plein de petites images</li>\n",
    "    <li>Convolution</li>\n",
    "    <li>Normalisation<br><img src=\"https://cdn-images-1.medium.com/max/800/1*Hiq-rLFGDpESpr8QNsJ1jg.png\" filter='invert(1)' -webkit-filter='invert(1)'>\n",
    "    </li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectifs : \n",
    "<ol>\n",
    "    <li>Fonction pour ajouter ajout couche de convolution</li>\n",
    "    <li>Fonction pour sauvegarde et réentrainement</li>\n",
    "    <li>Implémenter DiscoGan</li>\n",
    "    <li>Modifier entrainement pour entrainer plus de fois le générateur que le discriminateur</li>\n",
    "    <li>Modifier façon dont récupère variables pour entrainement</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN_essai_test3 : Essai à nouveau avec le réseau</br>\n",
    "GAN_essai_test4 : change fonction de coût : enlève le log ; batch_normalization ; <a href='https://github.com/llSourcell/Pokemon_GAN/blob/master/pokeGAN.py'>source : Generating Pokemon with a Generative Adversarial Network</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 4\n",
    "nom = 'GAN_essai_test'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "    def denseLayer(input,nbNeurones,nom,start=False,generator=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if start==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    def convLayer(input,kernel,filters,start=False,generator=False):\n",
    "      couche = tf.layers.conv2d(input, filters=filters, kernel_size=kernel,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if start==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    def generator(input,neurones):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'denseGenerator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'denseGenerator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "          if i != 0 and i%1 == 0:\n",
    "            couche = tf.contrib.layers.batch_norm(couche, is_training=True, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='normalization_'+str(i))\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'denseGenerator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(input,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generator(gen_input,[100,100,100])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminator(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,[100,100,100],'Faux')\n",
    "    \n",
    "#     gen_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_faux,1e-10,10)))\n",
    "#     disc_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(disc_vrai,1e-10,10)) + tf.log(tf.clip_by_value(1. - disc_faux,1e-10,10)))#Attention peut tendre vers -\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux) - tf.reduce_mean(disc_vrai)  # This optimizes the discriminator.\n",
    "    gen_loss = -tf.reduce_mean(disc_faux)  # This optimizes the generator\n",
    "  \n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    disc_vars = disc_vars1+disc_vars2\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        init.run()\n",
    "        for i in range(1,50):\n",
    "          for _ in range(6):#Entraine générateur\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,gl = sess.run([train_gen,gen_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          for _ in range(1):#Entraine discriminateur\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          if i % 5 == 0:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "          if i % 5 == 10:\n",
    "            tf_saver.save(sess,'/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_vrai_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas de convergence : cela doit provenir de l'incohérence du système : en effet, le générateur est censé être un encodeur donc réseau neuronal avec des couches de convolution pour réduire la dimensionnalité des données, et les discriminateurs les décodeurs. Il se pose également la question de l'influence de l'influence de la normalisation par batch : est-ce cela qui permet d'avoir une courbe d'avoir une courbe d'apprentissage plus 'lisse' ou bien est-ce la structure même de GAN avec discriminateur et générateur qui induit cela. Et surtout est-ce vraiment la question comme le modèle n'est pas adapté... Le problème au niveau du log pouvait également provenir du fait qu'on travaillait avec des entier non signés sur 8 bits : le réseau n'avait eut-être pas assez d'amplitude pour séparer les éléments importants du bruit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en place de ce modèle qui plante à cause des couches de convolution de noyaux trop grands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 6\n",
    "nom = 'GAN_essai_test_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,generator=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if start==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='normalization_'+nom[-1])\n",
    "      if poolingType == 'mean':\n",
    "        couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "      else:\n",
    "        couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if start==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if end == True else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def generator(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'denseGenerator0',True)\n",
    "      saver = [kernel_saver,bias_saver]\n",
    "      kernelBiasList = [kernel,bias]\n",
    "      sortiesImages = [sortieImage]\n",
    "#       for i in range(1,len(kernels)-1):\n",
    "#         couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'denseGenerator'+str(i),end=True)\n",
    "#         kernelBiasList.append(kernel)\n",
    "#         kernelBiasList.append(bias)\n",
    "#         saver.append(kernel_saver)\n",
    "#         saver.append(bias_saver)\n",
    "#         sortiesImages.append(sortieImage)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'denseGenerator'+str(len(pooling)+1))\n",
    "      kernelBiasList.append(kernel)\n",
    "      kernelBiasList.append(bias)\n",
    "      saver.append(kernel_saver)\n",
    "      saver.append(bias_saver)\n",
    "      sortiesImages.append(sortieImage)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(input,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,neurones[0],'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    \n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generator(gen_input,[55,27,13],[96,256,384],[3,3,1],['mean','mean','mean'])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminator(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,[384,100,100],'Faux')\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux) - tf.reduce_mean(disc_vrai)  # This optimizes the discriminator.\n",
    "    gen_loss = -tf.reduce_mean(disc_faux)  # This optimizes the generator\n",
    "  \n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    disc_vars = disc_vars1+disc_vars2\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(1,50):\n",
    "          for _ in range(6):#Entraine générateur\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,gl = sess.run([train_gen,gen_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          for _ in range(1):#Entraine discriminateur\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          if i % 2 == 0:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "          if i % 5 == 10:\n",
    "            tf_saver.save(sess,'/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_vrai_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'où démultiplication suivant la note sur les couches de convolution du livre sur tensorflow : 2 couches de 3*3 équivalent à 1 couche de 9*9. Ici comme la source de ce principe n'a pas été éclaircie mais que globalement on perçoit la raison de ce principe, on spécifiera manuellement la démultiplication quitte à faire après une fonction renvoit la liste de démultiplication automatiquement..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 3, 3], [3, 3, 3], [2, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "a = [[2]+3*[3],3*[3],2*[2]+[3]]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60,  70,  80,  90, 101, 111, 121, 131, 142, 152, 162, 173, 183,\n",
       "       193, 203, 214, 224, 234, 244, 255, 265, 275, 286, 296, 306, 316,\n",
       "       327, 337, 347, 357, 368, 378, 388, 399])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linspace(60,399,34,dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter une tentative avec reshape de tensorflow mais comme pour numpy il garde le même nombre d'éléments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai sans TPU ou GPU. Parfosi le problème vient de là... A noter que dès le début 4Gb de RAM sont occupés par le modèle. La session plante toujours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai en travaillant avec des images de 199 et non 399 --> Fonctionne ; on relance en essai 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 7\n",
    "nom = 'GAN_essai_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(input,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    #generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[55,27,13],[96,256,384],[3,3,1],['mean','mean','mean'])\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminatorDense(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,np.linspace(100,199,7,dtype=np.int),np.linspace(150,3,7,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    disc_loss = tf.reduce_mean(disc_faux-disc_input)  # This optimizes the discriminator.\n",
    "    gen_loss = -tf.reduce_mean(disc_faux)  # This optimizes the generator\n",
    "  \n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    disc_vars = disc_vars2 + disc_vars1\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(1,5):\n",
    "          print('Generateur')\n",
    "          for a in range(6):#Entraine générateur\n",
    "            print('Iteration' +str(a),end=', ')\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,gl = sess.run([train_gen,gen_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print('Discriminateurs')\n",
    "          for _ in range(1):#Entraine discriminateur\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          if i % 1 == 0:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "          if i % 5 == 10:\n",
    "            tf_saver.save(sess,'/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_vrai_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Taille de gen_input : (7, 199, 199, 3)</br>\n",
    "Taille de convGenerator0_split_0 : (7, 199, 199, 50)</br>\n",
    "Taille de convGenerator0_split_1 : (7, 199, 199, 50)</br>\n",
    "Taille de convGenerator1_split_0 : (7, 100, 100, 100)</br>\n",
    "Taille de convGenerator2_split_0 : (7, 100, 100, 150)</br>\n",
    "Taille de convGenerator2_split_1 : (7, 100, 100, 150)</br>\n",
    "Taille de denseVraiDiscriminator0 : (7, 199, 199, 3)</br>\n",
    "Taille de denseVraiDiscriminator1 : (7, 199, 199, 100)</br>\n",
    "Taille de denseVraiDiscriminator2 : (7, 199, 199, 100)</br>\n",
    "Taille de denseVraiDiscriminator3 : (7, 199, 199, 100)</br>\n",
    "Taille de denseVraiDiscriminator4 : (7, 199, 199, 3)</br>\n",
    "Taille de l'entrée du discriminateur : (7, 50, 50, 150)</br>\n",
    "Taille du noyau de deconvolution de deconvDiscriminatorFaux0 : 51</br>\n",
    "Taille de deconvDiscriminatorFaux0 : (7, 100, 100, 150)</br>\n",
    "Taille du noyau de deconvolution de deconvDiscriminatorFaux1 : 17</br>\n",
    "Taille de deconvDiscriminatorFaux1 : (7, 116, 116, 125)</br>\n",
    "Taille du noyau de deconvolution de deconvDiscriminatorFaux2 : 18</br>\n",
    "Taille de deconvDiscriminatorFaux2 : (7, 133, 133, 101)</br>\n",
    "Taille du noyau de deconvolution de deconvDiscriminatorFaux3 : 17</br>\n",
    "Taille de deconvDiscriminatorFaux3 : (7, 149, 149, 76)</br>\n",
    "Taille du noyau de deconvolution de deconvDiscriminatorFaux4 : 18</br>\n",
    "Taille de deconvDiscriminatorFaux4 : (7, 166, 166, 52)</br>\n",
    "Taille du noyau de deconvolution de deconvDiscriminatorFaux5 : 17</br>\n",
    "Taille de deconvDiscriminatorFaux5 : (7, 182, 182, 27)</br>\n",
    "Taille du noyau de deconvolution de deconvDiscriminatorFaux6 : 18</br>\n",
    "Taille de deconvDiscriminatorFaux6 : (7, 199, 199, 3)</br>\n",
    "Taille de sortie disc_faux : (7, 199, 199, 3)</br>\n",
    "Taille de sortie disc_input : (7, 199, 199, 3)</br>\n",
    "Generator variables : ...</br>\n",
    "Trainable variables : ...</br>\n",
    "Entrainement....</br>\n",
    "Generateur</br>\n",
    "Iteration0, Iteration1, Iteration2, Iteration3, Iteration4, Iteration5, Discriminateurs</br>\n",
    "Step 1: Generator Loss: -0.000001, Discriminator Loss: -11.519882</br>\n",
    "Generateur</br>\n",
    "Iteration0, Iteration1, Iteration2, Iteration3, Iteration4, Iteration5, Discriminateurs</br>\n",
    "Step 2: Generator Loss: -0.000005, Discriminator Loss: -13.513853</br>\n",
    "Generateur</br>\n",
    "Iteration0, Iteration1, Iteration2, Iteration3, Iteration4, Iteration5, Discriminateurs</br>\n",
    "Step 3: Generator Loss: -0.000002, Discriminator Loss: -21.686550</br>\n",
    "Generateur</br>\n",
    "Iteration0, Iteration1, Iteration2, Iteration3, Iteration4, Iteration5, Discriminateurs</br>\n",
    "Step 4: Generator Loss: -0.000005, Discriminator Loss: -18.946047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visiblement, je me suis trompé dans les priorité d'entrainement : le générateur est trop précis et le discriminateur pas assez.</br>\n",
    "En conséquence, on ajuste au fur et à mesure de la façon suivante : </br>\n",
    "Avec générateur et discriminateur on doit toujours avoir 10 entrainement par etape d'entrainement</br>\n",
    "On pondère en fonction le nombre d'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'où l'essai 9 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 9\n",
    "nom = 'GAN_essai_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(input,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    #generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[55,27,13],[96,256,384],[3,3,1],['mean','mean','mean'])\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminatorDense(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,np.linspace(100,199,7,dtype=np.int),np.linspace(150,3,7,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux-disc_input)  # This optimizes the discriminator.\n",
    "    gen_loss = -tf.reduce_mean(disc_faux)  # This optimizes the generator\n",
    "  \n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    disc_vars = disc_vars2 + disc_vars1\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        init.run()\n",
    "        print('Entrainement....')\n",
    "        lastDisc, lastGen = 1,1\n",
    "        for i in range(1,5):\n",
    "          nbEntrainementGenerateur,nbEntrainementDiscriminateur = 5,5\n",
    "          if lastDisc > lastGen:\n",
    "            nbEntrainementGenerateur = 5 + 4-int(4*lastGen/lastDisc)\n",
    "            nbEntrainementDiscriminateur = 10 - nbEntrainementGenerateur\n",
    "          elif lastGen > lastDisc:\n",
    "            nbEntrainementDiscriminateur = 5 + 4-int(4*lastDisc/lastGen)\n",
    "            nbEntrainementGenerateur = 10 - nbEntrainementDiscriminateur\n",
    "          print('Generateur : ',end='')\n",
    "          for a in range(nbEntrainementGenerateur):#Entraine générateur\n",
    "            print('Iteration ' +str(a),end=', ')\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,gl = sess.run([train_gen,gen_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print()\n",
    "          print('Discriminateur : ',end='')\n",
    "          for _ in range(nbEntrainementDiscriminateur):#Entraine discriminateur\n",
    "            print('Iteration ' +str(a),end=', ')\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print()\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "          lastDisc, lastGen = dl,gl\n",
    "          if i % 5 == 10:\n",
    "            tf_saver.save(sess,'/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_vrai_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais à cause du manque d'iteration le modèle n'a pas renvoyé d'image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On augmente le nombre d'iteration et pour tenter d'améliorer la précision on augmente la zone d'image analysée. Faisons un essai pour voir ce que donne l'image même si la fonction de coût qui minimisera par la suite la moyenne de l'image de sortie a un comportement obscur... Elle permet peut-être de ne pas avoir de valeurs trop extrèmes. D'où l'essai 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def next_batch(batch_size, noises, images,tailleAttendue):#ATTENTION : pr tenter d'améliorer l'apprentissage, on augmente la taille minimale d'image prise\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "#     print(\"Size of {}\".format(len(noises)))\n",
    "    noiseTensor = []\n",
    "    imageTensor = []\n",
    "#     import matplotlib.pyplot as plt\n",
    "    for i in range(batch_size):\n",
    "      choix = np.random.randint(0,len(images))#choix aléatoire de l'image\n",
    "      image = cv2.imread(images[choix],1)#Ouvre en rgb l'image nettoyée\n",
    "      noise = cv2.imread(noises[choix],1)#Ouvre en rgb l'image bruitée\n",
    "      angle = np.random.randint(0,90)\n",
    "      taille = np.random.randint(int(tailleAttendue*2**0.5)*4, image.shape[0]+1)# Racine de deux pour pouvoir toujours récupérer la taille désirée\n",
    "      ##ATENTION : ci-dessus peut porter à erreur si dépasse image.shape[0]+1\n",
    "      resizedImage = cv2.resize(image,(taille,taille))\n",
    "      resizedNoise = cv2.resize(noise,(taille,taille))\n",
    "      \n",
    "      rows,cols = resizedImage.shape[:2]\n",
    "#       print('CentreRow : ')\n",
    "#       print((taille//2,rows-taille//2))\n",
    "#       print('CentreCols : ')\n",
    "#       print((taille//2,cols-taille//2))\n",
    "#       tmp = taille//2\n",
    "      \n",
    "      centre = (np.random.randint(taille//2,rows-taille//2) if taille//2<rows-taille//2 else taille//2,np.random.randint(taille//2,cols-taille//2) if taille//2<cols-taille//2 else taille//2)\n",
    "      M = cv2.getRotationMatrix2D(centre,angle,1)\n",
    "\n",
    "      rotatedImage = cv2.warpAffine(resizedImage,M,(cols,rows))\n",
    "      rotatedNoise = cv2.warpAffine(resizedNoise,M,(cols,rows))\n",
    "#       print('Rotation : '+str(rotatedImage.shape))\n",
    "\n",
    "      resultImage = rotatedImage[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
    "      resultNoise = rotatedNoise[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
    "#       print('Crop : '+str(resultImage.shape))\n",
    "#       plt.imshow(resultImage)\n",
    "\n",
    "      noiseTensor.append(np.array(resultNoise,np.uint8))\n",
    "      imageTensor.append(np.array(resultImage,np.uint8))\n",
    "#       print(imageTensor)\n",
    "#     plt.show()\n",
    "    noiseTensor,imageTensor = np.array(noiseTensor,np.float32), np.array(imageTensor,np.float32)\n",
    "    return noiseTensor, imageTensor\n",
    "# next_batch(3,noises,images,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 10\n",
    "nom = 'GAN_essai_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(input,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(input,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    #generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[55,27,13],[96,256,384],[3,3,1],['mean','mean','mean'])\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminatorDense(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,np.linspace(100,199,7,dtype=np.int),np.linspace(150,3,7,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux-disc_input)  # This optimizes the discriminator.\n",
    "    gen_loss = -tf.reduce_mean(disc_faux)  # This optimizes the generator\n",
    "  \n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    disc_vars = disc_vars2 + disc_vars1\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        init.run()\n",
    "        print('Entrainement....')\n",
    "        lastDisc, lastGen = 1,1\n",
    "        for i in range(1,5):\n",
    "          nbEntrainementGenerateur,nbEntrainementDiscriminateur = 1,9# Pour favoriser le discriminateur d'emblée\n",
    "          if lastDisc > lastGen:\n",
    "            nbEntrainementDiscriminateur = 5 + 4-int(4*lastGen/lastDisc)\n",
    "            nbEntrainementGenerateur = 10 - nbEntrainementDiscriminateur\n",
    "          elif lastGen > lastDisc:\n",
    "            nbEntrainementGenerateur = 5 + 4-int(4*lastDisc/lastGen)\n",
    "            nbEntrainementDiscriminateur = 10 - nbEntrainementGenerateur\n",
    "          print('Generateur : ',end='')\n",
    "          for a in range(nbEntrainementGenerateur):#Entraine générateur\n",
    "            print('Iteration ' +str(a),end=', ')\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,gl = sess.run([train_gen,gen_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print()\n",
    "          print('Discriminateur : ',end='')\n",
    "          for b in range(nbEntrainementDiscriminateur):#Entraine discriminateur\n",
    "            print('Iteration ' +str(b),end=', ')\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print()\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "          lastDisc, lastGen = abs(dl),abs(gl)\n",
    "          if i % 5 == 10:\n",
    "            tf_saver.save(sess,'/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_vrai_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 11\n",
    "nom = 'GAN_essai_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(inpt,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    #generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[55,27,13],[96,256,384],[3,3,1],['mean','mean','mean'])\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    disc_vrai,disc_vars1,disc_saver1,disc_vrai_sortiesImages = discriminatorDense(disc_input,[100,100,100],'Vrai')\n",
    "    disc_faux,disc_vars2,disc_saver2,disc_faux_sortiesImages = discriminator(generator,np.linspace(100,199,7,dtype=np.int),np.linspace(150,3,7,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux-disc_input)  # This optimizes the discriminator.\n",
    "    gen_loss = -tf.reduce_mean(disc_faux)  # This optimizes the generator\n",
    "  \n",
    "    loss_saver_gen = tf.summary.scalar(\"Generator_loss\",gen_loss)\n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",gen_loss)\n",
    "    \n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    disc_vars = disc_vars2 + disc_vars1\n",
    "    saver = gen_saver + disc_saver2 + disc_saver1\n",
    "    print(\"Generator variables : \"+str(gen_vars))\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "#         init.run()\n",
    "        tf_saver.restore(sess, 'model/model'+nom+'.ckpt')\n",
    "        print('Entrainement....')\n",
    "        lastDisc, lastGen = 1,1\n",
    "        for i in range(6,50):\n",
    "          nbEntrainementGenerateur,nbEntrainementDiscriminateur = 1,9# Pour favoriser le discriminateur d'emblée\n",
    "          if lastDisc > lastGen:\n",
    "            nbEntrainementDiscriminateur = 5 + 4-int(4*lastGen/lastDisc)\n",
    "            nbEntrainementGenerateur = 10 - nbEntrainementDiscriminateur\n",
    "          elif lastGen > lastDisc:\n",
    "            nbEntrainementGenerateur = 5 + 4-int(4*lastDisc/lastGen)\n",
    "            nbEntrainementDiscriminateur = 10 - nbEntrainementGenerateur\n",
    "          print('Generateur : ',end='')\n",
    "          for a in range(nbEntrainementGenerateur):#Entraine générateur\n",
    "            print('Iteration ' +str(a),end=', ')\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,gl = sess.run([train_gen,gen_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print()\n",
    "          print('Discriminateur : ',end='')\n",
    "          for b in range(nbEntrainementDiscriminateur):#Entraine discriminateur\n",
    "            print('Iteration ' +str(b),end=', ')\n",
    "            gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "            _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print()\n",
    "          summary_str = loss_saver_gen.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(gen_saver,gen_inpt,disc_inpt)\n",
    "          save(disc_saver1,gen_inpt,disc_inpt)\n",
    "          save(disc_saver2,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "          lastDisc, lastGen = abs(dl),abs(gl)\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_vrai_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme l'apprentissage du générateur dégrade plutôt l'image en lui imposant une image aussi lisse que possible, on revient sur un fonctionnement d'autoencodeur mais qui sera cette fois plus adapté car comportant des couches de déconvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après plus de 50 iterations en fait problème d'enregistrement. Après modifications on tente à nouveau : essai 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "forceInit = True\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 13\n",
    "nom = 'GAN_essai_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print('precDimension : '+str(precDimension))\n",
    "#           print('outputSize : '+str(outputSize))\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(inpt,3,'dense'+str(ID)+'Discriminator0',True)\n",
    "        saver = [kernel_saver,bias_saver]\n",
    "        kernelBiasList = [kernel,bias]\n",
    "        sortiesImages = [sortieImage]\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'Discriminator'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        \n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,3,'dense'+str(ID)+'Discriminator'+str(len(neurones)+1))\n",
    "        kernelBiasList.append(kernel)\n",
    "        kernelBiasList.append(bias)\n",
    "        saver.append(kernel_saver)\n",
    "        saver.append(bias_saver)\n",
    "        sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    print(generator.get_shape())\n",
    "    nbCouchesDsicriminateur = 15\n",
    "    disc_faux,disc_vars,disc_saver,disc_faux_sortiesImages = discriminator(generator,np.linspace(int(generator.get_shape()[1]),199,nbCouchesDsicriminateur+1,dtype=np.int)[1:],np.linspace(int(generator.get_shape()[-1]),3,nbCouchesDsicriminateur,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux-disc_input)  # This optimizes the discriminator.\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",disc_loss)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    saver =  disc_saver\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_disc = optimizer_disc.minimize(disc_loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('./checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('./checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('./checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14]) > lastTrained:\n",
    "              lastTrained = int(elem[14])\n",
    "          tf_saver.restore(sess, 'checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(0,51):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print()\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(disc_saver,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après cet essai on ajoute des couches intégralement connectées au milieu pour améliorer le traitement 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "forceInit = True\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 14\n",
    "nom = 'GAN_essai_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "      \n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    couche,kernelBiasList,saver,sortiesImages = discriminatorDense(generator,[150,300,150],'Lien')\n",
    "    nbCouchesDsicriminateur = 15\n",
    "    disc_faux,disc_vars,disc_saver,disc_faux_sortiesImages = discriminator(generator,np.linspace(int(generator.get_shape()[1]),199,nbCouchesDsicriminateur+1,dtype=np.int)[1:],np.linspace(int(generator.get_shape()[-1]),3,nbCouchesDsicriminateur,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux-disc_input)  # This optimizes the discriminator.\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",disc_loss)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    saver =  disc_saver\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_disc = optimizer_disc.minimize(disc_loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('./checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('./checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('./checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14]) > lastTrained:\n",
    "              lastTrained = int(elem[14])\n",
    "          tf_saver.restore(sess, 'checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(0,101):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          print()\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(disc_saver,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rien ne converge. Pour l'essai 15_C et 15_N on essai de reproduire avec la structure du 14 l'image clean ou l'image noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but serait de mettre en place l'architecture suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"D:/Github/StructureIA.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "forceInit = False\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 15\n",
    "nom = 'GAN_essai_'+str(i)+'_N'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "      \n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    couche,kernelBiasList,saver,sortiesImages = discriminatorDense(generator,[150,300,150],'Lien')\n",
    "    nbCouchesDsicriminateur = 15\n",
    "    disc_faux,disc_vars,disc_saver,disc_faux_sortiesImages = discriminator(generator,np.linspace(int(generator.get_shape()[1]),199,nbCouchesDsicriminateur+1,dtype=np.int)[1:],np.linspace(int(generator.get_shape()[-1]),3,nbCouchesDsicriminateur,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux-gen_input)  # This optimizes the discriminator.\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",disc_loss)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    saver =  disc_saver\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_disc = optimizer_disc.minimize(disc_loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('./checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('./checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('./checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14]) > lastTrained:\n",
    "              lastTrained = int(elem[14])\n",
    "          tf_saver.restore(sess, 'checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(0,101):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(disc_saver,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "forceInit = False\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 15\n",
    "nom = 'GAN_essai_'+str(i)+'_C'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "      \n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(disc_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    couche,kernelBiasList,saver,sortiesImages = discriminatorDense(generator,[150,300,150],'Lien')\n",
    "    nbCouchesDsicriminateur = 15\n",
    "    disc_faux,disc_vars,disc_saver,disc_faux_sortiesImages = discriminator(generator,np.linspace(int(generator.get_shape()[1]),199,nbCouchesDsicriminateur+1,dtype=np.int)[1:],np.linspace(int(generator.get_shape()[-1]),3,nbCouchesDsicriminateur,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(disc_faux-disc_input)  # This optimizes the discriminator.\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",disc_loss)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    saver =  disc_saver\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_disc = optimizer_disc.minimize(disc_loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('./checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('./checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('./checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14]) > lastTrained:\n",
    "              lastTrained = int(elem[14])\n",
    "          tf_saver.restore(sess, 'checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(0,101):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "          save(disc_saver,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme visiblement (1/5 entrainement fini pour les 2 modèles) le modèle ne converge pas, il manquerait des images. En effet, le lot de données doit être trop restreint pour dégager la cohérence du lot.  Pour cela on prendra des images sur astrobin on détériorera l'image artificiellement pour observer si cela permet d'améliorer l'apprentissage. En effet, cela permettrait de montrer plus d'exemples pertinents au modèle (avec plus d'objets). A ce moment là on ne redimensionnerait que peu la taille de l'image pour permettre au modèle de se concentrer sur l'apprentissage des caractéristiques propres aux 'objets du ciel profond'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On envisage l'algorithme suivant pour détériorer les images artificiellement avec tout d'abord un exemple de fonctionnement en local :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(732, 1100, 3)\n",
      "(732, 1100, 3)\n",
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "image = cv2.imread('D:/Google drive/TIPE/Images_source/clean/3.jpg',1)\n",
    "noiseImage = cv2.imread('D:/Google drive/TIPE/Images_source/noise/3.jpg',1)\n",
    "row,col,ch = image.shape\n",
    "image = cv2.resize(image,(col//5,row//5))\n",
    "row,col,ch = image.shape\n",
    "gauss = np.array(np.random.normal(np.mean(noiseImage),1,(row,col,ch)),dtype=np.int)\n",
    "print(gauss.shape)\n",
    "noisy = image + gauss\n",
    "imageContrast = (np.array(image,dtype=np.float64)-np.mean(image))//2+np.mean(image)\n",
    "imageContrast = np.array(imageContrast,dtype=np.float64)\n",
    "print(noisy.shape)\n",
    "print(type(image)==type(noisy))\n",
    "# print(noisy[20:50,20:50,0])\n",
    "# cv2.destroyAllWindows()\n",
    "plt.figure(3)\n",
    "plt.imshow(image)\n",
    "plt.figure(2)\n",
    "plt.imshow(image//np.random.randint(3,7)+gauss)\n",
    "plt.figure(1)\n",
    "plt.imshow(noiseImage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Néanmoins ce programme nécessite la valeur moyenne de l'image bruitée donc l'image bruitée ce qui n'est pas le but.</br>\n",
    "Sur mes images affichons les valeurs moyennes de chaque image non traitée : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-08-27 22:04:00','2018-08-05 01:29:00','2018-08-02 01:10:00','2018-08-02 00:57:00','2018-08-02 01:10:00','2018-08-06 00:22:00','2018-08-06 00:22:00']\n"
     ]
    }
   ],
   "source": [
    "temps = ['27/08/2018,22:04','05/08/2018,01:29','02/08/2018,01:10','02/08/2018,00:57','02/08/2018,01:10','06/08/2018,00:22','28/08/2018,00:31']\n",
    "print(\"[\",end='')\n",
    "for elem in temps[:-1]:\n",
    "    sselemts = elem.split(',')\n",
    "    date = sselemts[0].split('/')\n",
    "    print(\"'\"+date[-1]+\"-\"+date[1]+\"-\"+date[0]+\" \"+sselemts[-1]+\":00',\",end='')\n",
    "sselemts = elem.split(',')\n",
    "date = sselemts[0].split('/')\n",
    "print(\"'\"+date[-1]+\"-\"+date[1]+\"-\"+date[0]+\" \"+sselemts[-1]+\":00'\",end='')\n",
    "print(']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://docs.astropy.org/en/stable/generated/examples/coordinates/plot_obs-planning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://astroplan.readthedocs.io/en/latest/_modules/astroplan/moon.html ne fonctionnent pas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai avec https://astroplan.readthedocs.io/en/v0.1/_modules/astroplan/moon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "from astropy.coordinates import get_moon, get_sun\n",
    "\n",
    "__all__ = [\"moon_phase_angle\", \"moon_illumination\"]\n",
    "def moon_illumination(time, ephemeris=None):\n",
    "    \"\"\"\n",
    "    Calculate fraction of the moon illuminated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : `~astropy.time.Time`\n",
    "        Time of observation\n",
    "\n",
    "    ephemeris : str, optional\n",
    "        Ephemeris to use.  If not given, use the one set with\n",
    "        `~astropy.coordinates.solar_system_ephemeris` (which is\n",
    "        set to 'builtin' by default).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k : float\n",
    "        Fraction of moon illuminated\n",
    "    \"\"\"\n",
    "    i = moon_phase_angle(time, ephemeris=ephemeris)\n",
    "    k = (1 + np.cos(i))/2.0\n",
    "    return k.value\n",
    "\n",
    "def moon_phase_angle(time, ephemeris=None):\n",
    "    \"\"\"\n",
    "    Calculate lunar orbital phase in radians.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : `~astropy.time.Time`\n",
    "        Time of observation\n",
    "\n",
    "    ephemeris : str, optional\n",
    "        Ephemeris to use.  If not given, use the one set with\n",
    "        `~astropy.coordinates.solar_system_ephemeris` (which is\n",
    "        set to 'builtin' by default).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    i : float\n",
    "        Phase angle of the moon [radians]\n",
    "    \"\"\"\n",
    "    # TODO: cache these sun/moon SkyCoord objects\n",
    "\n",
    "    sun = get_sun(time)\n",
    "    moon = get_moon(time, ephemeris=ephemeris)\n",
    "    elongation = sun.separation(moon)\n",
    "    return np.arctan2(sun.distance*np.sin(elongation),\n",
    "                      moon.distance - sun.distance*np.cos(elongation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sortOnList(listList,indexListSort):\n",
    "    arraySort = [(list(range(len(listList[0])))[i],listList[indexListSort][i]) for i in range(len(listList[indexListSort]))]\n",
    "#     print(arraySort)\n",
    "    arraySort.sort(key=lambda x: x[1])\n",
    "    sortedTupple = []\n",
    "    for i in range(len(listList)):\n",
    "        listIndices = [arraySort[a][0] for a in range(len(arraySort))]\n",
    "        array = [listList[i][indice] for indice in listIndices]\n",
    "        sortedTupple.append(array)\n",
    "    return sortedTupple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne de 51.570884, phase de 98.900000, distanceLune 43.383333 pour l'image 1\n",
      "Moyenne de 115.856622, phase de 42.400000, distanceLune 92.866667 pour l'image 2\n",
      "Moyenne de 42.923483, phase de 73.000000, distanceLune 107.366667 pour l'image 3\n",
      "Moyenne de 19.210552, phase de 73.000000, distanceLune 62.550000 pour l'image 4\n",
      "Moyenne de 1.992748, phase de 73.000000, distanceLune 74.200000 pour l'image 5\n",
      "Moyenne de 155.987816, phase de 31.600000, distanceLune 99.633333 pour l'image 6\n",
      "Moyenne de 68.023631, phase de 31.600000, distanceLune 69.466667 pour l'image 7\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "temps = ['2018-08-27 22:04:00','2018-08-05 01:29:00','2018-08-02 01:10:00','2018-08-02 00:57:00','2018-08-02 01:10:00','2018-08-06 00:22:00','2018-08-06 00:22:00']\n",
    "lune = [98.9,42.4,73.0,73.0,73.0,31.6,31.6]\n",
    "distanceObjetLune = [43+23/60,92+52/60,107+22/60,62+33/60,74+12/60,99+38/60,69+28/60]\n",
    "indices = list(range(1,8))\n",
    "moyennes = []\n",
    "import numpy as np\n",
    "for i in range(1,8):\n",
    "    imageNoise = cv2.imread('D:/Google drive/TIPE/Images_source/noise/'+str(i)+'.jpg',1)\n",
    "    moyennes.append(np.mean(imageNoise))\n",
    "    print(\"Moyenne de %f, phase de %f, distanceLune %f pour l'image %i\" % (moyennes[-1],lune[i-1],distanceObjetLune[i-1],i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indicesSortLn,lnSortLn,dSortLn,mSortLn = sortOnList([indices,lune,distanceObjetLune,moyennes],1)\n",
    "indicesSortD,lnSortD,dSortD,mSortD = sortOnList([indices,lune,distanceObjetLune,moyennes],2)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib ipympl\n",
    "\n",
    "plt.figure(1)\n",
    "plt.xlabel('Illumination lune')\n",
    "plt.ylabel(\"g = distance de l'objet à la lune ; b = moyenne de l'image\")\n",
    "plt.plot(lnSortLn,dSortLn,linestyle='None',marker='o',color='g')\n",
    "for i in range(len(lnSortLn)):\n",
    "    plt.annotate(str(indicesSortLn[i])+'.jpg',(lnSortLn[i],dSortLn[i]))\n",
    "plt.plot(lnSortLn,mSortLn,linestyle='None',marker='o',color='b')\n",
    "for i in range(len(lnSortLn)):\n",
    "    plt.annotate(str(indicesSortLn[i])+'.jpg',(lnSortD[i],mSortLn[i]))\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"distance de l'objet à la lune\")\n",
    "plt.ylabel(\"r = moyenne de l'image\")\n",
    "plt.plot(dSortD,(np.array(mSortD)),linestyle='None',marker='o',color='r')\n",
    "for i in range(len(lnSortD)):\n",
    "    plt.annotate(str(indicesSortD[i])+'.jpg',(dSortD[i],mSortD[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas de relation particulière... même si la courbe avec moyenne et distance à la lune parait plus intéressante (points moins resserés)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourrait utiliser à nouveau une ia pour trouver une potentielle relation entre distance de l'objet observé et moyenne de l'image mais il faudrait plus d'images obligatoirement ici (faisant varier la distance à la lune sans post traitement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On refait ça sur plus d'images trouvées dans le disque dur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On renomme déjà les images présentes dans le dossier pour aller plus vite après."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src http://docs.astropy.org/en/stable/api/astropy.coordinates.SkyCoord.html#astropy.coordinates.SkyCoord.from_name ; http://docs.astropy.org/en/stable/coordinates/matchsep.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fichiers = os.listdir(\"D:/Github/ImagesAnalyseVoile\")[:-1]\n",
    "fichiersComplets = [\"D:/Github/ImagesAnalyseVoile/\"+fichiers[i] for i in range(len(fichiers))]\n",
    "# print(fichiers)\n",
    "dates = []\n",
    "illuminationLn = []\n",
    "from PIL import Image\n",
    "def get_date_taken(path):\n",
    "    return Image.open(path)._getexif()[36867]\n",
    "from astroplan import Observer\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import EarthLocation,SkyCoord\n",
    "from astropy.time import Time,TimezoneInfo\n",
    "location = EarthLocation.from_geodetic((45+56/60)*u.deg, (6+42/60)*u.deg,1000*u.m)\n",
    "passy = Observer(location=location, name=\"Passy\", timezone=TimezoneInfo(utc_offset=2*u.hour))\n",
    "nomObjets = []\n",
    "separationLn = []\n",
    "ISO = []\n",
    "tpose = []\n",
    "moyennes = []\n",
    "dbObjects = [('M51','13h29m53s +47d11m43s'),('NGC7023','21h01m37s +68d09m48s'),('NGC281','0h52m25s +56d33m54s'),('M27','19h59m36s +22d43m16s'),\n",
    "             ('M33','1h33m51s +30d39m36s'),('M20','18h02m42s -22d58m19s'),('M13','16h41m42s +36d27m41s')]# Dictionnaire peut-être plus adapté mais bon\n",
    "def convertCoordFormat(chaine):\n",
    "    import re\n",
    "    L = re.split('\\s|h|m|s|d',chaine)[:-1]\n",
    "    del L[3]\n",
    "#     print(L)\n",
    "    L = list(map(int,L))\n",
    "    RA,DEC = L[:3],L[3:]\n",
    "    RA,DEC = RA[0]+RA[1]/60+RA[2]/3600,DEC[0]+DEC[1]/60+DEC[2]/3600\n",
    "    return RA,DEC\n",
    "\n",
    "def searchCoord(name):\n",
    "    global dbObjects\n",
    "    i = 0\n",
    "    elem = dbObjects[0][0]\n",
    "    while elem != name and i < len(dbObjects):\n",
    "        i += 1\n",
    "        elem = dbObjects[i][0]\n",
    "    return convertCoordFormat(dbObjects[i][1])\n",
    "\n",
    "for fichier in fichiersComplets:\n",
    "#     print(fichier,end=', ')\n",
    "    val = get_date_taken(fichier).split(' ')\n",
    "    dates.append('-'.join(val[0].split(':'))+' '+val[1])\n",
    "    time = Time(dates[-1])\n",
    "    \n",
    "    imageNoise = cv2.imread(fichier,1)\n",
    "    moyennes.append(np.mean(imageNoise))\n",
    "    \n",
    "    illuminationLn.append(int(passy.moon_illumination(time)*100))\n",
    "    \n",
    "    nomObjets.append(str(fichier[32:-4]))#prémâché par lightroom\n",
    "    \n",
    "    objet = searchCoord(nomObjets[-1])\n",
    "    coordObj = SkyCoord(ra=objet[0],dec=objet[1], frame=\"icrs\",unit=(u.hour,u.deg))\n",
    "    coordLn = get_moon(time=time,location=location)\n",
    "    sep = coordObj.separation(coordLn)\n",
    "    separationLn.append(sep.deg)\n",
    "    \n",
    "    ISO.append(Image.open(fichiersComplets[i])._getexif()[34855])\n",
    "    \n",
    "    tpose.append(Image.open(fichiersComplets[i])._getexif()[33434][0])\n",
    "    \n",
    "#     print(objet)\n",
    "#     print(nomObjets[-1],end=', ')\n",
    "#     print(sep.deg)\n",
    "#     print(get_date_taken(fichier))\n",
    "#     print(val)\n",
    "#     print(dates[-1])\n",
    "#     print(str(illuminationLn[-1])+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 113.79591981420148 12800 64 100.55385098793752\n"
     ]
    }
   ],
   "source": [
    "print(illuminationLn[0], separationLn[0],ISO[0],tpose[0], moyennes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lienAstrobin\"></a> \n",
    "Plus qu'à faire l'IA qui donnera en fonction de `separationLn`, `ISO`, `tpose`, `illuminationLn` la moyenne de l'image puis une fois entrainée faire le protocle de récupération des images sur astrobin : exemple d'adresse : https://www.astrobin.com/full/392940/0/?nc=all (mis en place [ici](#algorithmeAstrobin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IA : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size, illuminationLn, separationLn,ISO,tpose, moyennes):\n",
    "    indices = np.arange(0,len(illuminationLn),1)\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[0:batch_size]\n",
    "    illuminationLnArray = np.reshape(np.array([illuminationLn[i] for i in indices]),(batch_size,1))\n",
    "    separationLnArray = np.reshape(np.array([separationLn[i] for i in indices]),(batch_size,1))\n",
    "    ISOArray = np.reshape(np.array([ISO[i] for i in indices]),(batch_size,1))\n",
    "    tposeArray = np.reshape(np.array([tpose[i] for i in indices]),(batch_size,1))\n",
    "    moyennesArray = np.reshape(np.array([moyennes[i] for i in indices]),(batch_size,1))\n",
    "    inpt = np.concatenate((illuminationLnArray,separationLnArray,ISOArray,tposeArray),axis=1)\n",
    "    return np.array(inpt,dtype=np.float32),np.array(moyennesArray,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_batch(7,illuminationLn, separationLn,ISO,tpose, moyennes)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def data(illuminationLn, separationLn,ISO,tpose, moyennes):\n",
    "    indices = np.arange(0,len(illuminationLn),1)\n",
    "    np.random.shuffle(indices)\n",
    "    illuminationLnArray = np.reshape(np.array([illuminationLn[i] for i in indices]),(len(separationLn),1))\n",
    "    separationLnArray = np.reshape(np.array([separationLn[i] for i in indices]),(len(separationLn),1))\n",
    "    ISOArray = np.reshape(np.array([ISO[i] for i in indices]),(len(separationLn),1))\n",
    "    tposeArray = np.reshape(np.array([tpose[i] for i in indices]),(len(separationLn),1))\n",
    "    moyennesArray = np.reshape(np.array([moyennes[i] for i in indices]),(len(separationLn),1))\n",
    "    inpt = np.concatenate((illuminationLnArray,separationLnArray,ISOArray,tposeArray),axis=1)\n",
    "    return np.array(inpt,dtype=np.float32),np.array(moyennesArray,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de denseLien_1 : (7, 4)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_2 : (7, 100)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_3 : (7, 150)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_4 : (7, 100)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_5 : (7, 50)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_6 : (7, 1)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "['checkpoint', 'checkpoint_at_35.ckpt.data-00000-of-00001', 'checkpoint_at_35.ckpt.index', 'checkpoint_at_35.ckpt.meta', 'checkpoint_at_40.ckpt.data-00000-of-00001', 'checkpoint_at_40.ckpt.index', 'checkpoint_at_40.ckpt.meta', 'checkpoint_at_45.ckpt.data-00000-of-00001', 'checkpoint_at_45.ckpt.index', 'checkpoint_at_45.ckpt.meta', 'checkpoint_at_50.ckpt.data-00000-of-00001', 'checkpoint_at_50.ckpt.index', 'checkpoint_at_50.ckpt.meta']\n",
      "INFO:tensorflow:Restoring parameters from D:/Github/ImagesAnalyseVoile/IA/checkpoints_Moyenne_image_0/checkpoint_at_50.ckpt\n",
      "Entrainement....\n",
      "Step 0: Discriminator Loss: 61163970560.000000\n",
      "Step 1: Discriminator Loss: 49650774016.000000\n",
      "Step 2: Discriminator Loss: 39767715840.000000\n",
      "Step 3: Discriminator Loss: 32323192832.000000\n",
      "Step 4: Discriminator Loss: 26890205184.000000\n",
      "Step 5: Discriminator Loss: 22871703552.000000\n",
      "Step 6: Discriminator Loss: 19786764288.000000\n",
      "Step 7: Discriminator Loss: 17415419904.000000\n",
      "Step 8: Discriminator Loss: 15482985472.000000\n",
      "Step 9: Discriminator Loss: 13960121344.000000\n",
      "Step 10: Discriminator Loss: 12646964224.000000\n",
      "Step 11: Discriminator Loss: 11576364032.000000\n",
      "Step 12: Discriminator Loss: 10637241344.000000\n",
      "Step 13: Discriminator Loss: 9808997376.000000\n",
      "Step 14: Discriminator Loss: 9084500992.000000\n",
      "Step 15: Discriminator Loss: 8463805952.000000\n",
      "Step 16: Discriminator Loss: 7877727744.000000\n",
      "Step 17: Discriminator Loss: 7360304640.000000\n",
      "Step 18: Discriminator Loss: 6894335488.000000\n",
      "Step 19: Discriminator Loss: 6459362304.000000\n",
      "Step 20: Discriminator Loss: 6063190016.000000\n",
      "Step 21: Discriminator Loss: 5715876864.000000\n",
      "Step 22: Discriminator Loss: 5372361728.000000\n",
      "Step 23: Discriminator Loss: 5064734208.000000\n",
      "Step 24: Discriminator Loss: 4789718528.000000\n",
      "Step 25: Discriminator Loss: 4523127808.000000\n",
      "Step 26: Discriminator Loss: 4280938752.000000\n",
      "Step 27: Discriminator Loss: 4039876352.000000\n",
      "Step 28: Discriminator Loss: 3832038656.000000\n",
      "Step 29: Discriminator Loss: 3634810880.000000\n",
      "Step 30: Discriminator Loss: 3442446336.000000\n",
      "Step 31: Discriminator Loss: 3260230400.000000\n",
      "Step 32: Discriminator Loss: 3094507008.000000\n",
      "Step 33: Discriminator Loss: 2940296448.000000\n",
      "Step 34: Discriminator Loss: 2788434432.000000\n",
      "Step 35: Discriminator Loss: 2649572096.000000\n",
      "Step 36: Discriminator Loss: 2505642496.000000\n",
      "Step 37: Discriminator Loss: 2382739968.000000\n",
      "Step 38: Discriminator Loss: 2269503232.000000\n",
      "Step 39: Discriminator Loss: 2148494592.000000\n",
      "Step 40: Discriminator Loss: 2043838336.000000\n",
      "Step 41: Discriminator Loss: 1943296896.000000\n",
      "Step 42: Discriminator Loss: 1849983744.000000\n",
      "Step 43: Discriminator Loss: 1752776704.000000\n",
      "Step 44: Discriminator Loss: 1666505472.000000\n",
      "Step 45: Discriminator Loss: 1583397504.000000\n",
      "Step 46: Discriminator Loss: 1503931264.000000\n",
      "Step 47: Discriminator Loss: 1425580160.000000\n",
      "Step 48: Discriminator Loss: 1353922944.000000\n",
      "Step 49: Discriminator Loss: 1285203072.000000\n",
      "Step 50: Discriminator Loss: 1214685824.000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "forceInit = False\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 0\n",
    "nom = 'Moyenne_image_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"D:/Github/ImagesAnalyseVoile/IA/tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "        return couche,kernelBiasList,saver\n",
    "    \n",
    "    inpt = tf.placeholder(tf.float32,shape=[batch_size,4],name='inpt')\n",
    "    moyenne = tf.placeholder(tf.float32,shape=[batch_size,1],name='moyenne')\n",
    "    \n",
    "    import numpy as np\n",
    "    couche,kernelBiasList,saver = discriminatorDense(inpt,[4,100,150,100,50,1],'Lien')\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(couche-inpt))  # This optimizes the discriminator.\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",loss)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    train_disc = optimizer_disc.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14:16]) > lastTrained:\n",
    "              lastTrained = int(elem[14:16])\n",
    "          tf_saver.restore(sess, 'D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(0,51):\n",
    "#           print(batch_size,illuminationLn, separationLn,ISO,tpose, moyennes)\n",
    "          inptLayer, moyenneLayer = next_batch(batch_size,illuminationLn, separationLn,ISO,tpose, moyennes)#Problèmes si gen_input\n",
    "#           print(type(inpt),type(moyenne))\n",
    "          _,dl = sess.run([train_disc,loss], feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "#           save(disc_saver,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        tf_saver.save(sess, 'D:/Github/ImagesAnalyseVoile/IA/model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de denseLien_1 : (7, 4)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_2 : (7, 5)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_3 : (7, 6)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_4 : (7, 5)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_5 : (7, 1)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "No previous training found...\n",
      "Entrainement....\n",
      "Step 0: Discriminator Loss: 15938.234375 loss1 : 15938.234375\n",
      "Step 1: Discriminator Loss: 14448.819336 loss1 : 14448.819336\n",
      "Step 2: Discriminator Loss: 13018.708008 loss1 : 13018.708008\n",
      "Step 3: Discriminator Loss: 11637.017578 loss1 : 11637.017578\n",
      "Step 4: Discriminator Loss: 10341.463867 loss1 : 10341.463867\n",
      "Step 5: Discriminator Loss: 9107.377930 loss1 : 9107.377930\n",
      "Step 6: Discriminator Loss: 7953.870117 loss1 : 7953.870117\n",
      "Step 7: Discriminator Loss: 6879.565430 loss1 : 6879.565430\n",
      "Step 8: Discriminator Loss: 5814.137207 loss1 : 5814.137207\n",
      "Step 9: Discriminator Loss: 4825.145020 loss1 : 4825.145020\n",
      "Step 10: Discriminator Loss: 3889.126465 loss1 : 3889.126465\n",
      "Step 11: Discriminator Loss: 2992.819092 loss1 : 2992.819092\n",
      "Step 12: Discriminator Loss: 2166.636963 loss1 : 2166.636963\n",
      "Step 13: Discriminator Loss: 1398.290771 loss1 : 1398.290771\n",
      "Step 14: Discriminator Loss: 635.593628 loss1 : 635.593628\n",
      "Step 15: Discriminator Loss: -82.264091 loss1 : 82.264091\n",
      "Step 16: Discriminator Loss: -598.744995 loss1 : 598.744995\n",
      "Step 17: Discriminator Loss: -972.778442 loss1 : 972.778442\n",
      "Step 18: Discriminator Loss: -1207.031982 loss1 : 1207.031982\n",
      "Step 19: Discriminator Loss: -1361.685303 loss1 : 1361.685303\n",
      "Step 20: Discriminator Loss: -1414.932251 loss1 : 1414.932251\n",
      "Step 21: Discriminator Loss: -1412.194702 loss1 : 1412.194702\n",
      "Step 22: Discriminator Loss: -1352.208008 loss1 : 1352.208008\n",
      "Step 23: Discriminator Loss: -1242.711182 loss1 : 1242.711182\n",
      "Step 24: Discriminator Loss: -1079.831299 loss1 : 1079.831299\n",
      "Step 25: Discriminator Loss: -893.816772 loss1 : 893.816772\n",
      "Step 26: Discriminator Loss: -667.845703 loss1 : 667.845703\n",
      "Step 27: Discriminator Loss: -424.071991 loss1 : 424.071991\n",
      "Step 28: Discriminator Loss: -141.673828 loss1 : 141.673828\n",
      "Step 29: Discriminator Loss: 152.752365 loss1 : 152.752365\n",
      "Step 30: Discriminator Loss: 357.051208 loss1 : 357.051208\n",
      "Step 31: Discriminator Loss: 496.922424 loss1 : 496.922424\n",
      "Step 32: Discriminator Loss: 554.501953 loss1 : 554.501953\n",
      "Step 33: Discriminator Loss: 561.141907 loss1 : 561.141907\n",
      "Step 34: Discriminator Loss: 517.137024 loss1 : 517.137024\n",
      "Step 35: Discriminator Loss: 419.544922 loss1 : 419.544922\n",
      "Step 36: Discriminator Loss: 282.122498 loss1 : 282.122498\n",
      "Step 37: Discriminator Loss: 111.598076 loss1 : 111.598076\n",
      "Step 38: Discriminator Loss: -98.437500 loss1 : 98.437500\n",
      "Step 39: Discriminator Loss: -220.088455 loss1 : 220.088455\n",
      "Step 40: Discriminator Loss: -282.632385 loss1 : 282.632385\n",
      "Step 41: Discriminator Loss: -306.860077 loss1 : 306.860077\n",
      "Step 42: Discriminator Loss: -268.708984 loss1 : 268.708984\n",
      "Step 43: Discriminator Loss: -218.690018 loss1 : 218.690018\n",
      "Step 44: Discriminator Loss: -108.345146 loss1 : 108.345146\n",
      "Step 45: Discriminator Loss: 35.026787 loss1 : 35.026787\n",
      "Step 46: Discriminator Loss: 113.638954 loss1 : 113.638954\n",
      "Step 47: Discriminator Loss: 141.572540 loss1 : 141.572540\n",
      "Step 48: Discriminator Loss: 113.872208 loss1 : 113.872208\n",
      "Step 49: Discriminator Loss: 46.708149 loss1 : 46.708149\n",
      "Step 50: Discriminator Loss: -47.134209 loss1 : 47.134209\n",
      "Step 51: Discriminator Loss: -96.388672 loss1 : 96.388672\n",
      "Step 52: Discriminator Loss: -88.599747 loss1 : 88.599747\n",
      "Step 53: Discriminator Loss: -45.831333 loss1 : 45.831333\n",
      "Step 54: Discriminator Loss: 25.489258 loss1 : 25.489258\n",
      "Step 55: Discriminator Loss: 60.438198 loss1 : 60.438198\n",
      "Step 56: Discriminator Loss: 43.447544 loss1 : 43.447544\n",
      "Step 57: Discriminator Loss: -6.557199 loss1 : 6.557199\n",
      "Step 58: Discriminator Loss: -22.125278 loss1 : 22.125278\n",
      "Step 59: Discriminator Loss: 14.350586 loss1 : 14.350586\n",
      "Step 60: Discriminator Loss: 7.657366 loss1 : 7.657366\n",
      "Step 61: Discriminator Loss: -56.445454 loss1 : 56.445454\n",
      "Step 62: Discriminator Loss: -51.181362 loss1 : 51.181362\n",
      "Step 63: Discriminator Loss: -8.652902 loss1 : 8.652902\n",
      "Step 64: Discriminator Loss: 62.526508 loss1 : 62.526508\n",
      "Step 65: Discriminator Loss: 96.791710 loss1 : 96.791710\n",
      "Step 66: Discriminator Loss: 64.309433 loss1 : 64.309433\n",
      "Step 67: Discriminator Loss: 4.347935 loss1 : 4.347935\n",
      "Step 68: Discriminator Loss: -90.945732 loss1 : 90.945732\n",
      "Step 69: Discriminator Loss: -131.165039 loss1 : 131.165039\n",
      "Step 70: Discriminator Loss: -143.744278 loss1 : 143.744278\n",
      "Step 71: Discriminator Loss: -85.968468 loss1 : 85.968468\n",
      "Step 72: Discriminator Loss: -0.912807 loss1 : 0.912807\n",
      "Step 73: Discriminator Loss: 108.109375 loss1 : 108.109375\n",
      "Step 74: Discriminator Loss: 167.732422 loss1 : 167.732422\n",
      "Step 75: Discriminator Loss: 182.429962 loss1 : 182.429962\n",
      "Step 76: Discriminator Loss: 151.243027 loss1 : 151.243027\n",
      "Step 77: Discriminator Loss: 81.655273 loss1 : 81.655273\n",
      "Step 78: Discriminator Loss: -27.414202 loss1 : 27.414202\n",
      "Step 79: Discriminator Loss: -85.333145 loss1 : 85.333145\n",
      "Step 80: Discriminator Loss: -99.202843 loss1 : 99.202843\n",
      "Step 81: Discriminator Loss: -67.275108 loss1 : 67.275108\n",
      "Step 82: Discriminator Loss: 5.128348 loss1 : 5.128348\n",
      "Step 83: Discriminator Loss: 37.562641 loss1 : 37.562641\n",
      "Step 84: Discriminator Loss: 15.421456 loss1 : 15.421456\n",
      "Step 85: Discriminator Loss: -56.697544 loss1 : 56.697544\n",
      "Step 86: Discriminator Loss: -59.360493 loss1 : 59.360493\n",
      "Step 87: Discriminator Loss: -33.544922 loss1 : 33.544922\n",
      "Step 88: Discriminator Loss: 31.398855 loss1 : 31.398855\n",
      "Step 89: Discriminator Loss: 67.327286 loss1 : 67.327286\n",
      "Step 90: Discriminator Loss: 38.904297 loss1 : 38.904297\n",
      "Step 91: Discriminator Loss: -33.530693 loss1 : 33.530693\n",
      "Step 92: Discriminator Loss: -50.156528 loss1 : 50.156528\n",
      "Step 93: Discriminator Loss: -8.023856 loss1 : 8.023856\n",
      "Step 94: Discriminator Loss: 47.765625 loss1 : 47.765625\n",
      "Step 95: Discriminator Loss: 70.913925 loss1 : 70.913925\n",
      "Step 96: Discriminator Loss: 39.379463 loss1 : 39.379463\n",
      "Step 97: Discriminator Loss: -27.067801 loss1 : 27.067801\n",
      "Step 98: Discriminator Loss: -54.692524 loss1 : 54.692524\n",
      "Step 99: Discriminator Loss: -19.153738 loss1 : 19.153738\n",
      "Step 100: Discriminator Loss: 49.050503 loss1 : 49.050503\n",
      "Step 101: Discriminator Loss: 68.900108 loss1 : 68.900108\n",
      "Step 102: Discriminator Loss: 45.500835 loss1 : 45.500835\n",
      "Step 103: Discriminator Loss: -35.306782 loss1 : 35.306782\n",
      "Step 104: Discriminator Loss: -44.176338 loss1 : 44.176338\n",
      "Step 105: Discriminator Loss: -16.205915 loss1 : 16.205915\n",
      "Step 106: Discriminator Loss: 44.006836 loss1 : 44.006836\n",
      "Step 107: Discriminator Loss: 74.453407 loss1 : 74.453407\n",
      "Step 108: Discriminator Loss: 31.747908 loss1 : 31.747908\n",
      "Step 109: Discriminator Loss: -30.523159 loss1 : 30.523159\n",
      "Step 110: Discriminator Loss: -63.660156 loss1 : 63.660156\n",
      "Step 111: Discriminator Loss: -13.078404 loss1 : 13.078404\n",
      "Step 112: Discriminator Loss: 51.519390 loss1 : 51.519390\n",
      "Step 113: Discriminator Loss: 53.235771 loss1 : 53.235771\n",
      "Step 114: Discriminator Loss: 24.971262 loss1 : 24.971262\n",
      "Step 115: Discriminator Loss: -33.302177 loss1 : 33.302177\n",
      "Step 116: Discriminator Loss: -55.593052 loss1 : 55.593052\n",
      "Step 117: Discriminator Loss: -24.571848 loss1 : 24.571848\n",
      "Step 118: Discriminator Loss: 38.447266 loss1 : 38.447266\n",
      "Step 119: Discriminator Loss: 54.510323 loss1 : 54.510323\n",
      "Step 120: Discriminator Loss: 21.840960 loss1 : 21.840960\n",
      "Step 121: Discriminator Loss: -57.483955 loss1 : 57.483955\n",
      "Step 122: Discriminator Loss: -70.895645 loss1 : 70.895645\n",
      "Step 123: Discriminator Loss: -38.411274 loss1 : 38.411274\n",
      "Step 124: Discriminator Loss: 23.986887 loss1 : 23.986887\n",
      "Step 125: Discriminator Loss: 36.301899 loss1 : 36.301899\n",
      "Step 126: Discriminator Loss: 6.327846 loss1 : 6.327846\n",
      "Step 127: Discriminator Loss: -63.914341 loss1 : 63.914341\n",
      "Step 128: Discriminator Loss: -88.465126 loss1 : 88.465126\n",
      "Step 129: Discriminator Loss: -62.191406 loss1 : 62.191406\n",
      "Step 130: Discriminator Loss: 14.882812 loss1 : 14.882812\n",
      "Step 131: Discriminator Loss: 37.774834 loss1 : 37.774834\n",
      "Step 132: Discriminator Loss: 2.885463 loss1 : 2.885463\n",
      "Step 133: Discriminator Loss: -78.313759 loss1 : 78.313759\n",
      "Step 134: Discriminator Loss: -85.452011 loss1 : 85.452011\n",
      "Step 135: Discriminator Loss: -61.927876 loss1 : 61.927876\n",
      "Step 136: Discriminator Loss: 1.241629 loss1 : 1.241629\n",
      "Step 137: Discriminator Loss: 25.037807 loss1 : 25.037807\n",
      "Step 138: Discriminator Loss: -12.955217 loss1 : 12.955217\n",
      "Step 139: Discriminator Loss: 12.490792 loss1 : 12.490792\n",
      "Step 140: Discriminator Loss: -32.455078 loss1 : 32.455078\n",
      "Step 141: Discriminator Loss: -8.277205 loss1 : 8.277205\n",
      "Step 142: Discriminator Loss: 56.479633 loss1 : 56.479633\n",
      "Step 143: Discriminator Loss: 64.069756 loss1 : 64.069756\n",
      "Step 144: Discriminator Loss: 23.526367 loss1 : 23.526367\n",
      "Step 145: Discriminator Loss: -48.210938 loss1 : 48.210938\n",
      "Step 146: Discriminator Loss: -77.373329 loss1 : 77.373329\n",
      "Step 147: Discriminator Loss: -52.576729 loss1 : 52.576729\n",
      "Step 148: Discriminator Loss: 11.465402 loss1 : 11.465402\n",
      "Step 149: Discriminator Loss: 24.954800 loss1 : 24.954800\n",
      "Step 150: Discriminator Loss: -8.650251 loss1 : 8.650251\n",
      "Step 151: Discriminator Loss: 9.999442 loss1 : 9.999442\n",
      "Step 152: Discriminator Loss: -5.520926 loss1 : 5.520926\n",
      "Step 153: Discriminator Loss: 6.405692 loss1 : 6.405692\n",
      "Step 154: Discriminator Loss: -29.364956 loss1 : 29.364956\n",
      "Step 155: Discriminator Loss: -18.260044 loss1 : 18.260044\n",
      "Step 156: Discriminator Loss: 39.473774 loss1 : 39.473774\n",
      "Step 157: Discriminator Loss: 53.164619 loss1 : 53.164619\n",
      "Step 158: Discriminator Loss: 25.111467 loss1 : 25.111467\n",
      "Step 159: Discriminator Loss: -73.582726 loss1 : 73.582726\n",
      "Step 160: Discriminator Loss: -90.913223 loss1 : 90.913223\n",
      "Step 161: Discriminator Loss: -69.470009 loss1 : 69.470009\n",
      "Step 162: Discriminator Loss: -7.271484 loss1 : 7.271484\n",
      "Step 163: Discriminator Loss: 112.257530 loss1 : 112.257530\n",
      "Step 164: Discriminator Loss: 152.562363 loss1 : 152.562363\n",
      "Step 165: Discriminator Loss: 144.803146 loss1 : 144.803146\n",
      "Step 166: Discriminator Loss: 108.659874 loss1 : 108.659874\n",
      "Step 167: Discriminator Loss: 15.561803 loss1 : 15.561803\n",
      "Step 168: Discriminator Loss: -125.339561 loss1 : 125.339561\n",
      "Step 169: Discriminator Loss: -179.803299 loss1 : 179.803299\n",
      "Step 170: Discriminator Loss: -208.096268 loss1 : 208.096268\n",
      "Step 171: Discriminator Loss: -187.975998 loss1 : 187.975998\n",
      "Step 172: Discriminator Loss: -106.915878 loss1 : 106.915878\n",
      "Step 173: Discriminator Loss: -2.220424 loss1 : 2.220424\n",
      "Step 174: Discriminator Loss: 151.408615 loss1 : 151.408615\n",
      "Step 175: Discriminator Loss: 226.915604 loss1 : 226.915604\n",
      "Step 176: Discriminator Loss: 255.664062 loss1 : 255.664062\n",
      "Step 177: Discriminator Loss: 236.094452 loss1 : 236.094452\n",
      "Step 178: Discriminator Loss: 160.853653 loss1 : 160.853653\n",
      "Step 179: Discriminator Loss: 66.220284 loss1 : 66.220284\n",
      "Step 180: Discriminator Loss: -76.635048 loss1 : 76.635048\n",
      "Step 181: Discriminator Loss: -153.953400 loss1 : 153.953400\n",
      "Step 182: Discriminator Loss: -185.728928 loss1 : 185.728928\n",
      "Step 183: Discriminator Loss: -165.029297 loss1 : 165.029297\n",
      "Step 184: Discriminator Loss: -116.468750 loss1 : 116.468750\n",
      "Step 185: Discriminator Loss: -15.168108 loss1 : 15.168108\n",
      "Step 186: Discriminator Loss: 116.757256 loss1 : 116.757256\n",
      "Step 187: Discriminator Loss: 190.603928 loss1 : 190.603928\n",
      "Step 188: Discriminator Loss: 230.139374 loss1 : 230.139374\n",
      "Step 189: Discriminator Loss: 185.829239 loss1 : 185.829239\n",
      "Step 190: Discriminator Loss: 114.519951 loss1 : 114.519951\n",
      "Step 191: Discriminator Loss: -0.149833 loss1 : 0.149833\n",
      "Step 192: Discriminator Loss: -31.833984 loss1 : 31.833984\n",
      "Step 193: Discriminator Loss: -34.095562 loss1 : 34.095562\n",
      "Step 194: Discriminator Loss: -6.485073 loss1 : 6.485073\n",
      "Step 195: Discriminator Loss: 89.545479 loss1 : 89.545479\n",
      "Step 196: Discriminator Loss: 100.691269 loss1 : 100.691269\n",
      "Step 197: Discriminator Loss: 92.372353 loss1 : 92.372353\n",
      "Step 198: Discriminator Loss: 22.610352 loss1 : 22.610352\n",
      "Step 199: Discriminator Loss: -60.637417 loss1 : 60.637417\n",
      "Step 200: Discriminator Loss: -122.243721 loss1 : 122.243721\n",
      "Step 201: Discriminator Loss: -111.633926 loss1 : 111.633926\n",
      "Step 202: Discriminator Loss: -76.682480 loss1 : 76.682480\n",
      "Step 203: Discriminator Loss: 13.116490 loss1 : 13.116490\n",
      "Step 204: Discriminator Loss: 53.174664 loss1 : 53.174664\n",
      "Step 205: Discriminator Loss: 24.039480 loss1 : 24.039480\n",
      "Step 206: Discriminator Loss: -23.924665 loss1 : 23.924665\n",
      "Step 207: Discriminator Loss: -43.089146 loss1 : 43.089146\n",
      "Step 208: Discriminator Loss: 2.973214 loss1 : 2.973214\n",
      "Step 209: Discriminator Loss: -15.206194 loss1 : 15.206194\n",
      "Step 210: Discriminator Loss: 27.042969 loss1 : 27.042969\n",
      "Step 211: Discriminator Loss: 0.960100 loss1 : 0.960100\n",
      "Step 212: Discriminator Loss: -43.177177 loss1 : 43.177177\n",
      "Step 213: Discriminator Loss: -53.714287 loss1 : 53.714287\n",
      "Step 214: Discriminator Loss: -15.185267 loss1 : 15.185267\n",
      "Step 215: Discriminator Loss: 58.698799 loss1 : 58.698799\n",
      "Step 216: Discriminator Loss: 86.783485 loss1 : 86.783485\n",
      "Step 217: Discriminator Loss: 64.344032 loss1 : 64.344032\n",
      "Step 218: Discriminator Loss: 2.490374 loss1 : 2.490374\n",
      "Step 219: Discriminator Loss: -99.414200 loss1 : 99.414200\n",
      "Step 220: Discriminator Loss: -145.097931 loss1 : 145.097931\n",
      "Step 221: Discriminator Loss: -156.673126 loss1 : 156.673126\n",
      "Step 222: Discriminator Loss: -93.107422 loss1 : 93.107422\n",
      "Step 223: Discriminator Loss: -12.247628 loss1 : 12.247628\n",
      "Step 224: Discriminator Loss: 109.411270 loss1 : 109.411270\n",
      "Step 225: Discriminator Loss: 178.212616 loss1 : 178.212616\n",
      "Step 226: Discriminator Loss: 172.143692 loss1 : 172.143692\n",
      "Step 227: Discriminator Loss: 133.168808 loss1 : 133.168808\n",
      "Step 228: Discriminator Loss: 58.778042 loss1 : 58.778042\n",
      "Step 229: Discriminator Loss: -52.884209 loss1 : 52.884209\n",
      "Step 230: Discriminator Loss: -98.551338 loss1 : 98.551338\n",
      "Step 231: Discriminator Loss: -104.379189 loss1 : 104.379189\n",
      "Step 232: Discriminator Loss: -93.090958 loss1 : 93.090958\n",
      "Step 233: Discriminator Loss: -8.868583 loss1 : 8.868583\n",
      "Step 234: Discriminator Loss: 108.477119 loss1 : 108.477119\n",
      "Step 235: Discriminator Loss: 163.680801 loss1 : 163.680801\n",
      "Step 236: Discriminator Loss: 163.890060 loss1 : 163.890060\n",
      "Step 237: Discriminator Loss: 132.847382 loss1 : 132.847382\n",
      "Step 238: Discriminator Loss: 44.376118 loss1 : 44.376118\n",
      "Step 239: Discriminator Loss: -68.378906 loss1 : 68.378906\n",
      "Step 240: Discriminator Loss: -121.486885 loss1 : 121.486885\n",
      "Step 241: Discriminator Loss: -131.264511 loss1 : 131.264511\n",
      "Step 242: Discriminator Loss: -93.387558 loss1 : 93.387558\n",
      "Step 243: Discriminator Loss: -35.027901 loss1 : 35.027901\n",
      "Step 244: Discriminator Loss: 78.628906 loss1 : 78.628906\n",
      "Step 245: Discriminator Loss: 125.830360 loss1 : 125.830360\n",
      "Step 246: Discriminator Loss: 127.550781 loss1 : 127.550781\n",
      "Step 247: Discriminator Loss: 100.702843 loss1 : 100.702843\n",
      "Step 248: Discriminator Loss: 6.562779 loss1 : 6.562779\n",
      "Step 249: Discriminator Loss: -106.989952 loss1 : 106.989952\n",
      "Step 250: Discriminator Loss: -169.804688 loss1 : 169.804688\n",
      "Step 251: Discriminator Loss: -187.496368 loss1 : 187.496368\n",
      "Step 252: Discriminator Loss: -145.102402 loss1 : 145.102402\n",
      "Step 253: Discriminator Loss: -69.775948 loss1 : 69.775948\n",
      "Step 254: Discriminator Loss: 20.994141 loss1 : 20.994141\n",
      "Step 255: Discriminator Loss: 80.605049 loss1 : 80.605049\n",
      "Step 256: Discriminator Loss: 75.148155 loss1 : 75.148155\n",
      "Step 257: Discriminator Loss: 42.142578 loss1 : 42.142578\n",
      "Step 258: Discriminator Loss: -34.080914 loss1 : 34.080914\n",
      "Step 259: Discriminator Loss: -60.376675 loss1 : 60.376675\n",
      "Step 260: Discriminator Loss: -56.929829 loss1 : 56.929829\n",
      "Step 261: Discriminator Loss: 4.807757 loss1 : 4.807757\n",
      "Step 262: Discriminator Loss: 12.615514 loss1 : 12.615514\n",
      "Step 263: Discriminator Loss: -18.516741 loss1 : 18.516741\n",
      "Step 264: Discriminator Loss: -1.864537 loss1 : 1.864537\n",
      "Step 265: Discriminator Loss: 29.092215 loss1 : 29.092215\n",
      "Step 266: Discriminator Loss: 47.993999 loss1 : 47.993999\n",
      "Step 267: Discriminator Loss: 3.144531 loss1 : 3.144531\n",
      "Step 268: Discriminator Loss: -79.232559 loss1 : 79.232559\n",
      "Step 269: Discriminator Loss: -90.844864 loss1 : 90.844864\n",
      "Step 270: Discriminator Loss: -83.676895 loss1 : 83.676895\n",
      "Step 271: Discriminator Loss: -21.482283 loss1 : 21.482283\n",
      "Step 272: Discriminator Loss: 87.787109 loss1 : 87.787109\n",
      "Step 273: Discriminator Loss: 112.208565 loss1 : 112.208565\n",
      "Step 274: Discriminator Loss: 112.921593 loss1 : 112.921593\n",
      "Step 275: Discriminator Loss: 60.990654 loss1 : 60.990654\n",
      "Step 276: Discriminator Loss: -19.860769 loss1 : 19.860769\n",
      "Step 277: Discriminator Loss: -57.702568 loss1 : 57.702568\n",
      "Step 278: Discriminator Loss: -53.801201 loss1 : 53.801201\n",
      "Step 279: Discriminator Loss: 5.922712 loss1 : 5.922712\n",
      "Step 280: Discriminator Loss: 1.187639 loss1 : 1.187639\n",
      "Step 281: Discriminator Loss: -33.224751 loss1 : 33.224751\n",
      "Step 282: Discriminator Loss: -14.027902 loss1 : 14.027902\n",
      "Step 283: Discriminator Loss: 24.132534 loss1 : 24.132534\n",
      "Step 284: Discriminator Loss: 32.284878 loss1 : 32.284878\n",
      "Step 285: Discriminator Loss: -2.563337 loss1 : 2.563337\n",
      "Step 286: Discriminator Loss: -0.942383 loss1 : 0.942383\n",
      "Step 287: Discriminator Loss: 45.023018 loss1 : 45.023018\n",
      "Step 288: Discriminator Loss: 39.015205 loss1 : 39.015205\n",
      "Step 289: Discriminator Loss: -8.564592 loss1 : 8.564592\n",
      "Step 290: Discriminator Loss: -15.031529 loss1 : 15.031529\n",
      "Step 291: Discriminator Loss: 47.448940 loss1 : 47.448940\n",
      "Step 292: Discriminator Loss: 50.848213 loss1 : 50.848213\n",
      "Step 293: Discriminator Loss: -0.287946 loss1 : 0.287946\n",
      "Step 294: Discriminator Loss: 4.845563 loss1 : 4.845563\n",
      "Step 295: Discriminator Loss: -43.875835 loss1 : 43.875835\n",
      "Step 296: Discriminator Loss: -28.453264 loss1 : 28.453264\n",
      "Step 297: Discriminator Loss: 13.618722 loss1 : 13.618722\n",
      "Step 298: Discriminator Loss: 25.534597 loss1 : 25.534597\n",
      "Step 299: Discriminator Loss: -20.499163 loss1 : 20.499163\n",
      "Step 300: Discriminator Loss: -27.559570 loss1 : 27.559570\n",
      "-1.6875826\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "forceInit = True\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 5*10**-3\n",
    "i = 28\n",
    "nom = 'Moyenne_image_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"D:/Github/ImagesAnalyseVoile/IA/tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver\n",
    "\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "        return couche,kernelBiasList,saver\n",
    "    \n",
    "    inpt = tf.placeholder(tf.float32,shape=[batch_size,4],name='inpt')\n",
    "    moyenne = tf.placeholder(tf.float32,shape=[batch_size,1],name='moyenne')\n",
    "    \n",
    "    import numpy as np\n",
    "    couche,kernelBiasList,saver = discriminatorDense(inpt,[4,5,6,5,1],'Lien')\n",
    "    \n",
    "    loss = tf.reduce_mean(couche-inpt)  # This optimizes the discriminator.\n",
    "    loss1 = tf.sign(loss)*loss\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",loss1)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    train_disc = optimizer_disc.minimize(loss1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14:16]) > lastTrained:\n",
    "              lastTrained = int(elem[14:16])\n",
    "          tf_saver.restore(sess, 'D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        lastPrec = 1\n",
    "        pente = []\n",
    "        for i in range(0,301):\n",
    "#           print(batch_size,illuminationLn, separationLn,ISO,tpose, moyennes)\n",
    "          inptLayer, moyenneLayer = next_batch(batch_size,illuminationLn, separationLn,ISO,tpose, moyennes)#Problèmes si gen_input\n",
    "#           print(type(inpt),type(moyenne))\n",
    "          _,dl,l1 = sess.run([train_disc,loss,loss1], feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "#           save(disc_saver,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Discriminator Loss: %f loss1 : %f' % (i, dl,l1))\n",
    "          pente.append((dl-lastPrec)/lastPrec)\n",
    "#           print('Pente : %f'%(pente[-1]))\n",
    "          lastPrec = dl\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        print(np.mean(pente[1:]))\n",
    "        tf_saver.save(sess, 'D:/Github/ImagesAnalyseVoile/IA/model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def submitImageData(batch_size, illuminationLn, separationLn,ISO,tpose, moyennes):\n",
    "    illuminationLnArray = np.reshape(np.array([illuminationLn for _ in range(batch_size)]),(batch_size,1))\n",
    "    separationLnArray = np.reshape(np.array([separationLn for _ in range(batch_size)]),(batch_size,1))\n",
    "    ISOArray = np.reshape(np.array([ISO for _ in range(batch_size)]),(batch_size,1))\n",
    "    tposeArray = np.reshape(np.array([tpose for _ in range(batch_size)]),(batch_size,1))\n",
    "    moyennesArray = np.reshape(np.array([moyennes for _ in range(batch_size)]),(batch_size,1))\n",
    "    inpt = np.concatenate((illuminationLnArray,separationLnArray,ISOArray,tposeArray),axis=1)\n",
    "    return np.array(inpt,dtype=np.float32),np.array(moyennesArray,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de denseLien_1 : (7, 4)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_2 : (7, 5)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_3 : (7, 6)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_4 : (7, 5)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_5 : (7, 1)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Restoring parameters from D:/Github/ImagesAnalyseVoile/IA/model/modelMoyenne_image_28.ckpt\n",
      "[[3278.1338]\n",
      " [3278.1338]\n",
      " [3278.1338]\n",
      " [3278.1338]\n",
      " [3278.134 ]\n",
      " [3278.134 ]\n",
      " [3278.134 ]]\n",
      "[[100.55385]\n",
      " [100.55385]\n",
      " [100.55385]\n",
      " [100.55385]\n",
      " [100.55385]\n",
      " [100.55385]\n",
      " [100.55385]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "forceInit = True\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 5*10**-3\n",
    "i = 28\n",
    "nom = 'Moyenne_image_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"D:/Github/ImagesAnalyseVoile/IA/tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver\n",
    "\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "        return couche,kernelBiasList,saver\n",
    "    \n",
    "    inpt = tf.placeholder(tf.float32,shape=[batch_size,4],name='inpt')\n",
    "    moyenne = tf.placeholder(tf.float32,shape=[batch_size,1],name='moyenne')\n",
    "    \n",
    "    import numpy as np\n",
    "    couche,kernelBiasList,saver = discriminatorDense(inpt,[4,5,6,5,1],'Lien')\n",
    "    \n",
    "    loss = tf.reduce_mean(couche-inpt)  # This optimizes the discriminator.\n",
    "    loss1 = tf.sign(loss)*loss\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",loss1)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    train_disc = optimizer_disc.minimize(loss1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        tf_saver.restore(sess, 'D:/Github/ImagesAnalyseVoile/IA/model/model'+nom+'.ckpt')\n",
    "        inptLayer, moyenneLayer = submitImageData(batch_size,illuminationLn[0], separationLn[0],ISO[0],tpose[0],moyennes[0])\n",
    "        moyenneResult = couche.eval(feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "        print(moyenneResult)\n",
    "        print(moyenneLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème : l'image est sur 8 bit donc une valeur moyenne à 3278 est absurde et va tout saturer... -> On change la fonction d'activation si on dépasse 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, illuminationLn, separationLn,ISO,tpose, moyennes):\n",
    "    indices = np.arange(0,len(illuminationLn),1)\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[0:batch_size]\n",
    "    illuminationLnArray = np.reshape(np.array([illuminationLn[i] for i in indices]),(batch_size,1))\n",
    "    separationLnArray = np.reshape(np.array([separationLn[i] for i in indices]),(batch_size,1))\n",
    "    ISOArray = np.reshape(np.array([ISO[i] for i in indices]),(batch_size,1))\n",
    "    tposeArray = np.reshape(np.array([tpose[i] for i in indices]),(batch_size,1))\n",
    "    moyennesArray = np.reshape(np.array([moyennes[i] for i in indices]),(batch_size,1))\n",
    "    inpt = np.concatenate((illuminationLnArray,separationLnArray,ISOArray,tposeArray),axis=1)\n",
    "    return np.array(inpt,dtype=np.float32),np.array(moyennesArray,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_batch(7,illuminationLn, separationLn,ISO,tpose, moyennes)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(illuminationLn, separationLn,ISO,tpose, moyennes):\n",
    "    indices = np.arange(0,len(illuminationLn),1)\n",
    "    np.random.shuffle(indices)\n",
    "    illuminationLnArray = np.reshape(np.array([illuminationLn[i] for i in indices]),(len(separationLn),1))\n",
    "    separationLnArray = np.reshape(np.array([separationLn[i] for i in indices]),(len(separationLn),1))\n",
    "    ISOArray = np.reshape(np.array([ISO[i] for i in indices]),(len(separationLn),1))\n",
    "    tposeArray = np.reshape(np.array([tpose[i] for i in indices]),(len(separationLn),1))\n",
    "    moyennesArray = np.reshape(np.array([moyennes[i] for i in indices]),(len(separationLn),1))\n",
    "    inpt = np.concatenate((illuminationLnArray,separationLnArray,ISOArray,tposeArray),axis=1)\n",
    "    return np.array(inpt,dtype=np.float32),np.array(moyennesArray,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de denseLien_1 : (7, 4)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_2 : (7, 100)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_3 : (7, 150)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_4 : (7, 100)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_5 : (7, 50)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_6 : (7, 1)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "['checkpoint', 'checkpoint_at_35.ckpt.data-00000-of-00001', 'checkpoint_at_35.ckpt.index', 'checkpoint_at_35.ckpt.meta', 'checkpoint_at_40.ckpt.data-00000-of-00001', 'checkpoint_at_40.ckpt.index', 'checkpoint_at_40.ckpt.meta', 'checkpoint_at_45.ckpt.data-00000-of-00001', 'checkpoint_at_45.ckpt.index', 'checkpoint_at_45.ckpt.meta', 'checkpoint_at_50.ckpt.data-00000-of-00001', 'checkpoint_at_50.ckpt.index', 'checkpoint_at_50.ckpt.meta']\n",
      "INFO:tensorflow:Restoring parameters from D:/Github/ImagesAnalyseVoile/IA/checkpoints_Moyenne_image_0/checkpoint_at_50.ckpt\n",
      "Entrainement....\n",
      "Step 0: Discriminator Loss: 38452.437500\n",
      "Step 1: Discriminator Loss: 38248.878906\n",
      "Step 2: Discriminator Loss: 36917.433594\n",
      "Step 3: Discriminator Loss: 38842.480469\n",
      "Step 4: Discriminator Loss: 27158.371094\n",
      "Step 5: Discriminator Loss: 38862.113281\n",
      "Step 6: Discriminator Loss: 40075.167969\n",
      "Step 7: Discriminator Loss: 35894.636719\n",
      "Step 8: Discriminator Loss: 44907.523438\n",
      "Step 9: Discriminator Loss: 28498.685547\n",
      "Step 10: Discriminator Loss: 26483.404297\n",
      "Step 11: Discriminator Loss: 36689.488281\n",
      "Step 12: Discriminator Loss: 36342.132812\n",
      "Step 13: Discriminator Loss: 32112.943359\n",
      "Step 14: Discriminator Loss: 35514.582031\n",
      "Step 15: Discriminator Loss: 38406.628906\n",
      "Step 16: Discriminator Loss: 55722.472656\n",
      "Step 17: Discriminator Loss: 41704.011719\n",
      "Step 18: Discriminator Loss: 38423.136719\n",
      "Step 19: Discriminator Loss: 40300.335938\n",
      "Step 20: Discriminator Loss: 48828.937500\n",
      "Step 21: Discriminator Loss: 26905.330078\n",
      "Step 22: Discriminator Loss: 33866.675781\n",
      "Step 23: Discriminator Loss: 42691.964844\n",
      "Step 24: Discriminator Loss: 52636.394531\n",
      "Step 25: Discriminator Loss: 42690.816406\n",
      "Step 26: Discriminator Loss: 24481.291016\n",
      "Step 27: Discriminator Loss: 41680.335938\n",
      "Step 28: Discriminator Loss: 38925.253906\n",
      "Step 29: Discriminator Loss: 51675.656250\n",
      "Step 30: Discriminator Loss: 34865.839844\n",
      "Step 31: Discriminator Loss: 55712.937500\n",
      "Step 32: Discriminator Loss: 34627.687500\n",
      "Step 33: Discriminator Loss: 37903.121094\n",
      "Step 34: Discriminator Loss: 38274.429688\n",
      "Step 35: Discriminator Loss: 53418.000000\n",
      "Step 36: Discriminator Loss: 41503.945312\n",
      "Step 37: Discriminator Loss: 55636.160156\n",
      "Step 38: Discriminator Loss: 31853.990234\n",
      "Step 39: Discriminator Loss: 46056.261719\n",
      "Step 40: Discriminator Loss: 49313.648438\n",
      "Step 41: Discriminator Loss: 38617.464844\n",
      "Step 42: Discriminator Loss: 43826.464844\n",
      "Step 43: Discriminator Loss: 45069.101562\n",
      "Step 44: Discriminator Loss: 48851.542969\n",
      "Step 45: Discriminator Loss: 42504.792969\n",
      "Step 46: Discriminator Loss: 33919.843750\n",
      "Step 47: Discriminator Loss: 45511.019531\n",
      "Step 48: Discriminator Loss: 41120.101562\n",
      "Step 49: Discriminator Loss: 28412.923828\n",
      "Step 50: Discriminator Loss: 44886.250000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "forceInit = False\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 0.0002\n",
    "i = 0\n",
    "nom = 'Moyenne_image_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"D:/Github/ImagesAnalyseVoile/IA/tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "        return couche,kernelBiasList,saver\n",
    "    \n",
    "    inpt = tf.placeholder(tf.float32,shape=[batch_size,4],name='inpt')\n",
    "    moyenne = tf.placeholder(tf.float32,shape=[batch_size,1],name='moyenne')\n",
    "    \n",
    "    import numpy as np\n",
    "    couche,kernelBiasList,saver = discriminatorDense(inpt,[4,100,150,100,50,1],'Lien')\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(couche-inpt))  # This optimizes the discriminator.\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",loss)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    train_disc = optimizer_disc.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14:16]) > lastTrained:\n",
    "              lastTrained = int(elem[14:16])\n",
    "          tf_saver.restore(sess, 'D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(0,51):\n",
    "#           print(batch_size,illuminationLn, separationLn,ISO,tpose, moyennes)\n",
    "          inptLayer, moyenneLayer = next_batch(batch_size,illuminationLn, separationLn,ISO,tpose, moyennes)#Problèmes si gen_input\n",
    "#           print(type(inpt),type(moyenne))\n",
    "          _,dl = sess.run([train_disc,loss], feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "#           save(disc_saver,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        tf_saver.save(sess, 'D:/Github/ImagesAnalyseVoile/IA/model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de denseLien_1 : (7, 4)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_2 : (7, 1)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "No previous training found...\n",
      "Entrainement....\n",
      "Step 0: Discriminator Loss: 377.842285 loss1 : 377.842285\n",
      "Step 1: Discriminator Loss: 354.464355 loss1 : 354.464355\n",
      "Step 2: Discriminator Loss: 332.076630 loss1 : 332.076630\n",
      "Step 3: Discriminator Loss: 307.846405 loss1 : 307.846405\n",
      "Step 4: Discriminator Loss: 285.816345 loss1 : 285.816345\n",
      "Step 5: Discriminator Loss: 261.933533 loss1 : 261.933533\n",
      "Step 6: Discriminator Loss: 240.051208 loss1 : 240.051208\n",
      "Step 7: Discriminator Loss: 214.381012 loss1 : 214.381012\n",
      "Step 8: Discriminator Loss: 194.285553 loss1 : 194.285553\n",
      "Step 9: Discriminator Loss: 170.101456 loss1 : 170.101456\n",
      "Step 10: Discriminator Loss: 148.519165 loss1 : 148.519165\n",
      "Step 11: Discriminator Loss: 122.593872 loss1 : 122.593872\n",
      "Step 12: Discriminator Loss: 103.256699 loss1 : 103.256699\n",
      "Step 13: Discriminator Loss: 80.375580 loss1 : 80.375580\n",
      "Step 14: Discriminator Loss: 57.495438 loss1 : 57.495438\n",
      "Step 15: Discriminator Loss: 32.357491 loss1 : 32.357491\n",
      "Step 16: Discriminator Loss: 11.733456 loss1 : 11.733456\n",
      "Step 17: Discriminator Loss: -11.147416 loss1 : 11.147416\n",
      "Step 18: Discriminator Loss: -31.111420 loss1 : 31.111420\n",
      "Step 19: Discriminator Loss: -43.974739 loss1 : 43.974739\n",
      "Step 20: Discriminator Loss: -50.225174 loss1 : 50.225174\n",
      "Step 21: Discriminator Loss: -57.969673 loss1 : 57.969673\n",
      "Step 22: Discriminator Loss: -58.024406 loss1 : 58.024406\n",
      "Step 23: Discriminator Loss: -59.396217 loss1 : 59.396217\n",
      "Step 24: Discriminator Loss: -56.308262 loss1 : 56.308262\n",
      "Step 25: Discriminator Loss: -49.089397 loss1 : 49.089397\n",
      "Step 26: Discriminator Loss: -41.482307 loss1 : 41.482307\n",
      "Step 27: Discriminator Loss: -35.148621 loss1 : 35.148621\n",
      "Step 28: Discriminator Loss: -22.961386 loss1 : 22.961386\n",
      "Step 29: Discriminator Loss: -13.225780 loss1 : 13.225780\n",
      "Step 30: Discriminator Loss: 1.387535 loss1 : 1.387535\n",
      "Step 31: Discriminator Loss: 10.959023 loss1 : 10.959023\n",
      "Step 32: Discriminator Loss: 16.715252 loss1 : 16.715252\n",
      "Step 33: Discriminator Loss: 19.012920 loss1 : 19.012920\n",
      "Step 34: Discriminator Loss: 19.684799 loss1 : 19.684799\n",
      "Step 35: Discriminator Loss: 15.275818 loss1 : 15.275818\n",
      "Step 36: Discriminator Loss: 13.174400 loss1 : 13.174400\n",
      "Step 37: Discriminator Loss: 6.458691 loss1 : 6.458691\n",
      "Step 38: Discriminator Loss: -3.183819 loss1 : 3.183819\n",
      "Step 39: Discriminator Loss: -5.707359 loss1 : 5.707359\n",
      "Step 40: Discriminator Loss: -7.673712 loss1 : 7.673712\n",
      "Step 41: Discriminator Loss: -9.366081 loss1 : 9.366081\n",
      "Step 42: Discriminator Loss: -4.312555 loss1 : 4.312555\n",
      "Step 43: Discriminator Loss: 0.526907 loss1 : 0.526907\n",
      "Step 44: Discriminator Loss: 0.381447 loss1 : 0.381447\n",
      "Step 45: Discriminator Loss: 1.587488 loss1 : 1.587488\n",
      "Step 46: Discriminator Loss: -0.637182 loss1 : 0.637182\n",
      "Step 47: Discriminator Loss: -2.978981 loss1 : 2.978981\n",
      "Step 48: Discriminator Loss: 0.870186 loss1 : 0.870186\n",
      "Step 49: Discriminator Loss: 0.520813 loss1 : 0.520813\n",
      "Step 50: Discriminator Loss: -2.092037 loss1 : 2.092037\n",
      "Step 51: Discriminator Loss: -1.638670 loss1 : 1.638670\n",
      "Step 52: Discriminator Loss: 0.102102 loss1 : 0.102102\n",
      "Step 53: Discriminator Loss: -0.168655 loss1 : 0.168655\n",
      "Step 54: Discriminator Loss: 1.878996 loss1 : 1.878996\n",
      "Step 55: Discriminator Loss: 1.934650 loss1 : 1.934650\n",
      "Step 56: Discriminator Loss: -2.906960 loss1 : 2.906960\n",
      "Step 57: Discriminator Loss: -3.029034 loss1 : 3.029034\n",
      "Step 58: Discriminator Loss: 0.781805 loss1 : 0.781805\n",
      "Step 59: Discriminator Loss: 0.454161 loss1 : 0.454161\n",
      "Step 60: Discriminator Loss: -1.626707 loss1 : 1.626707\n",
      "Step 61: Discriminator Loss: -2.169922 loss1 : 2.169922\n",
      "Step 62: Discriminator Loss: -1.489697 loss1 : 1.489697\n",
      "Step 63: Discriminator Loss: 2.856590 loss1 : 2.856590\n",
      "Step 64: Discriminator Loss: 6.007504 loss1 : 6.007504\n",
      "Step 65: Discriminator Loss: 5.155678 loss1 : 5.155678\n",
      "Step 66: Discriminator Loss: 0.559418 loss1 : 0.559418\n",
      "Step 67: Discriminator Loss: -4.496841 loss1 : 4.496841\n",
      "Step 68: Discriminator Loss: -4.677801 loss1 : 4.677801\n",
      "Step 69: Discriminator Loss: -4.409973 loss1 : 4.409973\n",
      "Step 70: Discriminator Loss: -3.929203 loss1 : 3.929203\n",
      "Step 71: Discriminator Loss: 2.685884 loss1 : 2.685884\n",
      "Step 72: Discriminator Loss: 2.494278 loss1 : 2.494278\n",
      "Step 73: Discriminator Loss: 3.354368 loss1 : 3.354368\n",
      "Step 74: Discriminator Loss: 0.987362 loss1 : 0.987362\n",
      "Step 75: Discriminator Loss: -3.883839 loss1 : 3.883839\n",
      "Step 76: Discriminator Loss: -5.981785 loss1 : 5.981785\n",
      "Step 77: Discriminator Loss: -6.089443 loss1 : 6.089443\n",
      "Step 78: Discriminator Loss: -4.966095 loss1 : 4.966095\n",
      "Step 79: Discriminator Loss: -0.276295 loss1 : 0.276295\n",
      "Step 80: Discriminator Loss: 6.246552 loss1 : 6.246552\n",
      "Step 81: Discriminator Loss: 8.283737 loss1 : 8.283737\n",
      "Step 82: Discriminator Loss: 10.724922 loss1 : 10.724922\n",
      "Step 83: Discriminator Loss: 5.490759 loss1 : 5.490759\n",
      "Step 84: Discriminator Loss: 7.612995 loss1 : 7.612995\n",
      "Step 85: Discriminator Loss: 0.068687 loss1 : 0.068687\n",
      "Step 86: Discriminator Loss: -5.695879 loss1 : 5.695879\n",
      "Step 87: Discriminator Loss: -9.448414 loss1 : 9.448414\n",
      "Step 88: Discriminator Loss: -10.995078 loss1 : 10.995078\n",
      "Step 89: Discriminator Loss: -12.174503 loss1 : 12.174503\n",
      "Step 90: Discriminator Loss: -7.508907 loss1 : 7.508907\n",
      "Step 91: Discriminator Loss: -3.989021 loss1 : 3.989021\n",
      "Step 92: Discriminator Loss: 2.887840 loss1 : 2.887840\n",
      "Step 93: Discriminator Loss: 8.299545 loss1 : 8.299545\n",
      "Step 94: Discriminator Loss: 9.504894 loss1 : 9.504894\n",
      "Step 95: Discriminator Loss: 8.303696 loss1 : 8.303696\n",
      "Step 96: Discriminator Loss: 4.938387 loss1 : 4.938387\n",
      "Step 97: Discriminator Loss: -0.377892 loss1 : 0.377892\n",
      "Step 98: Discriminator Loss: -2.371355 loss1 : 2.371355\n",
      "Step 99: Discriminator Loss: -2.333511 loss1 : 2.333511\n",
      "Step 100: Discriminator Loss: -0.518764 loss1 : 0.518764\n",
      "Step 101: Discriminator Loss: 4.359016 loss1 : 4.359016\n",
      "Step 102: Discriminator Loss: 5.503584 loss1 : 5.503584\n",
      "Step 103: Discriminator Loss: 4.703274 loss1 : 4.703274\n",
      "Step 104: Discriminator Loss: 1.696373 loss1 : 1.696373\n",
      "Step 105: Discriminator Loss: -4.857693 loss1 : 4.857693\n",
      "Step 106: Discriminator Loss: -4.990553 loss1 : 4.990553\n",
      "Step 107: Discriminator Loss: -5.190741 loss1 : 5.190741\n",
      "Step 108: Discriminator Loss: -2.125988 loss1 : 2.125988\n",
      "Step 109: Discriminator Loss: 1.958340 loss1 : 1.958340\n",
      "Step 110: Discriminator Loss: 3.805061 loss1 : 3.805061\n",
      "Step 111: Discriminator Loss: 3.179314 loss1 : 3.179314\n",
      "Step 112: Discriminator Loss: 0.331352 loss1 : 0.331352\n",
      "Step 113: Discriminator Loss: -6.058647 loss1 : 6.058647\n",
      "Step 114: Discriminator Loss: -6.087990 loss1 : 6.087990\n",
      "Step 115: Discriminator Loss: -7.700043 loss1 : 7.700043\n",
      "Step 116: Discriminator Loss: -2.999056 loss1 : 2.999056\n",
      "Step 117: Discriminator Loss: 1.185373 loss1 : 1.185373\n",
      "Step 118: Discriminator Loss: 3.118277 loss1 : 3.118277\n",
      "Step 119: Discriminator Loss: 1.104836 loss1 : 1.104836\n",
      "Step 120: Discriminator Loss: 0.293985 loss1 : 0.293985\n",
      "Step 121: Discriminator Loss: -4.998356 loss1 : 4.998356\n",
      "Step 122: Discriminator Loss: -7.024523 loss1 : 7.024523\n",
      "Step 123: Discriminator Loss: -6.562355 loss1 : 6.562355\n",
      "Step 124: Discriminator Loss: -6.860149 loss1 : 6.860149\n",
      "Step 125: Discriminator Loss: 0.864565 loss1 : 0.864565\n",
      "Step 126: Discriminator Loss: 3.332380 loss1 : 3.332380\n",
      "Step 127: Discriminator Loss: 0.867523 loss1 : 0.867523\n",
      "Step 128: Discriminator Loss: -0.445034 loss1 : 0.445034\n",
      "Step 129: Discriminator Loss: -0.137665 loss1 : 0.137665\n",
      "Step 130: Discriminator Loss: 1.468582 loss1 : 1.468582\n",
      "Step 131: Discriminator Loss: -0.365898 loss1 : 0.365898\n",
      "Step 132: Discriminator Loss: 3.024525 loss1 : 3.024525\n",
      "Step 133: Discriminator Loss: 2.990820 loss1 : 2.990820\n",
      "Step 134: Discriminator Loss: -0.281944 loss1 : 0.281944\n",
      "Step 135: Discriminator Loss: -1.924526 loss1 : 1.924526\n",
      "Step 136: Discriminator Loss: 0.196365 loss1 : 0.196365\n",
      "Step 137: Discriminator Loss: 1.723703 loss1 : 1.723703\n",
      "Step 138: Discriminator Loss: -0.919422 loss1 : 0.919422\n",
      "Step 139: Discriminator Loss: -1.011465 loss1 : 1.011465\n",
      "Step 140: Discriminator Loss: 1.191463 loss1 : 1.191463\n",
      "Step 141: Discriminator Loss: -1.029247 loss1 : 1.029247\n",
      "Step 142: Discriminator Loss: 2.905369 loss1 : 2.905369\n",
      "Step 143: Discriminator Loss: 2.436609 loss1 : 2.436609\n",
      "Step 144: Discriminator Loss: -0.271704 loss1 : 0.271704\n",
      "Step 145: Discriminator Loss: -0.925514 loss1 : 0.925514\n",
      "Step 146: Discriminator Loss: -0.159843 loss1 : 0.159843\n",
      "Step 147: Discriminator Loss: 5.450846 loss1 : 5.450846\n",
      "Step 148: Discriminator Loss: 5.615669 loss1 : 5.615669\n",
      "Step 149: Discriminator Loss: 4.690361 loss1 : 4.690361\n",
      "Step 150: Discriminator Loss: 3.420027 loss1 : 3.420027\n",
      "Step 151: Discriminator Loss: -1.678228 loss1 : 1.678228\n",
      "Step 152: Discriminator Loss: -5.870663 loss1 : 5.870663\n",
      "Step 153: Discriminator Loss: -3.760794 loss1 : 3.760794\n",
      "Step 154: Discriminator Loss: -1.278561 loss1 : 1.278561\n",
      "Step 155: Discriminator Loss: 2.739603 loss1 : 2.739603\n",
      "Step 156: Discriminator Loss: 3.169083 loss1 : 3.169083\n",
      "Step 157: Discriminator Loss: 2.476936 loss1 : 2.476936\n",
      "Step 158: Discriminator Loss: 0.927065 loss1 : 0.927065\n",
      "Step 159: Discriminator Loss: -3.475630 loss1 : 3.475630\n",
      "Step 160: Discriminator Loss: -7.513763 loss1 : 7.513763\n",
      "Step 161: Discriminator Loss: -5.731771 loss1 : 5.731771\n",
      "Step 162: Discriminator Loss: -2.603540 loss1 : 2.603540\n",
      "Step 163: Discriminator Loss: 2.043777 loss1 : 2.043777\n",
      "Step 164: Discriminator Loss: 2.049187 loss1 : 2.049187\n",
      "Step 165: Discriminator Loss: 1.459824 loss1 : 1.459824\n",
      "Step 166: Discriminator Loss: -1.374958 loss1 : 1.374958\n",
      "Step 167: Discriminator Loss: -1.625206 loss1 : 1.625206\n",
      "Step 168: Discriminator Loss: 0.452965 loss1 : 0.452965\n",
      "Step 169: Discriminator Loss: 0.022537 loss1 : 0.022537\n",
      "Step 170: Discriminator Loss: -0.734846 loss1 : 0.734846\n",
      "Step 171: Discriminator Loss: -2.798325 loss1 : 2.798325\n",
      "Step 172: Discriminator Loss: 0.821350 loss1 : 0.821350\n",
      "Step 173: Discriminator Loss: 0.498102 loss1 : 0.498102\n",
      "Step 174: Discriminator Loss: -3.524414 loss1 : 3.524414\n",
      "Step 175: Discriminator Loss: -3.545658 loss1 : 3.545658\n",
      "Step 176: Discriminator Loss: -1.265335 loss1 : 1.265335\n",
      "Step 177: Discriminator Loss: 3.089745 loss1 : 3.089745\n",
      "Step 178: Discriminator Loss: 6.603691 loss1 : 6.603691\n",
      "Step 179: Discriminator Loss: 5.260900 loss1 : 5.260900\n",
      "Step 180: Discriminator Loss: 2.723236 loss1 : 2.723236\n",
      "Step 181: Discriminator Loss: -2.299091 loss1 : 2.299091\n",
      "Step 182: Discriminator Loss: -4.533760 loss1 : 4.533760\n",
      "Step 183: Discriminator Loss: -4.762268 loss1 : 4.762268\n",
      "Step 184: Discriminator Loss: -3.616951 loss1 : 3.616951\n",
      "Step 185: Discriminator Loss: 2.843355 loss1 : 2.843355\n",
      "Step 186: Discriminator Loss: 2.812386 loss1 : 2.812386\n",
      "Step 187: Discriminator Loss: 3.523312 loss1 : 3.523312\n",
      "Step 188: Discriminator Loss: -0.705753 loss1 : 0.705753\n",
      "Step 189: Discriminator Loss: 0.867970 loss1 : 0.867970\n",
      "Step 190: Discriminator Loss: -1.677252 loss1 : 1.677252\n",
      "Step 191: Discriminator Loss: -2.186529 loss1 : 2.186529\n",
      "Step 192: Discriminator Loss: 0.597212 loss1 : 0.597212\n",
      "Step 193: Discriminator Loss: 0.363808 loss1 : 0.363808\n",
      "Step 194: Discriminator Loss: -5.499527 loss1 : 5.499527\n",
      "Step 195: Discriminator Loss: -2.589859 loss1 : 2.589859\n",
      "Step 196: Discriminator Loss: -0.263638 loss1 : 0.263638\n",
      "Step 197: Discriminator Loss: 4.115366 loss1 : 4.115366\n",
      "Step 198: Discriminator Loss: 5.769697 loss1 : 5.769697\n",
      "Step 199: Discriminator Loss: 3.687897 loss1 : 3.687897\n",
      "Step 200: Discriminator Loss: 0.672935 loss1 : 0.672935\n",
      "Step 201: Discriminator Loss: -2.531033 loss1 : 2.531033\n",
      "Step 202: Discriminator Loss: -5.245192 loss1 : 5.245192\n",
      "Step 203: Discriminator Loss: -4.449041 loss1 : 4.449041\n",
      "Step 204: Discriminator Loss: -1.899181 loss1 : 1.899181\n",
      "Step 205: Discriminator Loss: 0.866058 loss1 : 0.866058\n",
      "Step 206: Discriminator Loss: 4.011360 loss1 : 4.011360\n",
      "Step 207: Discriminator Loss: 3.876579 loss1 : 3.876579\n",
      "Step 208: Discriminator Loss: 0.515190 loss1 : 0.515190\n",
      "Step 209: Discriminator Loss: -4.344292 loss1 : 4.344292\n",
      "Step 210: Discriminator Loss: -5.928078 loss1 : 5.928078\n",
      "Step 211: Discriminator Loss: -6.024015 loss1 : 6.024015\n",
      "Step 212: Discriminator Loss: -3.371126 loss1 : 3.371126\n",
      "Step 213: Discriminator Loss: -0.062672 loss1 : 0.062672\n",
      "Step 214: Discriminator Loss: 8.301483 loss1 : 8.301483\n",
      "Step 215: Discriminator Loss: 10.023239 loss1 : 10.023239\n",
      "Step 216: Discriminator Loss: 12.273258 loss1 : 12.273258\n",
      "Step 217: Discriminator Loss: 10.811069 loss1 : 10.811069\n",
      "Step 218: Discriminator Loss: 7.710159 loss1 : 7.710159\n",
      "Step 219: Discriminator Loss: 0.305740 loss1 : 0.305740\n",
      "Step 220: Discriminator Loss: -6.986408 loss1 : 6.986408\n",
      "Step 221: Discriminator Loss: -9.332204 loss1 : 9.332204\n",
      "Step 222: Discriminator Loss: -11.374229 loss1 : 11.374229\n",
      "Step 223: Discriminator Loss: -10.471377 loss1 : 10.471377\n",
      "Step 224: Discriminator Loss: -7.374142 loss1 : 7.374142\n",
      "Step 225: Discriminator Loss: -2.301765 loss1 : 2.301765\n",
      "Step 226: Discriminator Loss: 5.052389 loss1 : 5.052389\n",
      "Step 227: Discriminator Loss: 7.056381 loss1 : 7.056381\n",
      "Step 228: Discriminator Loss: 8.254890 loss1 : 8.254890\n",
      "Step 229: Discriminator Loss: 7.030502 loss1 : 7.030502\n",
      "Step 230: Discriminator Loss: 5.042147 loss1 : 5.042147\n",
      "Step 231: Discriminator Loss: -1.728477 loss1 : 1.728477\n",
      "Step 232: Discriminator Loss: -2.287127 loss1 : 2.287127\n",
      "Step 233: Discriminator Loss: -2.254898 loss1 : 2.254898\n",
      "Step 234: Discriminator Loss: -0.444546 loss1 : 0.444546\n",
      "Step 235: Discriminator Loss: 2.469124 loss1 : 2.469124\n",
      "Step 236: Discriminator Loss: 6.068769 loss1 : 6.068769\n",
      "Step 237: Discriminator Loss: 5.259426 loss1 : 5.259426\n",
      "Step 238: Discriminator Loss: -4.587807 loss1 : 4.587807\n",
      "Step 239: Discriminator Loss: 1.317701 loss1 : 1.317701\n",
      "Step 240: Discriminator Loss: -1.350571 loss1 : 1.350571\n",
      "Step 241: Discriminator Loss: -0.962389 loss1 : 0.962389\n",
      "Step 242: Discriminator Loss: -0.780212 loss1 : 0.780212\n",
      "Step 243: Discriminator Loss: 4.969881 loss1 : 4.969881\n",
      "Step 244: Discriminator Loss: 7.016544 loss1 : 7.016544\n",
      "Step 245: Discriminator Loss: 0.397873 loss1 : 0.397873\n",
      "Step 246: Discriminator Loss: 2.528666 loss1 : 2.528666\n",
      "Step 247: Discriminator Loss: -2.537851 loss1 : 2.537851\n",
      "Step 248: Discriminator Loss: -6.300598 loss1 : 6.300598\n",
      "Step 249: Discriminator Loss: -4.060846 loss1 : 4.060846\n",
      "Step 250: Discriminator Loss: -3.526367 loss1 : 3.526367\n",
      "Step 251: Discriminator Loss: -0.483788 loss1 : 0.483788\n",
      "Step 252: Discriminator Loss: 7.457512 loss1 : 7.457512\n",
      "Step 253: Discriminator Loss: 12.827705 loss1 : 12.827705\n",
      "Step 254: Discriminator Loss: 11.757652 loss1 : 11.757652\n",
      "Step 255: Discriminator Loss: 12.098198 loss1 : 12.098198\n",
      "Step 256: Discriminator Loss: 6.514130 loss1 : 6.514130\n",
      "Step 257: Discriminator Loss: 2.309175 loss1 : 2.309175\n",
      "Step 258: Discriminator Loss: -5.021063 loss1 : 5.021063\n",
      "Step 259: Discriminator Loss: -8.829262 loss1 : 8.829262\n",
      "Step 260: Discriminator Loss: -10.424268 loss1 : 10.424268\n",
      "Step 261: Discriminator Loss: -11.534843 loss1 : 11.534843\n",
      "Step 262: Discriminator Loss: -6.516956 loss1 : 6.516956\n",
      "Step 263: Discriminator Loss: -3.417725 loss1 : 3.417725\n",
      "Step 264: Discriminator Loss: 3.432526 loss1 : 3.432526\n",
      "Step 265: Discriminator Loss: 9.194080 loss1 : 9.194080\n",
      "Step 266: Discriminator Loss: 9.870377 loss1 : 9.870377\n",
      "Step 267: Discriminator Loss: 7.250229 loss1 : 7.250229\n",
      "Step 268: Discriminator Loss: 5.756994 loss1 : 5.756994\n",
      "Step 269: Discriminator Loss: -1.508503 loss1 : 1.508503\n",
      "Step 270: Discriminator Loss: -4.026867 loss1 : 4.026867\n",
      "Step 271: Discriminator Loss: -3.990978 loss1 : 3.990978\n",
      "Step 272: Discriminator Loss: 0.249794 loss1 : 0.249794\n",
      "Step 273: Discriminator Loss: -0.452848 loss1 : 0.452848\n",
      "Step 274: Discriminator Loss: 0.272781 loss1 : 0.272781\n",
      "Step 275: Discriminator Loss: 1.758127 loss1 : 1.758127\n",
      "Step 276: Discriminator Loss: -6.595692 loss1 : 6.595692\n",
      "Step 277: Discriminator Loss: -0.980703 loss1 : 0.980703\n",
      "Step 278: Discriminator Loss: 0.715147 loss1 : 0.715147\n",
      "Step 279: Discriminator Loss: 0.911428 loss1 : 0.911428\n",
      "Step 280: Discriminator Loss: -1.650150 loss1 : 1.650150\n",
      "Step 281: Discriminator Loss: -2.172857 loss1 : 2.172857\n",
      "Step 282: Discriminator Loss: 0.597456 loss1 : 0.597456\n",
      "Step 283: Discriminator Loss: -1.514854 loss1 : 1.514854\n",
      "Step 284: Discriminator Loss: 2.424156 loss1 : 2.424156\n",
      "Step 285: Discriminator Loss: 2.002760 loss1 : 2.002760\n",
      "Step 286: Discriminator Loss: -4.038315 loss1 : 4.038315\n",
      "Step 287: Discriminator Loss: -1.273178 loss1 : 1.273178\n",
      "Step 288: Discriminator Loss: 0.913881 loss1 : 0.913881\n",
      "Step 289: Discriminator Loss: -0.719673 loss1 : 0.719673\n",
      "Step 290: Discriminator Loss: 1.303078 loss1 : 1.303078\n",
      "Step 291: Discriminator Loss: 0.823334 loss1 : 0.823334\n",
      "Step 292: Discriminator Loss: -5.812471 loss1 : 5.812471\n",
      "Step 293: Discriminator Loss: -0.280493 loss1 : 0.280493\n",
      "Step 294: Discriminator Loss: 1.346506 loss1 : 1.346506\n",
      "Step 295: Discriminator Loss: 1.479555 loss1 : 1.479555\n",
      "Step 296: Discriminator Loss: -1.138910 loss1 : 1.138910\n",
      "Step 297: Discriminator Loss: -1.710201 loss1 : 1.710201\n",
      "Step 298: Discriminator Loss: 0.511040 loss1 : 0.511040\n",
      "Step 299: Discriminator Loss: -1.074417 loss1 : 1.074417\n",
      "Step 300: Discriminator Loss: 2.260101 loss1 : 2.260101\n",
      "-2.0970998\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "forceInit = False\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 5*10**-4\n",
    "i = 33\n",
    "nom = 'Moyenne_image_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"D:/Github/ImagesAnalyseVoile/IA/tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver\n",
    "\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "        return couche,kernelBiasList,saver\n",
    "    \n",
    "    inpt = tf.placeholder(tf.float32,shape=[batch_size,4],name='inpt')\n",
    "    moyenne = tf.placeholder(tf.float32,shape=[batch_size,1],name='moyenne')\n",
    "    \n",
    "    import numpy as np\n",
    "    couche,kernelBiasList,saver = discriminatorDense(inpt,[4,1],'Lien')\n",
    "    \n",
    "    loss = tf.reduce_max(couche-inpt)  # This optimizes the discriminator.\n",
    "    loss1 = tf.sign(loss)*loss\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",loss1)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    train_disc = optimizer_disc.minimize(loss1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14:17]) > lastTrained:\n",
    "              lastTrained = int(elem[14:17])\n",
    "          tf_saver.restore(sess, 'D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        lastPrec = 1\n",
    "        pente = []\n",
    "        for i in range(0,301):\n",
    "#           print(batch_size,illuminationLn, separationLn,ISO,tpose, moyennes)\n",
    "          inptLayer, moyenneLayer = next_batch(batch_size,illuminationLn, separationLn,ISO,tpose, moyennes)#Problèmes si gen_input\n",
    "#           print(type(inpt),type(moyenne))\n",
    "          _,dl,l1 = sess.run([train_disc,loss,loss1], feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "#           save(disc_saver,gen_inpt,disc_inpt)\n",
    "          print('Step %i: Discriminator Loss: %f loss1 : %f' % (i, dl,l1))\n",
    "          pente.append((dl-lastPrec)/lastPrec)\n",
    "#           print('Pente : %f'%(pente[-1]))\n",
    "          lastPrec = dl\n",
    "          if i % 5 == 0:\n",
    "            tf_saver.save(sess,'D:/Github/ImagesAnalyseVoile/IA/checkpoints_'+nom+'/checkpoint_at_'+str(i)+'.ckpt')\n",
    "        print(np.mean(pente[1:]))\n",
    "        tf_saver.save(sess, 'D:/Github/ImagesAnalyseVoile/IA/model/model'+nom+'.ckpt')\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def submitImageData(batch_size, illuminationLn, separationLn,ISO,tpose, moyennes):\n",
    "    illuminationLnArray = np.reshape(np.array([illuminationLn for _ in range(batch_size)]),(batch_size,1))\n",
    "    separationLnArray = np.reshape(np.array([separationLn for _ in range(batch_size)]),(batch_size,1))\n",
    "    ISOArray = np.reshape(np.array([ISO for _ in range(batch_size)]),(batch_size,1))\n",
    "    tposeArray = np.reshape(np.array([tpose for _ in range(batch_size)]),(batch_size,1))\n",
    "    moyennesArray = np.reshape(np.array([moyennes for _ in range(batch_size)]),(batch_size,1))\n",
    "    inpt = np.concatenate((illuminationLnArray,separationLnArray,ISOArray,tposeArray),axis=1)\n",
    "    return np.array(inpt,dtype=np.float32),np.array(moyennesArray,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de denseLien_1 : (7, 4)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "Taille de denseLien_2 : (7, 1)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Restoring parameters from D:/Github/ImagesAnalyseVoile/IA/model/modelMoyenne_image_33.ckpt\n",
      "[[62.994953]\n",
      " [62.994953]\n",
      " [62.994953]\n",
      " [62.994953]\n",
      " [62.994953]\n",
      " [62.994953]\n",
      " [62.994953]]\n",
      "[[100.55385]\n",
      " [100.55385]\n",
      " [100.55385]\n",
      " [100.55385]\n",
      " [100.55385]\n",
      " [100.55385]\n",
      " [100.55385]]\n",
      "[[-3.6005047e+01 -5.0800968e+01 -1.2737005e+04 -1.0050468e+00]\n",
      " [-3.6005047e+01 -5.0800968e+01 -1.2737005e+04 -1.0050468e+00]\n",
      " [-3.6005047e+01 -5.0800968e+01 -1.2737005e+04 -1.0050468e+00]\n",
      " [-3.6005047e+01 -5.0800968e+01 -1.2737005e+04 -1.0050468e+00]\n",
      " [-3.6005047e+01 -5.0800968e+01 -1.2737005e+04 -1.0050468e+00]\n",
      " [-3.6005047e+01 -5.0800968e+01 -1.2737005e+04 -1.0050468e+00]\n",
      " [-3.6005047e+01 -5.0800968e+01 -1.2737005e+04 -1.0050468e+00]]\n",
      "-1.0050468\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "forceInit = True\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate = 5*10**-2\n",
    "i = 33\n",
    "nom = 'Moyenne_image_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"D:/Github/ImagesAnalyseVoile/IA/tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver\n",
    "\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "        return couche,kernelBiasList,saver\n",
    "    \n",
    "    inpt = tf.placeholder(tf.float32,shape=[batch_size,4],name='inpt')\n",
    "    moyenne = tf.placeholder(tf.float32,shape=[batch_size,1],name='moyenne')\n",
    "    \n",
    "    import numpy as np\n",
    "    couche,kernelBiasList,saver = discriminatorDense(inpt,[4,1],'Lien')\n",
    "    \n",
    "    loss = tf.reduce_max(couche-inpt)  # This optimizes the discriminator.\n",
    "    diff = couche-inpt\n",
    "    loss1 = tf.sign(loss)*loss\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",loss1)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    train_disc = optimizer_disc.minimize(loss1)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        tf_saver.restore(sess, 'D:/Github/ImagesAnalyseVoile/IA/model/model'+nom+'.ckpt')\n",
    "        inptLayer, moyenneLayer = submitImageData(batch_size,illuminationLn[0], separationLn[0],ISO[0],tpose[0],moyennes[0])\n",
    "        moyenneResult = couche.eval(feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "        difference = diff.eval(feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "        lossEval = loss.eval(feed_dict={inpt: inptLayer,moyenne: moyenneLayer})\n",
    "        print(moyenneResult)\n",
    "        print(moyenneLayer)\n",
    "        print(difference)\n",
    "        print(lossEval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion : il faut faire attention à quelle fonction de coût on utilise et particulièrement au signe du gradient qu'il est parfois nécessaire de modifier ainsi que la fonction reduce_sum pas toujours adaptée. De plus le pas d'apprentissage est un facteur très important pour modifier le comportement du modèle</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Github/ImagesAnalyseVoile/01_M51.jpg a une moyenne réelle de : 100.553851\n",
      "(433, 770, 3)\n",
      "(732, 1100, 3)\n",
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390cc8ac734e4ef49f36545afd29209f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36501315608040b0b185da828cf80f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0190c66a643f4182aa937577a29e3ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0501bfce5a91471a9b15e0ab03528ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib ipympl\n",
    "image = cv2.imread('D:/Google drive/TIPE/Images_source/clean/1.jpg',1)\n",
    "print(fichiersComplets[0]+' a une moyenne réelle de : %f'% moyennes[0])\n",
    "noiseImage = cv2.imread('D:/Google drive/TIPE/Images_source/noise/1.jpg',1)\n",
    "row,col,ch = image.shape\n",
    "image = cv2.resize(image,(col//5,row//5))\n",
    "row,col,ch = image.shape\n",
    "gauss = np.array(np.random.normal(63,1,(row,col,ch)),dtype=np.int)\n",
    "gauss1 = np.array(np.random.normal(np.mean(noiseImage),1,(row,col,ch)),dtype=np.int)\n",
    "print(gauss.shape)\n",
    "print(noisy.shape)\n",
    "print(type(image)==type(noisy))\n",
    "# print(noisy[20:50,20:50,0])\n",
    "# cv2.destroyAllWindows()\n",
    "plt.figure(4)\n",
    "plt.title('Clean')\n",
    "plt.imshow(image)\n",
    "plt.figure(3)\n",
    "plt.title('Créée avec vraie moyenne')\n",
    "custom1 = image//np.random.randint(3,7)+gauss1\n",
    "plt.imshow(custom1)\n",
    "print(np.max(custom1))\n",
    "plt.figure(2)\n",
    "plt.title('Créée avec moyenne estimée')\n",
    "custom2 = image//np.random.randint(3,7)+gauss\n",
    "print(np.max(custom2))\n",
    "plt.imshow(custom2)\n",
    "plt.figure(1)\n",
    "plt.title('Noise')\n",
    "plt.imshow(noiseImage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai non abouti avec Keras : à terme très utile mais nécessite un apprentissage complet -> pas le temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"sequential_1_input:0\", shape=(?, 4), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1063\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[1;32m-> 1064\u001b[1;33m                                                     allow_operation=False)\n\u001b[0m\u001b[0;32m   1065\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3034\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3035\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3113\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3114\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3115\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor Tensor(\"sequential_1_input:0\", shape=(?, 4), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-a835e637dc8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     28\u001b[0m     \u001b[0minpt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmoyenne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milluminationLn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparationLn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mISO\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoyennes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoyenne\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2719\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2721\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2693\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2694\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1065\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[1;32m-> 1067\u001b[1;33m                             + e.args[0])\n\u001b[0m\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"sequential_1_input:0\", shape=(?, 4), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "batch_size = 7\n",
    "learning_rate = 10**-9\n",
    "i = 13\n",
    "nom = 'Moyenne_image_'+str(i)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"D:/Github/ImagesAnalyseVoile/IA/tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "\n",
    "import numpy as np\n",
    "with tf.Graph().as_default():\n",
    "    model = Sequential([\n",
    "    layers.Dense(4, activation=tf.nn.relu),\n",
    "    layers.Dense(10, activation=tf.nn.relu),\n",
    "    layers.Dense(50, activation=tf.nn.relu),\n",
    "    layers.Dense(10, activation=tf.nn.relu),\n",
    "    layers.Dense(1, activation=tf.nn.relu),\n",
    "    ])\n",
    "    model.compile(optimizer='adam', \n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "    inpt,moyenne = data(illuminationLn, separationLn,ISO,tpose, moyennes)\n",
    "    model.fit(inpt, moyenne, epochs=500)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"algorithmeAstrobin\"></a> Maintenant il faut récupérer les images et les métadonnées associées comme dit [ici](#lienAstrobin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai de récupération d'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://cdn.astrobin.com/thumbs/Xo22AB02MYyF_1824x0_fpu6-biL.jpg\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://cdn.astrobin.com/thumbs/Xo22AB02MYyF_1824x0_fpu6-biL.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robin\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel\\__main__.py:46: DeprecationWarning: AppURLopener style of invoking requests is deprecated. Use newer urlopen functions/methods\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:/Github/ImagesAnalyseVoile/Xo22AB02MYyF_1824x0_fpu6-biL.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://www.astrobin.com/full/392940/0/?nc=all'\n",
    "def retrieveImg(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    resp = urllib.request.urlopen(req)\n",
    "    respData = resp.read().decode(\"utf-8\")\n",
    "    Limg = []\n",
    " \n",
    "    driver = webdriver.Chrome(\"D:/Github/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    images = driver.find_elements_by_tag_name('img')\n",
    "    for image in images:\n",
    "        print(image.get_attribute('src'))\n",
    "        Limg.append(image.get_attribute('src'))\n",
    "\n",
    "    driver.close()\n",
    "    print(Limg)\n",
    "    if len(Limg) == 0:\n",
    "        print('No images found')\n",
    "        file = urllib.request.urlopen(url)\n",
    "        page = file.read().decode(\"utf-8\")\n",
    "        file.close()\n",
    "        f = open('C:/Users/Robin/Desktop/PB'+url[-6:-5]+'.html', 'w')\n",
    "        f.write(page)\n",
    "        f.close\n",
    "#         raise ValueError('Error')\n",
    "        return None\n",
    "    LimgSelect = []\n",
    "    for img in Limg:\n",
    "        if img[-3:] == 'jpg':\n",
    "            LimgSelect.append(img)\n",
    "    imgPath = 'D:/Github/ImagesAnalyseVoile/'\n",
    "    if len(LimgSelect) == 0:\n",
    "        return None\n",
    "    choixIndex = 1 if len(LimgSelect)>1 else 0\n",
    "    imgPath += LimgSelect[choixIndex].split('/')[-1]\n",
    "    class AppURLopener(urllib.request.FancyURLopener):\n",
    "        version = \"Mozilla/5.0\"\n",
    "\n",
    "    opener = AppURLopener()\n",
    "    f = open(imgPath,'wb')\n",
    "    f.write(opener.open(LimgSelect[choixIndex]).read())\n",
    "    f.close()\n",
    "    # print(type(opener.open(Limg[0]).read()))\n",
    "    return imgPath\n",
    "retrieveImg(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déjà on reconstruit l'url de l'image avec les infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.astrobin.com/full/392940/0/?nc=all\"\n",
    "L = url.split('/')\n",
    "del L[3]\n",
    "del L[-1]\n",
    "urlInfos = '/'.join(L) + \"/\"\n",
    "print(urlInfos == \"https://www.astrobin.com/392940/0/\")\n",
    "# https://www.astrobin.com/392940/0/\n",
    "def reconstructURLInfo(url):\n",
    "    L = url.split('/')\n",
    "    del L[3]\n",
    "    del L[-1]\n",
    "    urlInfos = '/'.join(L) + \"/\"\n",
    "    return urlInfos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on doit sélectionner les images comportant des information sur les iso et le temps de pose unitaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRaDecDateISOtPose(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    resp = urllib.request.urlopen(req)\n",
    "    respData = resp.read().decode(\"utf-8\")\n",
    "#     print(respData)\n",
    "    def searchAfter(webPageStr,listeChaine_LimiteFin):\n",
    "        i = 0\n",
    "        L = [[] for i in range(len(listeChaine_LimiteFin))]\n",
    "        while i<len(webPageStr)-max([len(listeChaine_LimiteFin[i][0]) for i in range(len(listeChaine_LimiteFin))]):\n",
    "            for index,chaine_limiteFin in enumerate(listeChaine_LimiteFin):\n",
    "                chaine,limiteFin = chaine_limiteFin[0],chaine_limiteFin[1]\n",
    "                if webPageStr[i:i+len(chaine)] == chaine:\n",
    "                    i = i+len(chaine)\n",
    "                    txt = ''\n",
    "                    while webPageStr[i:i+len(limiteFin)] != limiteFin and i<len(webPageStr)-len(limiteFin):\n",
    "                        txt += str(webPageStr[i])\n",
    "                        i += 1\n",
    "                    L[index].append(txt)\n",
    "            i += 1\n",
    "        return L\n",
    "    Lfiltres = [[\"RA center:</strong>\",\"degrees\"],[\"DEC center:</strong>\",\"degrees\"],[\"start_date=\",\"&amp\"],[\"Frames:</strong>\",\"</p>\"]]\n",
    "    Linfos = searchAfter(respData,Lfiltres)\n",
    "#     print(Linfos)\n",
    "    for i in range(len(Linfos)):\n",
    "        if len(Linfos[i]) == 0:\n",
    "            print('Manque trop de données')\n",
    "            return None\n",
    "    part1 = [float(Linfos[0][i]) for i in range(len(Linfos[0]))]\n",
    "    part2 = [float(Linfos[1][i]) for i in range(len(Linfos[1]))]\n",
    "    Linfos = part1+part2+Linfos[2:]\n",
    "    i = 0\n",
    "    iso,tpose = [],[]\n",
    "    while i<len(Linfos[-1][0]):\n",
    "        if Linfos[-1][0][i:i+3] == 'ISO':\n",
    "            a = i+3\n",
    "            isoChaine = ''\n",
    "            while (Linfos[-1][0][a:a+3] != '<br' and Linfos[-1][0][a:a+2] != '\\n') and a<len(Linfos[-1][0]):\n",
    "                isoChaine += Linfos[-1][0][a]\n",
    "                a += 1\n",
    "            iso.append(isoChaine)\n",
    "            a = i-1\n",
    "            tposeChaine = ''\n",
    "            while Linfos[-1][0][a] != ':' and a>0:\n",
    "                tposeChaine += Linfos[-1][0][a]\n",
    "                a -= 1\n",
    "            tpose.append(tposeChaine[::-1])\n",
    "            i += 1\n",
    "        else: \n",
    "            i += 1\n",
    "    tpose = [tpose[index].strip().split('x') for index in range(len(tpose))]\n",
    "    tpose = [[tpose[index][0]]+[tpose[index][1][:-1]] for index in range(len(tpose))]\n",
    "    try:\n",
    "        Linfos = [part1]+[part2]+[Linfos[2]]+[[float(iso[i]) for i in range(len(iso))]]+[tpose]\n",
    "    except:\n",
    "        return None\n",
    "    for i in range(2,len(Linfos)):\n",
    "        if len(Linfos[i]) == 0:\n",
    "            print('Manque données annexes (ISO ~)')\n",
    "            return None\n",
    "    return Linfos\n",
    "# var = getRaDecDateISOtPose(\"https://www.astrobin.com/393134/?nc=all\")\n",
    "# print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention à la mutabilité !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], []]\n",
      "[[3, 2], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "a = [[] for i in range(4)]\n",
    "print(a)\n",
    "a[0].append(2)\n",
    "a[0].append(2)\n",
    "a[0][0] = 3\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'où au total :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.astrobin.com/393040/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://cdn.astrobin.com/thumbs/Ei7W0Dem_YUO_1824x0_wmhqkGbg.jpg\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://cdn.astrobin.com/thumbs/Ei7W0Dem_YUO_1824x0_wmhqkGbg.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robin\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel\\__main__.py:46: DeprecationWarning: AppURLopener style of invoking requests is deprecated. Use newer urlopen functions/methods\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok : téléchargée\n",
      "https://www.astrobin.com/393039/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393038/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393037/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://cdn.astrobin.com/thumbs/bqQq1gKgBYeG_1824x0_wmhqkGbg.jpg\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://cdn.astrobin.com/thumbs/bqQq1gKgBYeG_1824x0_wmhqkGbg.jpg']\n",
      "Ok : téléchargée\n",
      "https://www.astrobin.com/393036/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://cdn.astrobin.com/thumbs/O2j4H1KClsIz_1824x0_wmhqkGbg.jpg\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://cdn.astrobin.com/thumbs/O2j4H1KClsIz_1824x0_wmhqkGbg.jpg']\n",
      "Ok : téléchargée\n",
      "https://www.astrobin.com/393035/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393034/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393033/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393032/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393031/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393030/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393029/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393028/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393027/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393026/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393025/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393024/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393023/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393022/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393021/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393020/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393019/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://placehold.it/1824x1216/222/333&text=Chargement%20en%20cours...\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://placehold.it/1824x1216/222/333&text=Chargement%20en%20cours...']\n",
      "Erreur peut-être de format ou autre : voir balise de l'image à cette adresse : https://www.astrobin.com/full/393019/0/?nc=all\n",
      "https://www.astrobin.com/393018/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393017/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393016/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393015/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393014/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393013/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393012/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393011/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393010/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/393009/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://placehold.it/1824x1244/222/333&text=Chargement%20en%20cours...\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://placehold.it/1824x1244/222/333&text=Chargement%20en%20cours...']\n",
      "Erreur peut-être de format ou autre : voir balise de l'image à cette adresse : https://www.astrobin.com/full/393009/0/?nc=all\n",
      "https://www.astrobin.com/393008/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393007/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/393006/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/393005/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/393004/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393003/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/393002/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://cdn.astrobin.com/thumbs/b1_1FrE_S1DW_1824x0_wmhqkGbg.jpg\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://cdn.astrobin.com/thumbs/b1_1FrE_S1DW_1824x0_wmhqkGbg.jpg']\n",
      "Ok : téléchargée\n",
      "https://www.astrobin.com/393001/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/393000/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392999/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392998/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392997/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392996/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392995/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392994/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392993/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392992/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392991/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392990/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392989/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392988/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392987/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392986/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392985/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://placehold.it/1824x1194/222/333&text=Chargement%20en%20cours...\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://placehold.it/1824x1194/222/333&text=Chargement%20en%20cours...']\n",
      "Erreur peut-être de format ou autre : voir balise de l'image à cette adresse : https://www.astrobin.com/full/392985/0/?nc=all\n",
      "https://www.astrobin.com/392984/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392983/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392982/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392981/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392980/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://placehold.it/1824x1920/222/333&text=Chargement%20en%20cours...\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://placehold.it/1824x1920/222/333&text=Chargement%20en%20cours...']\n",
      "Erreur peut-être de format ou autre : voir balise de l'image à cette adresse : https://www.astrobin.com/full/392980/0/?nc=all\n",
      "https://www.astrobin.com/392979/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392978/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392977/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392976/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392975/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392974/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392973/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392972/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392971/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392970/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392969/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392968/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392967/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392966/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392965/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392964/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392963/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392962/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392961/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://cdn.astrobin.com/thumbs/lyGZD3pRmlvF_1824x0_wmhqkGbg.jpg\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://cdn.astrobin.com/thumbs/lyGZD3pRmlvF_1824x0_wmhqkGbg.jpg']\n",
      "Ok : téléchargée\n",
      "https://www.astrobin.com/392960/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://placehold.it/1824x1214/222/333&text=Chargement%20en%20cours...\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://placehold.it/1824x1214/222/333&text=Chargement%20en%20cours...']\n",
      "Erreur peut-être de format ou autre : voir balise de l'image à cette adresse : https://www.astrobin.com/full/392960/0/?nc=all\n",
      "https://www.astrobin.com/392959/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392958/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392957/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392956/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392955/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392954/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392953/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392952/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392951/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392950/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392949/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392948/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392947/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392946/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392945/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392944/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392943/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392942/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392941/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392940/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392939/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392938/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392937/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392936/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392935/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392934/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392933/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392932/0/\n",
      "ok assez de données\n",
      "https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png\n",
      "https://placehold.it/1824x1217/222/333&text=Chargement%20en%20cours...\n",
      "['https://cdn.astrobin.com/static/astrobin/images/astrobin-logo-small.81f9c70ff8b4.png', 'https://placehold.it/1824x1217/222/333&text=Chargement%20en%20cours...']\n",
      "Erreur peut-être de format ou autre : voir balise de l'image à cette adresse : https://www.astrobin.com/full/392932/0/?nc=all\n",
      "https://www.astrobin.com/392931/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392930/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392929/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392928/0/\n",
      "Le site n'a pas donné suite à la demande.....\n",
      "https://www.astrobin.com/392927/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392926/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392925/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392924/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392923/0/\n",
      "Manque trop de données\n",
      "https://www.astrobin.com/392922/0/\n",
      "Manque données annexes (ISO ~)\n",
      "https://www.astrobin.com/392921/0/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '133 (gain: 19.00) -25C bin 1x1\\n                                    \\n                                '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-45cbe1f56b29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructURLInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madresse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mLinfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetRaDecDateISOtPose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructURLInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madresse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mLinfos\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ok assez de données'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-a6d7062b426f>\u001b[0m in \u001b[0;36mgetRaDecDateISOtPose\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mtpose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mtpose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mLinfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpart1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpart2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLinfos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miso\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miso\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinfos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-a6d7062b426f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mtpose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mtpose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mLinfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpart1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpart2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLinfos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miso\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miso\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtpose\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinfos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '133 (gain: 19.00) -25C bin 1x1\\n                                    \\n                                '"
     ]
    }
   ],
   "source": [
    "derniereAdresse = \"https://www.astrobin.com/full/393040/0/?nc=all\"\n",
    "derniereAdresseSplit = derniereAdresse.split('/')\n",
    "i = int(derniereAdresseSplit[-3])\n",
    "nb = 100\n",
    "nbParpaquet = 10\n",
    "Info = []\n",
    "while i > i-nb:\n",
    "#     print(derniereAdresseSplit[-3])\n",
    "#     print(i)\n",
    "    derniereAdresseSplit[-3] = str(i)\n",
    "    adresse = '/'.join(derniereAdresseSplit)\n",
    "    print(reconstructURLInfo(adresse))\n",
    "    try:\n",
    "        Linfos = getRaDecDateISOtPose(reconstructURLInfo(adresse))\n",
    "        if Linfos != None:\n",
    "            print('ok assez de données')\n",
    "            for essai in range(1):\n",
    "                img = retrieveImg(adresse)\n",
    "                if img != None:\n",
    "                    print('Ok : téléchargée')\n",
    "                    Info.append(Linfos+[img])\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Erreur peut-être de format ou autre : voir balise de l'image à cette adresse : \"+adresse)\n",
    "    except urllib.error.HTTPError:\n",
    "        print(\"Le site n'a pas donné suite à la demande.....\")\n",
    "    i -= 1\n",
    "print(Info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumé démarche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution démarche\n",
    "Autoencodeur sur image : essayer de comprendre l'influence des couches de convolutions. Une IA ayant normalement besoin de beaucoup d'exemples on découpe chacune des 6 images en sous-images de 400 par 400 pixel (toujours en RGB)\n",
    "Comme cela ne fonctionnait pas, utilisation de couches intégralement connectée : normalement censées être plus précises. Influence du nombre de couches\n",
    "Retour sur les couches de convolution : tests plus formels en faisant varier la taille des noyaux pour voir si cela change la précision finale du modèle.\n",
    "Pas de résultats.\n",
    "Après recherches, aperçu que ce sont les couches de pooling qui semblent pouvoir synthétiser l'information.\n",
    "Le système reste toujours imprécis. En effet, la seule solution trouvée pour pouvoir comparée l'image de dimension réduite à l'image originale était d'augmenter à nouveau l'image avec une fonction d'interpolation préconçue en ajoutant au  milieu des couches de convolution pour essayer de reprendre la même démarche qu'à la réduction de dimension : ajouter une étape avant de modifier la taille de l'image qui l'analyse.\n",
    "Comme cela ne fonctionne pas, ajout de couches denses pour améliorer la précision. Cela ne change rien.\n",
    "Comme j'ai senti que les couches de convolutions restaient un point obscur dans l'ia : essais sur le nombre de couches qui donnent pas de résultat réellement marqué : les courbes de précision comme précédemment sont très instables sans changement. En observant mon modèle : choix de fixer le nombre de filtre à 3 pour les 3 couches RGB. N'en faudrait-il pas plus ? Essai avec plus de filtres. Pas de changement probant. De même en essayant avec des couches denses. Peut-être cela provient du lot d'images pas assez diversifié pour trouver une tendance générale dans les photos. De plus les réglages ont imposer un recadrage pour certaines images et pas d'autres ce qui amène sur quelques images où une étoile fera une taille moyenne de par exemple 40 par 40 pixel et sur d'autres 20 pixels. Pour avoir une IA pouvant traiter une image en s'affranchissant de recadrages / type d'appareil télescope utilisé, on choisit alors de modifier aléatoirement l'image d'entrée pour entrainer l'ia en effectant : rotation, redimensionnement et recadrage aléatoire de l'image source de l'APN. \"Après avoir visionné cette vidéo Convolutional Neural Networks (CNNs) explained il apparait que le redimensionnement peut poser des problèmes par rapport à l'apprentissage : en effet, le cercle de diamètre différent de chaque étoile variera suivant la taille de la région de l'image considérée, d'autant plus si on la redimensionne aléatoirement (NB : quelques images de départ ont déjà été redimensionnées pour obtenir des zones exploitables). \n",
    "Cela permettra néanmoins d'avoir un réseau plus polyvalent et moins sensible aux artéfacts de retouche manuelle. Il serait alors nécessaire d'afficher les caractéritiques apprises par chaque couche pour déterminer ce qui est détecté. \n",
    "De plus, à la suite de cette vidéo, il apparait que les filtres correspondent aux nombre de formes détectables par la couche de convolution.\" De plus, \"Avant de lancer la réalisation de ce modèle, d'après les articles GANs from Scratch 1: A deep introduction et Generative Adversarial Networks using Tensorflow, il apparait que le type de réseau de neurone que je cherche à réaliser s'appelle un Réseaux antagonistes génératifs.\"\n",
    "De cette piste : réalisation de plusieurs modèles mettant en place le principe des GAN qui lie plusieurs encodeurs-decodeurs pour reproduire une image en entrée, en créer une autre. Cependant, ce réseau utilisant une fonction log comme fonction de coût, il sature très vite et donne des résultats très extrèmes. Néanmoins, après visionnage vidéo : meilleure compréhension du fonctionnement : ce réseau est constitué de paires encodeur-decodeur le premier résumant ce qu'il perçoit des tendances des images et le deuxième recréant une image à partir de l'encodage de celle-ci tout cela en tentant de suivre la règle qui impose de minimiser une fonction de coût (image de la précision).\n",
    "A cette occasion, et par une autre vidéo mise en place de la normalisation par lot qui permet normalement de réduire la 'saturation' de l'image : autrement dit cette couche du modèle permet de garder les données travaillée dans la plage moyenne de travail. \n",
    "Cela ne donne toujours rien. Les couches de convolution de convolution sont un peu mieux comprises : elles possèdent deux caractéristiques importantes : une taille de noyau et un nombre de filtres. La taille du noyau détermine quelle taille de zone doit être résumé en une seule valeur. Puis, le nombre de filtre détermine grossièrement le nombre de motifs, groupe de couleurs, ... que la couche peut détecter. Après entrevue avec Mr Quiblier, les couches de convolution utilise bien un produit de convolution pour synthétiser l'information. A ce moment on comprend mieux comment plusieurs couches avec des petits noyaux peuvent petit à petit regrouper des zones similaires et avoir le même rôle qu'une seule couche avec un gros noyau de convolution. D'où une démultiplication des couches de convolution en sous-couches avant pooling (qui réduit la taille et finit l'étape de synthèse). La fonction de coût reste un point obscur. Comme le GAN est un ensemble d'autoencodeurs liés à différents points, on revient sur le fonctionnement en autoencodeur. Le but serai de faire tout d'abord 2 autoencodeurs reproduisant respectivement l'image bruitée avec l'image bruitée en entrée et l'image nettoyée avec l'image nettoyée. A cette étape rien ne converge. Se pose toujours la question de la fonction de coût (moyenne de la différence) et du nombre d'image insuffisant. Mise en place d'un algorithme de bruitage d'image téléchargées automatiquement depuis internet : on a trouvé qu'on peut approximer l'image bruitée à partir de l'image nettoyée en effectant image//np.random.randint(3,7)+gauss avec une gaussienne centrée sur la moyenne de l'image bruitée qu'il est donc nécessaire d'estimée. Pour cela on entraine une ia pour trouver ce paramètre en fonction de la sensibilité, du temps de pose, de l'éloignement de l'objet photographié à la Lune, de la phase de la Lune. Puis on met en place l'algorithme allant sur Astrobin, sélectionnant les images ayant toutes les informations pour prédire la valeur moyenne de l'image avant traitement et on télécharge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallèlement à cela, le dernier point qu'il me reste à éclaircir est la fonction de coût. On repart sur les exemples d'autoencodeurs purs (GAN 15_N et 15_C pour avoir une idée pour l'image bruitée et débruitée : en effet, sur la photo une fonction de coût serait peut-être plus adaptée pour influencer les réglages dans le bon sens.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En terme de fonction de coût plusieurs choses peuvent être envisagées : \n",
    "<ul>\n",
    "    <li>En gardant $x=sortie-entree$</li>\n",
    "    <li>Choisir entre <a href = 'https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean'>tf.reduce_mean</a>, <a href = 'https://www.tensorflow.org/api_docs/python/tf/math/reduce_max'>tf.reduce_max</a>,  <a href = 'https://www.tensorflow.org/api_docs/python/tf/math/reduce_logsumexp'>tf.reduce_logsumexp</a>,  <a href = 'https://www.tensorflow.org/api_docs/python/tf/math/reduce_std'>tf.reduce_std</a>,  <a href = 'https://www.tensorflow.org/api_docs/python/tf/math/reduce_variance'>tf.reduce_variance</a> <a href='https://www.tensorflow.org/api_docs/python/tf/math'>(index des fonctions utilisables)</a></li>\n",
    "    <li>Mettre x au carré ou multiplier le résultat de la fonction par un coefficient supérieur à 1 pour augmenter l'agressivité des correction </li>\n",
    "    <li>Prendre la valeur absolue ou multiplier par le signe du résultat pour observer l'influence sur la convergens</li>\n",
    "    <li>Modifier le taux d'apprentissage : 10**-6,10**-4,10**-2,10**-1</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'où les essais suivants : </br>\n",
    "On  fait un `enregistrement automatique` toutes les `4 étapes` écrasant les précédentes sauvegarde et mettant `tout au fur et à mesure dans le dossier` des log et modèles</br>\n",
    "On arrête si diverge complètement ou est un instable sans amélioration avec l'entrainement</br>\n",
    "On `modifie le pas d'apprentissage`</br>\n",
    "De base l'entrainement se fait sur `50 époques` ce qui est suffisant pour se rendre compte (d'après les expériences précédentes) si le modèle va bien converger ou pas.</br>\n",
    "On `affiche régulièrement la valeur minimale, maximale et moyenne du tenseur`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Réalisé | Fonction de coût | Photo bruitée | Résultat | Photo traitée | Résultat |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| -- | tf.reduce_mean(sortie-entree) | Autoencodeur_N_mean_1_10e-6 | - |Autoencodeur_C_mean_1_10e-6| - |\n",
    "| -- | tf.reduce_mean(sortie-entree) | Autoencodeur_N_mean_1_10e-4 | - |Autoencodeur_C_mean_1_10e-4| - |\n",
    "| -- | tf.reduce_mean(sortie-entree) | Autoencodeur_N_mean_1_10e-2 | - |Autoencodeur_C_mean_1_10e-2| - |\n",
    "| -- | tf.reduce_mean(sortie-entree) | Autoencodeur_N_mean_1_10e-1 | - |Autoencodeur_C_mean_1_10e-1| - |\n",
    "| -- | tf.reduce_mean(sortie-entree) et </br> tf.sign() | Autoencodeur_N_mean_2_10e-6 | - | Autoencodeur_C_mean_2_10e-6 | - |\n",
    "| -- | tf.reduce_mean(sortie-entree) et </br> tf.sign() | Autoencodeur_N_mean_2_10e-4 | - | Autoencodeur_C_mean_2_10e-4 | - |\n",
    "| -- | tf.reduce_mean(sortie-entree) et </br> tf.sign() | Autoencodeur_N_mean_2_10e-2 | - | Autoencodeur_C_mean_2_10e-2 | - |\n",
    "| -- | tf.reduce_mean(sortie-entree) et </br> tf.sign() | Autoencodeur_N_mean_2_10e-1 | - | Autoencodeur_C_mean_2_10e-1 | - |\n",
    "| -- | tf.reduce_mean(sortie-entree) et </br> tf.abs() | Autoencodeur_N_mean_3_10e-6 | - | Autoencodeur_C_mean_3_10e-6 | - |\n",
    "| -- | tf.reduce_mean(sortie-entree) et </br> tf.abs() | Autoencodeur_N_mean_3_10e-4 | - | Autoencodeur_C_mean_3_10e-4 | - |\n",
    "| -- | tf.reduce_mean(sortie-entree) et </br> tf.abs() | Autoencodeur_N_mean_3_10e-2 | - | Autoencodeur_C_mean_3_10e-2 | - |\n",
    "| -- | tf.reduce_mean(sortie-entree) et </br> tf.abs() | Autoencodeur_N_mean_3_10e-1 | - | Autoencodeur_C_mean_3_10e-1 | - |\n",
    "| -- | tf.pow(sortie-entree,2) et </br> tf.reduce_mean() | Autoencodeur_N_mean_4_10e-6 | - | Autoencodeur_C_mean_4_10e-6 | - |\n",
    "| -- | tf.pow(sortie-entree,2) et </br> tf.reduce_mean() | Autoencodeur_N_mean_4_10e-4 | - | Autoencodeur_C_mean_4_10e-4 | - |\n",
    "| -- | tf.pow(sortie-entree,2) et </br> tf.reduce_mean() | Autoencodeur_N_mean_4_10e-2 | - | Autoencodeur_C_mean_4_10e-2 | - |\n",
    "| -- | tf.pow(sortie-entree,2) et </br> tf.reduce_mean() | Autoencodeur_N_mean_4_10e-1 | - | Autoencodeur_C_mean_4_10e-1 | - |\n",
    "| -- | tf.reduce_max(sortie-entree) | Autoencodeur_N_max_1_10e-6 | - |Autoencodeur_C_max_1_10e-6 | - |\n",
    "| -- | tf.reduce_max(sortie-entree) | Autoencodeur_N_max_1_10e-4 | - |Autoencodeur_C_max_1_10e-4 | - |\n",
    "| -- | tf.reduce_max(sortie-entree) | Autoencodeur_N_max_1_10e-2 | - |Autoencodeur_C_max_1_10e-2 | - |\n",
    "| -- | tf.reduce_max(sortie-entree) | Autoencodeur_N_max_1_10e-1 | - |Autoencodeur_C_max_1_10e-1 | - |\n",
    "| -- | tf.pow(sortie-entree,2) et </br> tf.reduce_max() | Autoencodeur_N_max_2_10e-6 | - |Autoencodeur_C_max_2_10e-6 | - |\n",
    "| -- | tf.pow(sortie-entree,2) et </br> tf.reduce_max() | Autoencodeur_N_max_2_10e-4 | - |Autoencodeur_C_max_2_10e-4 | - |\n",
    "| -- | tf.pow(sortie-entree,2) et </br> tf.reduce_max() | Autoencodeur_N_max_2_10e-2 | - |Autoencodeur_C_max_2_10e-2 | - |\n",
    "| -- | tf.pow(sortie-entree,2) et </br> tf.reduce_max() | Autoencodeur_N_max_2_10e-1 | - |Autoencodeur_C_max_2_10e-1 | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code de base :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "forceInit = False\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate_power = 6\n",
    "learning_rate = 10**-learning_rate_power\n",
    "i = 1\n",
    "nom = 'Autodencodeur_N_'+'mean'+'_'+str(i)+'_10e-'+str(learning_rate_power)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "      \n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    couche,kernelBiasList,saver,sortiesImages = discriminatorDense(generator,[150,300,150],'Lien')\n",
    "    nbCouchesDsicriminateur = 15\n",
    "    disc_faux,disc_vars,disc_saver,disc_faux_sortiesImages = discriminator(generator,np.linspace(int(generator.get_shape()[1]),199,nbCouchesDsicriminateur+1,dtype=np.int)[1:],np.linspace(int(generator.get_shape()[-1]),3,nbCouchesDsicriminateur,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    difference = disc_faux-gen_input\n",
    "    disc_loss = tf.reduce_mean(difference)\n",
    "    \n",
    "    maximumGradient = tf.reduce_max(difference)\n",
    "    minimumGradient = tf.reduce_min(difference)\n",
    "    meanGradient = tf.reduce_mean(difference)\n",
    "    maximumSortie = tf.reduce_max(disc_faux)\n",
    "    minimumSortie = tf.reduce_min(disc_faux)\n",
    "    meanSortie = tf.reduce_mean(disc_faux)\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",disc_loss)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    saver =  disc_saver\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_disc = optimizer_disc.minimize(disc_loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('./checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('./checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('./checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14]) > lastTrained:\n",
    "              lastTrained = int(elem[14])\n",
    "          tf_saver.restore(sess, 'checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(0,51):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "            \n",
    "          save(disc_saver,gen_inpt,disc_inpt)\n",
    "        \n",
    "          maximumGradientValue = maximumGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          minimumGradientValue = minimumGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          meanGradientValue = meanGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          maximumSortieValue = maximumSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          minimumSortieValue = minimumSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          meanSortieValue = meanSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "        \n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "          print('Différence : max %f, min %f, mean %f' % (maximumGradientValue, minimumGradientValue,meanGradientValue))\n",
    "          print('Sortie : max %f, min %f, mean %f' % (maximumSortieValue, minimumSortieValue,meanSortieValue))\n",
    "          if i % 4 == 0:\n",
    "            os.remove('model/model'+nom+'.ckpt')\n",
    "            tf_saver.save(sess,'model/model'+nom+'.ckpt')\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        os.remove('model/model'+nom+'.ckpt')\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 199\n",
    "width = 199\n",
    "channels = 3\n",
    "forceInit = False\n",
    "\n",
    "batch_size = 7\n",
    "learning_rate_power = 6\n",
    "learning_rate = 10**-learning_rate_power\n",
    "i = 1\n",
    "nom = 'Autodencodeur_N_'+'mean'+'_'+str(i)+'_10e-'+str(learning_rate_power)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "import traceback\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
    "with tf.Graph().as_default():\n",
    "  \n",
    "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
    "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
    "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "      sortieImage = tf.summary.image(\"output/\"+nom if end==True else \"input/\"+nom,tf.cast(couche,dtype=tf.uint8)) if nbNeurones == 3 else None\n",
    "      return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "    \n",
    "    def lrelu(x, leak=0.2): \n",
    "      return tf.maximum(x, leak * x)\n",
    "    \n",
    "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
    "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
    "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
    "             strides=1, padding='SAME',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        if pooling == True:\n",
    "          if poolingType == 'mean':\n",
    "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "          else:\n",
    "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(kernelListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(kernelListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
    "      \n",
    "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
    "      \"\"\"Ceci est une documentation\"\"\"\n",
    "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
    "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "#           print(precDimension)\n",
    "          return outputSize-(precDimension-1)*strides\n",
    "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
    "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
    "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
    "             strides=1, padding='VALID',\n",
    "             activation=tf.nn.relu, name=nom)\n",
    "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
    "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
    "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
    "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
    "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
    "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
    "        sortieImage = None\n",
    "        if start==True:\n",
    "          sortieImage = tf.summary.image(\"input/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        else:\n",
    "          sortieImage = tf.summary.image(\"output/\"+nom,tf.cast(couche,dtype=tf.uint8))\n",
    "        return couche,kernel,bias,kernel_saver,bias_saver,sortieImage\n",
    "      if type(sizeListOrNot) == list:\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
    "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
    "        for i in range(1,len(sizeListOrNot)):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
    "          kernelList.append(kernel)\n",
    "          biasList.append(bias)\n",
    "          kernel_saverList.append(kernel_saver)\n",
    "          bias_saverList.append(bias_saver)\n",
    "          sortieImageList.append(sortieImage)\n",
    "        return couche,kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList\n",
    "      else: \n",
    "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
    "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
    "      \"\"\"\n",
    "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
    "      Entree : \n",
    "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
    "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
    "                liste simple soit sous forme d'une double liste. \n",
    "                Chaque sous liste représente un noyau de convolution \n",
    "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
    "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
    "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
    "        poolingSize, les tailles des noyaux des couche de pooling\n",
    "        poolingType, le type de pooling utilisé : SAME en générale\n",
    "      Sortie : \n",
    "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(kernels)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "\n",
    "    def discriminator(inpt,size,num_filters,ID):\n",
    "      \"\"\"\n",
    "      Entrees : \n",
    "        inpt, couche d'entrée du discriminateur\n",
    "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
    "        num_filters, le nombre de filtre par couches\n",
    "        ID, id unique pour séparer deux potentiels discriminateurs\n",
    "      Sortie :\n",
    "        Couche traitée par le discriminateur\n",
    "      \"\"\"\n",
    "      kernelBiasList,saver,sortiesImages= [],[],[]\n",
    "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
    "      print(inpt.get_shape())\n",
    "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages):\n",
    "        if type(kernel) == list:\n",
    "          for k in kernel:\n",
    "            kernelBiasList.append(k)\n",
    "          for b in bias:\n",
    "            kernelBiasList.append(b)\n",
    "          for ks in kernel_saver:\n",
    "            saver.append(ks)\n",
    "          for bs in bias_saver:\n",
    "            saver.append(bs)\n",
    "          for i in sortieImage:\n",
    "            sortiesImages.append(i)\n",
    "        else:\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return None #Utilise la mutabilité des listes\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      for i in range(1,len(size)-1):\n",
    "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
    "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "\n",
    "      couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
    "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,sortieImage,kernelBiasList,saver,sortiesImages)\n",
    "      return couche,kernelBiasList,saver,sortiesImages\n",
    "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
    "        saver = []\n",
    "        kernelBiasList = []\n",
    "        sortiesImages = []\n",
    "        couche = inpt\n",
    "        for i,nb in enumerate(neurones):\n",
    "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
    "          kernelBiasList.append(kernel)\n",
    "          kernelBiasList.append(bias)\n",
    "          saver.append(kernel_saver)\n",
    "          saver.append(bias_saver)\n",
    "          sortiesImages.append(sortieImage)\n",
    "        return couche,kernelBiasList,saver,sortiesImages\n",
    "      \n",
    "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    generator,gen_vars,gen_saver,gen_sortiesImages = generateur(disc_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
    "    couche,kernelBiasList,saver,sortiesImages = discriminatorDense(generator,[150,300,150],'Lien')\n",
    "    nbCouchesDsicriminateur = 15\n",
    "    disc_faux,disc_vars,disc_saver,disc_faux_sortiesImages = discriminator(generator,np.linspace(int(generator.get_shape()[1]),199,nbCouchesDsicriminateur+1,dtype=np.int)[1:],np.linspace(int(generator.get_shape()[-1]),3,nbCouchesDsicriminateur,dtype=np.int),'Faux')\n",
    "    \n",
    "    gen_sortiesImages.append(tf.summary.image(\"inputGen/entree\",tf.cast(gen_input,dtype=tf.uint8)))\n",
    "    gen_sortiesImages.append(tf.summary.image(\"outputDisc/sortie\",tf.cast(disc_faux,dtype=tf.uint8)))\n",
    "    print('Taille de sortie disc_faux : ',end='')\n",
    "    print(disc_faux.get_shape())\n",
    "    print('Taille de sortie disc_input : ',end='')\n",
    "    print(disc_input.get_shape())\n",
    "    \n",
    "    \n",
    "    difference = disc_faux-gen_input\n",
    "    disc_loss = tf.reduce_mean(difference)\n",
    "    \n",
    "    maximumGradient = tf.reduce_max(difference)\n",
    "    minimumGradient = tf.reduce_min(difference)\n",
    "    meanGradient = tf.reduce_mean(difference)\n",
    "    maximumSortie = tf.reduce_max(disc_faux)\n",
    "    minimumSortie = tf.reduce_min(disc_faux)\n",
    "    meanSortie = tf.reduce_mean(disc_faux)\n",
    "  \n",
    "    loss_saver_disc = tf.summary.scalar(\"Discriminator_loss\",disc_loss)\n",
    "    \n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "    \n",
    "    saver =  disc_saver\n",
    "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
    "    train_disc = optimizer_disc.minimize(disc_loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    tf_saver = tf.train.Saver()\n",
    "\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        def save(saver_biasKernel,gen_inpt,disc_inpt):\n",
    "          for kernelBias in saver_biasKernel:\n",
    "            summary_str = kernelBias.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "          return None\n",
    "        if os.path.isdir('./checkpoints_'+nom) == True and forceInit == False:\n",
    "          print(os.listdir('./checkpoints_'+nom))\n",
    "          lastTrained = 0\n",
    "          for elem in os.listdir('./checkpoints_'+nom):\n",
    "            if elem[0:14] == 'checkpoint_at_' and int(elem[14]) > lastTrained:\n",
    "              lastTrained = int(elem[14])\n",
    "          tf_saver.restore(sess, 'checkpoints_'+nom+'/checkpoint_at_'+str(lastTrained)+'.ckpt')\n",
    "        else :\n",
    "          print('No previous training found...')\n",
    "          init.run()\n",
    "        print('Entrainement....')\n",
    "        for i in range(0,101):\n",
    "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
    "          _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
    "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          summary_writer.add_summary(summary_str,i)\n",
    "            \n",
    "          save(disc_saver,gen_inpt,disc_inpt)\n",
    "        \n",
    "          maximumGradientValue = maximumGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          minimumGradientValue = minimumGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          meanGradientValue = meanGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          maximumSortieValue = maximumSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          minimumSortieValue = minimumSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "          meanSortieValue = meanSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
    "        \n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "          print('Différence : max %f, min %f, mean %f' % (maximumGradientValue, minimumGradientValue,meanGradientValue))\n",
    "          print('Sortie : max %f, min %f, mean %f' % (maximumSortieValue, minimumSortieValue,meanSortieValue))\n",
    "          if i % 4 == 0:\n",
    "            os.remove('model/model'+nom+'.ckpt')\n",
    "            tf_saver.save(sess,'model/model'+nom+'.ckpt')\n",
    "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
    "        for sortieImage in gen_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        for sortieImage in disc_faux_sortiesImages:\n",
    "          if sortieImage != None:\n",
    "            try:\n",
    "              summary_writer.add_summary(sortieImage.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False}))\n",
    "            except Exception:\n",
    "              traceback.format_exc()\n",
    "        os.remove('model/model'+nom+'.ckpt')\n",
    "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Conv | 5 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 10 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 7 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 7 | 5 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 2 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Conv | 3 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 6 | 12 | 1 | Relu | \\ |\n",
    "| Conv | 9 | 9 | 1 | Relu | \\ |\n",
    "| Conv | 12 | 6 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 3 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Conv | 3 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 10 | 12 | 1 | Relu | \\ |\n",
    "| Conv | 17 | 9 | 1 | Relu | \\ |\n",
    "| Conv | 24 | 6 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 3 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai 3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Conv | 3 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 7 | 12 | 1 | Relu | \\ |\n",
    "| Conv | 11 | 9 | 1 | Relu | \\ |\n",
    "| Conv | 15 | 6 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 3 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Conv | 3 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 10| 12 | 1 | Relu | \\ |\n",
    "| Conv | 17 | 9 | 1 | Relu | \\ |\n",
    "| Conv | 24 | 6 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 3 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Conv | 3 | 20 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 18 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 7 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 7 | 12 | 1 | Relu | \\ |\n",
    "| Conv | 7 | 12 | 1 | Relu | \\ |\n",
    "| Conv | 10 | 10 | 1 | Relu | \\ |\n",
    "| Conv | 10 | 10 | 1 | Relu | \\ |\n",
    "| Conv | 12 | 7 | 1 | Relu | \\ |\n",
    "| Conv | 12 | 7 | 1 | Relu | \\ |\n",
    "| Conv | 15 | 7 | 1 | Relu | \\ |\n",
    "| Conv | 15 | 3 | 1 | Relu | \\ |\n",
    "| Conv | 18 | 3 | 1 | Relu | \\ |\n",
    "| Conv | 20 | 3 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 3 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv1 = tf.layers.conv2d(X, filters=filtre[0], kernel_size=noyaux[0],\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv1')\n",
      "maxPool1 = tf.layers.max_pooling2d(conv1, pool_size=(pool[0],pool[0]) strides=(pool[0],pool[0]), padding='same')\n",
      "\n",
      "conv2 = tf.layers.conv2d(maxPool1, filters=filtre[1], kernel_size=noyaux[1],\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv2')\n",
      "maxPool2 = tf.layers.max_pooling2d(conv2, pool_size=(pool[1],pool[1]) strides=(pool[1],pool[1]), padding='same')\n",
      "\n",
      "conv3 = tf.layers.conv2d(maxPool2, filters=filtre[2], kernel_size=noyaux[2],\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv3')\n",
      "maxPool3 = tf.layers.max_pooling2d(conv3, pool_size=(pool[2],pool[2]) strides=(pool[2],pool[2]), padding='same')\n",
      "\n",
      "conv4 = tf.layers.conv2d(maxPool3, filters=filtre[3], kernel_size=noyaux[3],\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv4')\n",
      "maxPool4 = tf.layers.max_pooling2d(conv4, pool_size=(pool[3],pool[3]) strides=(pool[3],pool[3]), padding='same')\n",
      "\n",
      "output = tf.layers.conv2d(maxPool4, filters=filtre[4], kernel_size=noyaux[4],\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv5')\n"
     ]
    }
   ],
   "source": [
    "nb = 9\n",
    "nom = ''\n",
    "lastName = ''\n",
    "for i in range(nb):\n",
    "    if i %2 == 0:\n",
    "        print()\n",
    "        if i == nb-1:\n",
    "            nom += \"output = tf.layers.conv2d(\"+lastName+\", filters=filtre[\"+str(i//2)+\"], kernel_size=noyaux[\"\n",
    "        elif i == 0:\n",
    "            nom += \"conv\"+str(i//2+1)+\" = tf.layers.conv2d(X, filters=filtre[\"+str(i//2)+\"], kernel_size=noyaux[\"\n",
    "            lastName = \"conv\"+str(i//2+1)\n",
    "        else:\n",
    "            nom += \"conv\"+str(i//2+1)+\" = tf.layers.conv2d(\"+lastName+\", filters=filtre[\"+str(i//2)+\"], kernel_size=noyaux[\"\n",
    "            lastName = \"conv\"+str(i//2+1)\n",
    "        nom += str(i//2)+\"],\\n \\t \\t \\t strides=1, padding='SAME',\\n \\t \\t \\t activation=tf.nn.relu, name='conv\"\n",
    "        nom += str(i//2+1)+\"')\"\n",
    "    else:\n",
    "        nom += 'maxPool' + str(i//2+1) + \" = tf.layers.max_pooling2d(\"+lastName+\", pool_size=(pool[\"+str(i//2)+\"],pool[\"+str(i//2)+\"]), strides=(pool[\"+str(i//2)+\"],pool[\"+str(i//2)+\"]), padding='same')\"\n",
    "        lastName = 'maxPool' + str(i//2+1)\n",
    "    print(nom)\n",
    "    nom = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Type de couche | Taille | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| Conv0 | - | 3 | 20 | 1 | Relu | \\ |\n",
      "| Conv1 | - | 3 | 15 | 1 | Relu | \\ |\n",
      "| Conv2 | - | 3 | 11 | 1 | Relu | \\ |\n",
      "| Conv3 | - | 3 | 7 | 1 | Relu | \\ |\n",
      "| Conv4 | - | 3 | 3 | 1 | Relu | \\ |\n",
      "\n",
      "conv0 = tf.layers.conv2d(X, filters=3, kernel_size=20,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv0')\n",
      "print('Shape of conv0 : '+str(conv0.get_shape()))\n",
      "conv1 = tf.layers.conv2d(conv0, filters=3, kernel_size=15,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv1')\n",
      "print('Shape of conv1 : '+str(conv1.get_shape()))\n",
      "conv2 = tf.layers.conv2d(conv1, filters=3, kernel_size=11,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv2')\n",
      "print('Shape of conv2 : '+str(conv2.get_shape()))\n",
      "conv3 = tf.layers.conv2d(conv2, filters=3, kernel_size=7,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv3')\n",
      "print('Shape of conv3 : '+str(conv3.get_shape()))\n",
      "conv4 = tf.layers.conv2d(conv3, filters=3, kernel_size=3,\n",
      " \t \t \t strides=1, padding='SAME',\n",
      " \t \t \t activation=tf.nn.relu, name='conv4')\n",
      "print('Shape of conv4 : '+str(conv4.get_shape()))\n"
     ]
    }
   ],
   "source": [
    "nb = 5\n",
    "nbResize = 5\n",
    "plage = 3,20\n",
    "plageResize = 3,15\n",
    "inputSize = 399\n",
    "import numpy as np\n",
    "chaineTableau = ''\n",
    "chaineTf = ''\n",
    "seprateur = '///'\n",
    "def convTableau(indice,filtre,noyau,chaineTableau):\n",
    "    chaineTableau += seprateur\n",
    "    chaineTableau += \"| Conv\"+str(indice)+\" | - | \"+str(filtre)+\" | \"+ str(noyau) +\" | 1 | Relu | \\ |\"\n",
    "    return chaineTableau\n",
    "def convTf(indice,filtre,noyau,chaineTf,first=False,last=False):\n",
    "    chaineTf += seprateur\n",
    "    global lastName\n",
    "    if last == True:\n",
    "        chaineTf += 'output'\n",
    "        lastName = 'output'\n",
    "    else:\n",
    "        chaineTf += \"conv\"+str(indice)\n",
    "    chaineTf += \" = tf.layers.conv2d(\"\n",
    "    if first == True:\n",
    "        chaineTf += \"X\"\n",
    "    else:\n",
    "        chaineTf += lastName\n",
    "    chaineTf+= \", filters=\"+str(filtre)+\", kernel_size=\"+str(noyau)+\",\\n \\t \\t \\t strides=1, padding='SAME',\\n \\t \\t \\t activation=tf.nn.relu, name='conv\"+str(indice)+\"')\"\n",
    "    lastName = \"conv\"+str(indice)\n",
    "    chaineTf += seprateur+\"print('Shape of \"+lastName+\" : '+str(\"+lastName+\".get_shape()))\"\n",
    "    return chaineTf\n",
    "def poolingTableau(indice,taille,pas,chaineTableau):\n",
    "    chaineTableau += seprateur\n",
    "    chaineTableau += \"| Pool\"+str(indice)+\" | \"+str(taille)+\" | - | - | \"+str(pas)+\" | Relu |-|\"\n",
    "    return chaineTableau\n",
    "def poolingTF(indice,taille,pas,chaineTf):\n",
    "    chaineTf += seprateur\n",
    "    global lastName\n",
    "    chaineTf += 'maxPool' + str(indice) + \" = tf.layers.max_pooling2d(\"+lastName+\", pool_size=(\"+str(taille)+\",\"+str(taille)+\"), strides=(\"+str(taille)+\",\"+str(taille)+\"), padding='same')\"\n",
    "    lastName = 'maxPool' + str(indice)\n",
    "    chaineTf += seprateur+\"print('Shape of \"+lastName+\" : '+str(\"+lastName+\".get_shape()))\"\n",
    "    chaineTf += seprateur\n",
    "    return chaineTf\n",
    "    \n",
    "def resizeTableau(indice,size,chaineTableau):\n",
    "    chaineTableau += seprateur\n",
    "    chaineTableau += \"| Resize\"+str(indice)+\" | \"+str(size)+\" | - | - | - | Relu | - |\"\n",
    "    return chaineTableau\n",
    "def resizeTf(indice,size,chaineTf):\n",
    "    chaineTf += seprateur\n",
    "    chaineTf += seprateur\n",
    "    global lastName\n",
    "    chaineTf += \"resized\"+str(indice)+\" = tf.image.resize_images(\"+lastName+\", size=(\"+str(size)+\",\"+str(size)+\"), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\"\n",
    "    lastName = \"resized\"+str(indice)\n",
    "    chaineTf += seprateur+\"print('Shape of \"+lastName+\" : '+str(\"+lastName+\".get_shape()))\"\n",
    "    return chaineTf\n",
    "def denseTableau(indice,nb,size,chaineTableau):\n",
    "    chaineTableau += seprateur\n",
    "    chaineTableau += \"| Dense\"+str(indice)+\" | \"+str(size)+\" | - | - | - | Relu | \"+str(nb)+\" |\"\n",
    "    return chaineTableau\n",
    "def denseTf(indice,nb,chaineTf):\n",
    "    chaineTf += seprateur\n",
    "    global lastName\n",
    "    chaineTf += \"dense\"+str(indice)+\" = tf.layers.dense(\"+lastName+\",\"+str(nb)+\")\"\n",
    "    lastName = \"dense\"+str(indice)\n",
    "    chaineTf += seprateur+\"print('Shape of \"+lastName+\" : '+str(\"+lastName+\".get_shape()))\"\n",
    "    return chaineTf\n",
    "def maxPoolingLinear(start,sizeInput,sizeOutput,nb,chaineTableau,chaineTf):\n",
    "    global lastName\n",
    "    filtre = np.linspace(plage[0],plage[1],nb,dtype=np.int)\n",
    "    noyaux = np.linspace(plage[1],plage[0],nb,dtype=np.int)\n",
    "    size = np.linspace(sizeInput,sizeOutput,nb+1,dtype=np.int)[1:]\n",
    "    for i in range(len(size)):\n",
    "        chaineTableau = resizeTableau(start+i,size[i],chaineTableau)\n",
    "        chaineTableau = convTableau(start+i,filtre[i],noyaux[i],chaineTableau)\n",
    "        chaineTf = resizeTf(start+i,size[i],chaineTf)\n",
    "        chaineTf = convTf(start+i,filtre[i],noyaux[i],chaineTf)\n",
    "    return chaineTableau,chaineTf\n",
    "# filtre = np.linspace(plage[0],plage[1],nb//2+1 if nb % 2 == 1 else nb//2,dtype=np.int)\n",
    "# noyaux = np.linspace(plage[1],plage[0],nb//2+1 if nb % 2 == 1 else nb//2,dtype=np.int)\n",
    "# filtre = np.concatenate((filtre,np.flip(filtre,axis=0) if nb % 2 == 0 else np.flip(filtre[:-1],axis=0)),axis=0).tolist()\n",
    "# noyaux = np.concatenate((noyaux,np.flip(noyaux,axis=0) if nb % 2 == 0 else np.flip(noyaux[:-1],axis=0)),axis=0).tolist()\n",
    "filtre = np.linspace(3,3,nb,dtype=np.int)\n",
    "noyaux = np.linspace(plage[1],plage[0],nb,dtype=np.int)\n",
    "filtreResize = np.linspace(3,3,nbResize,dtype=np.int)\n",
    "noyauxResize = np.linspace(plageResize[0],plageResize[1],nbResize,dtype=np.int)\n",
    "maxpooling = False\n",
    "poolingValue = 2\n",
    "denseLayer = True\n",
    "layers = [3 for i in range(15)]\n",
    "print()\n",
    "nom = \"\"\n",
    "last = ''\n",
    "lastName = 'X'\n",
    "chaineTableau += \"| Type de couche | Taille | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\\n| --- | --- | --- | --- | --- | --- | --- |\"\n",
    "currentSize = inputSize\n",
    "chaineTableau = convTableau(0,filtre[0],noyaux[0],chaineTableau)\n",
    "chaineTf = convTf(0,filtre[0],noyaux[0],chaineTf,first=True)\n",
    "# if maxpooling == True:\n",
    "#     currentSize //= poolingValue\n",
    "# #     print(type(i),type(currentSize),type(poolingValue),type(chaineTableau))\n",
    "#     chaineTableau = poolingTableau(i,currentSize,poolingValue,chaineTableau)\n",
    "#     chaineTf = poolingTF(0,currentSize,poolingValue,chaineTf)\n",
    "\n",
    "for i in range(1,len(filtre)-1):\n",
    "    chaineTableau = convTableau(i,filtre[i],noyaux[i],chaineTableau)\n",
    "    chaineTf = convTf(i,filtre[i],noyaux[i],chaineTf,chaineTf)\n",
    "#     if maxpooling == True:\n",
    "#         currentSize //= poolingValue\n",
    "#         chaineTableau = poolingTableau(i,currentSize,poolingValue,chaineTableau)\n",
    "#         chaineTf = poolingTF(i,currentSize,poolingValue,chaineTf)\n",
    "chaineTableau = convTableau(len(filtre)-1,filtre[-1],noyaux[-1],chaineTableau)\n",
    "chaineTf = convTf(len(filtre)-1,filtre[-1],noyaux[-1],chaineTf)\n",
    "# if denseLayer == True:\n",
    "#     for i in range(len(layers)):\n",
    "#         chaineTableau = denseTableau(i,layers[i],currentSize,chaineTableau)\n",
    "#         chaineTf = denseTf(i,layers[i],chaineTf)\n",
    "# if maxpooling == True:\n",
    "#     temp = maxPoolingLinear(len(filtre),currentSize,inputSize,5,chaineTableau,chaineTf)\n",
    "#     chaineTableau = temp[0]\n",
    "#     chaineTf = temp[1]\n",
    "\n",
    "#     print(chaineTf)\n",
    "# print(chaineTableau)\n",
    "chaineTableau = chaineTableau.split(seprateur)\n",
    "# print(chaineTableau)\n",
    "chaineTf = chaineTf.split(seprateur)\n",
    "# print(chaineTableau)\n",
    "for elem in chaineTableau:\n",
    "    print(elem)\n",
    "for elem in chaineTf:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Conv | 3 | 20 | 1 | Relu | \\ |\n",
    "| Conv | 5 | 17 | 1 | Relu | \\ |\n",
    "| Conv | 7 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 10 | 12 | 1 | Relu | \\ |\n",
    "| Conv | 12 | 10 | 1 | Relu | \\ |\n",
    "| Conv | 15 | 7 | 1 | Relu | \\ |\n",
    "| Conv | 17 | 5 | 1 | Relu | \\ |\n",
    "| Conv | 20 | 3 | 1 | Relu | \\ |\n",
    "| Conv | 17 | 5 | 1 | Relu | \\ |\n",
    "| Conv | 15 | 7 | 1 | Relu | \\ |\n",
    "| Conv | 12 | 10 | 1 | Relu | \\ |\n",
    "| Conv | 10 | 12 | 1 | Relu | \\ |\n",
    "| Conv | 7 | 15 | 1 | Relu | \\ |\n",
    "| Conv | 5 | 17 | 1 | Relu | \\ |\n",
    "| Conv | 3 | 20 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essai 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Conv | 3 | 15 | 1 | Relu | \\ |\n",
    "| Pool | / | / | 2 | Relu | \\ |\n",
    "| Conv | 7 | 12 | 1 | Relu | \\ |\n",
    "| Pool | / | / | 2 | Relu | \\ |\n",
    "| Conv | 11 | 9 | 1 | Relu | \\ |\n",
    "| Pool | / | / | 2 | Relu | \\ |\n",
    "| Conv | 15 | 6 | 1 | Relu | \\ |\n",
    "| Pool | / | / | 2 | Relu | \\ |\n",
    "| Conv | 3 | 3 | 1 | Relu | \\ |\n",
    "| Resize | - | - | - | - | - | - |\n",
    "| Conv | 3 | 5 | 1 | Relu | \\ |\n",
    "| Resize | - | - | - | - | - | - |\n",
    "| Conv | 3 | 10 | 1 | Relu | \\ |\n",
    "| Resize | - | - | - | - | - | - |\n",
    "| Conv | 3 | 14 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "nom = 'Essai7T'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}{}/\".format(root_logdir,nom,now)\n",
    "\n",
    "noyaux = [3,25,15,9,6,3]\n",
    "filtre = [7, 9, 11, 15,19,7]\n",
    "noyauxDecode = [5,10,14]\n",
    "# filtreDecode = [3, 5, 7, 11,15,3]\n",
    "pool = [2,2,2,2]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    # 399*399*batch_size\n",
    "    conv1 = tf.layers.conv2d(X, filters=filtre[0], kernel_size=noyaux[0],\n",
    " \t \t \t    strides=1, padding='SAME',\n",
    " \t \t \t    activation=tf.nn.relu, name='conv1')\n",
    "    maxPool1 = tf.layers.max_pooling2d(conv1, pool_size=(pool[0],pool[0]), strides=(pool[0],pool[0]), padding='same')\n",
    "    print(maxPool1.get_shape())\n",
    "    # 199*199*batch_size\n",
    "    conv2 = tf.layers.conv2d(maxPool1, filters=filtre[1], kernel_size=noyaux[1],\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv2')\n",
    "    maxPool2 = tf.layers.max_pooling2d(conv2, pool_size=(pool[1],pool[1]), strides=(pool[1],pool[1]), padding='same')\n",
    "    print(maxPool2.get_shape())\n",
    "    # 99*99*batch_size\n",
    "    conv3 = tf.layers.conv2d(maxPool2, filters=filtre[2], kernel_size=noyaux[2],\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv3')\n",
    "    maxPool3 = tf.layers.max_pooling2d(conv3, pool_size=(pool[2],pool[2]), strides=(pool[2],pool[2]), padding='same')\n",
    "    print(maxPool3.get_shape())\n",
    "    # 49*49*batch_size\n",
    "    conv4 = tf.layers.conv2d(maxPool3, filters=filtre[3], kernel_size=noyaux[3],\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv4')\n",
    "    maxPool4 = tf.layers.max_pooling2d(conv4, pool_size=(pool[3],pool[3]), strides=(pool[3],pool[3]), padding='same')\n",
    "    print(maxPool4.get_shape())\n",
    "    # 24*24*batch_size\n",
    "    encoded = tf.layers.conv2d(maxPool4, filters=filtre[4], kernel_size=noyaux[4],\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv5')\n",
    "    \n",
    "    upsample1 = tf.image.resize_images(encoded, size=(49,49), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print(upsample1.get_shape())\n",
    "    # Now 49*49xbatch_size\n",
    "    conv5 = tf.layers.conv2d(inputs=upsample1, filters=3, kernel_size=noyauxDecode[0], padding='same', activation=tf.nn.relu)\n",
    "    print(conv5.get_shape())\n",
    "    # Now 7x7xbatch_size\n",
    "    upsample2 = tf.image.resize_images(conv4, size=(199,199), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print(upsample2.get_shape())\n",
    "    # Now 199x199xbatch_size\n",
    "    conv6 = tf.layers.conv2d(inputs=upsample2, filters=3, kernel_size=noyauxDecode[1], padding='same', activation=tf.nn.relu)\n",
    "    print(conv6.get_shape())\n",
    "    # Now 199x199xbatch_size\n",
    "    upsample3 = tf.image.resize_images(conv5, size=(399,399), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print(upsample3.get_shape())\n",
    "    # Now 399x399xbatch_size\n",
    "    output = tf.layers.conv2d(inputs=upsample3, filters=3, kernel_size=noyauxDecode[2], padding='same', activation=tf.nn.relu)\n",
    "    print(output.get_shape())\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.square(output - X))\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "        loss_saver = tf.summary.scalar(\"Loss\",loss)\n",
    "        summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "    with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "        \n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op, feed_dict={X: y_batch, y: X_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver.eval(feed_dict={X: y_batch, y: X_batch})\n",
    "                  summary_writer.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train))\n",
    "        summary_writer.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      \n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essai 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Taille | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Conv0 | - | 3 | 20 | 1 | Relu | \\ |\n",
    "| Pool2 | 199 | - | - | 2 | Relu |-|\n",
    "| Conv1 | - | 7 | 15 | 1 | Relu | \\ |\n",
    "| Pool1 | 99 | - | - | 2 | Relu |-|\n",
    "| Conv2 | - | 11 | 11 | 1 | Relu | \\ |\n",
    "| Pool2 | 49 | - | - | 2 | Relu |-|\n",
    "| Conv3 | - | 15 | 7 | 1 | Relu | \\ |\n",
    "| Pool3 | 24 | - | - | 2 | Relu |-|\n",
    "| Conv4 | - | 20 | 3 | 1 | Relu | \\ |\n",
    "| Dense0 | 24 | - | - | - | Relu | 500 |\n",
    "| Dense1 | 24 | - | - | - | Relu | 200 |\n",
    "| Dense2 | 24 | - | - | - | Relu | 100 |\n",
    "| Resize5 | 99 | - | - | - | Relu | - |\n",
    "| Conv5 | - | 3 | 20 | 1 | Relu | \\ |\n",
    "| Resize6 | 174 | - | - | - | Relu | - |\n",
    "| Conv6 | - | 7 | 15 | 1 | Relu | \\ |\n",
    "| Resize7 | 249 | - | - | - | Relu | - |\n",
    "| Conv7 | - | 11 | 11 | 1 | Relu | \\ |\n",
    "| Resize8 | 324 | - | - | - | Relu | - |\n",
    "| Conv8 | - | 15 | 7 | 1 | Relu | \\ |\n",
    "| Resize9 | 399 | - | - | - | Relu | - |\n",
    "| Conv9 | - | 3 | 3 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "nom = 'Essai8'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}{}/\".format(root_logdir,nom,now)\n",
    "\n",
    "noyaux = [3,25,15,9,6,3]\n",
    "filtre = [7, 9, 11, 15,19,7]\n",
    "noyauxDecode = [5,10,14]\n",
    "# filtreDecode = [3, 5, 7, 11,15,3]\n",
    "pool = [2,2,2,2]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    # 399*399*batch_size\n",
    "    conv0 = tf.layers.conv2d(X, filters=3, kernel_size=20,\n",
    " \t \t \t strides=1, padding='SAME',\n",
    " \t \t \t activation=tf.nn.relu, name='conv0')\n",
    "    print('Shape of conv0 : '+str(conv0.get_shape()))\n",
    "    maxPool0 = tf.layers.max_pooling2d(conv0, pool_size=(199,199), strides=(199,199), padding='same')\n",
    "    print('Shape of maxPool0 : '+str(maxPool0.get_shape()))\n",
    "\n",
    "    conv1 = tf.layers.conv2d(maxPool0, filters=7, kernel_size=15,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv1')\n",
    "    print('Shape of conv1 : '+str(conv1.get_shape()))\n",
    "    maxPool1 = tf.layers.max_pooling2d(conv1, pool_size=(99,99), strides=(99,99), padding='same')\n",
    "    print('Shape of maxPool1 : '+str(maxPool1.get_shape()))\n",
    "\n",
    "    conv2 = tf.layers.conv2d(maxPool1, filters=11, kernel_size=11,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv2')\n",
    "    print('Shape of conv2 : '+str(conv2.get_shape()))\n",
    "    maxPool2 = tf.layers.max_pooling2d(conv2, pool_size=(49,49), strides=(49,49), padding='same')\n",
    "    print('Shape of maxPool2 : '+str(maxPool2.get_shape()))\n",
    "\n",
    "    conv3 = tf.layers.conv2d(maxPool2, filters=15, kernel_size=7,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv3')\n",
    "    print('Shape of conv3 : '+str(conv3.get_shape()))\n",
    "    maxPool3 = tf.layers.max_pooling2d(conv3, pool_size=(24,24), strides=(24,24), padding='same')\n",
    "    print('Shape of maxPool3 : '+str(maxPool3.get_shape()))\n",
    "\n",
    "    conv4 = tf.layers.conv2d(maxPool3, filters=20, kernel_size=3,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv4')\n",
    "    print('Shape of conv4 : '+str(conv4.get_shape()))\n",
    "    dense0 = tf.layers.dense(conv4,500)\n",
    "    print('Shape of dense0 : '+str(dense0.get_shape()))\n",
    "    dense1 = tf.layers.dense(dense0,200)\n",
    "    print('Shape of dense1 : '+str(dense1.get_shape()))\n",
    "    dense2 = tf.layers.dense(dense1,100)\n",
    "    print('Shape of dense2 : '+str(dense2.get_shape()))\n",
    "\n",
    "    resized5 = tf.image.resize_images(dense2, size=(99,99), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print('Shape of resized5 : '+str(resized5.get_shape()))\n",
    "    conv5 = tf.layers.conv2d(resized5, filters=3, kernel_size=20,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv5')\n",
    "    print('Shape of conv5 : '+str(conv5.get_shape()))\n",
    "\n",
    "    resized6 = tf.image.resize_images(conv5, size=(174,174), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print('Shape of resized6 : '+str(resized6.get_shape()))\n",
    "    conv6 = tf.layers.conv2d(resized6, filters=7, kernel_size=15,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv6')\n",
    "    print('Shape of conv6 : '+str(conv6.get_shape()))\n",
    "\n",
    "    resized7 = tf.image.resize_images(conv6, size=(249,249), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print('Shape of resized7 : '+str(resized7.get_shape()))\n",
    "    conv7 = tf.layers.conv2d(resized7, filters=11, kernel_size=11,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv7')\n",
    "    print('Shape of conv7 : '+str(conv7.get_shape()))\n",
    "\n",
    "    resized8 = tf.image.resize_images(conv7, size=(324,324), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print('Shape of resized8 : '+str(resized8.get_shape()))\n",
    "    conv8 = tf.layers.conv2d(resized8, filters=15, kernel_size=7,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv8')\n",
    "    print('Shape of conv8 : '+str(conv8.get_shape()))\n",
    "\n",
    "    resized9 = tf.image.resize_images(conv8, size=(399,399), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print('Shape of resized9 : '+str(resized9.get_shape()))\n",
    "    output = tf.layers.conv2d(resized9, filters=3, kernel_size=3,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='output')\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.square(output - X))\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "        loss_saver = tf.summary.scalar(\"Loss\",loss)\n",
    "        summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "    with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "        \n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op, feed_dict={X: y_batch, y: X_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver.eval(feed_dict={X: y_batch, y: X_batch})\n",
    "                  summary_writer.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train))\n",
    "        summary_writer.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      \n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essai 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Taille | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Dense0 | - | - | - | - | Relu | 800 |\n",
    "| Dense1 | - | - | - | - | Relu | 700 |\n",
    "| Dense2 | - | - | - | - | Relu | 500 |\n",
    "| Dense3 | - | - | - | - | Relu | 200 |\n",
    "| Dense4 | - | - | - | - | Relu | 500 |\n",
    "| Dense5 | - | - | - | - | Relu | 700 |\n",
    "| Dense6 | - | - | - | - | Relu | 800 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essai 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Taille | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Dense0 | - | - | - | - | Relu | 800 |\n",
    "| Dense1 | - | - | - | - | Relu | 700 |\n",
    "| Dense2 | - | - | - | - | Relu | 500 |\n",
    "| Dense3 | - | - | - | - | Relu | 200 |\n",
    "| Dense4 | - | - | - | - | Relu | 500 |\n",
    "| Dense5 | - | - | - | - | Relu | 700 |\n",
    "| Dense6 | - | - | - | - | Relu | 800 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "nom = 'Essai10'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}{}/\".format(root_logdir,nom,now)\n",
    "\n",
    "noyaux = [3,25,15,9,6,3]\n",
    "filtre = [7, 9, 11, 15,19,7]\n",
    "noyauxDecode = [5,10,14]\n",
    "# filtreDecode = [3, 5, 7, 11,15,3]\n",
    "pool = [2,2,2,2]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    # 399*399*batch_size\n",
    "    dense0 = tf.layers.dense(y,3)\n",
    "    print('Shape of dense0 : '+str(dense0.get_shape()))\n",
    "    dense1 = tf.layers.dense(dense0,3)\n",
    "    print('Shape of dense1 : '+str(dense1.get_shape()))\n",
    "    dense2 = tf.layers.dense(dense1,3)\n",
    "    print('Shape of dense2 : '+str(dense2.get_shape()))\n",
    "    dense3 = tf.layers.dense(dense2,3)\n",
    "    print('Shape of dense3 : '+str(dense3.get_shape()))\n",
    "    dense4 = tf.layers.dense(dense3,3)\n",
    "    print('Shape of dense4 : '+str(dense4.get_shape()))\n",
    "    dense5 = tf.layers.dense(dense4,3)\n",
    "    print('Shape of dense5 : '+str(dense5.get_shape()))\n",
    "    output = tf.layers.dense(dense5,3)\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.square(output - X))\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "        loss_saver = tf.summary.scalar(\"Loss\",loss)\n",
    "        summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "    with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "        \n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op, feed_dict={X: y_batch, y: X_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver.eval(feed_dict={X: y_batch, y: X_batch})\n",
    "                  summary_writer.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train))\n",
    "        summary_writer.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      \n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essai 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Taille | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Dense0 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense1 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense2 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense3 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense4 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense5 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense6 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense7 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense8 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense9 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense10 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense11 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense12 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense13 | 399 | - | - | - | Relu | 3 |\n",
    "| Dense14 | 399 | - | - | - | Relu | 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "nom = 'Essai11'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}{}/\".format(root_logdir,nom,now)\n",
    "\n",
    "noyaux = [3,25,15,9,6,3]\n",
    "filtre = [7, 9, 11, 15,19,7]\n",
    "noyauxDecode = [5,10,14]\n",
    "# filtreDecode = [3, 5, 7, 11,15,3]\n",
    "pool = [2,2,2,2]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    # 399*399*batch_size\n",
    "    dense0 = tf.layers.dense(X,3)\n",
    "    print('Shape of dense0 : '+str(dense0.get_shape()))\n",
    "    dense1 = tf.layers.dense(dense0,3)\n",
    "    print('Shape of dense1 : '+str(dense1.get_shape()))\n",
    "    dense2 = tf.layers.dense(dense1,3)\n",
    "    print('Shape of dense2 : '+str(dense2.get_shape()))\n",
    "    dense3 = tf.layers.dense(dense2,3)\n",
    "    print('Shape of dense3 : '+str(dense3.get_shape()))\n",
    "    dense4 = tf.layers.dense(dense3,3)\n",
    "    print('Shape of dense4 : '+str(dense4.get_shape()))\n",
    "    dense5 = tf.layers.dense(dense4,3)\n",
    "    print('Shape of dense5 : '+str(dense5.get_shape()))\n",
    "    dense6 = tf.layers.dense(dense5,3)\n",
    "    print('Shape of dense6 : '+str(dense6.get_shape()))\n",
    "    dense7 = tf.layers.dense(dense6,3)\n",
    "    print('Shape of dense7 : '+str(dense7.get_shape()))\n",
    "    dense8 = tf.layers.dense(dense7,3)\n",
    "    print('Shape of dense8 : '+str(dense8.get_shape()))\n",
    "    dense9 = tf.layers.dense(dense8,3)\n",
    "    print('Shape of dense9 : '+str(dense9.get_shape()))\n",
    "    dense10 = tf.layers.dense(dense9,3)\n",
    "    print('Shape of dense10 : '+str(dense10.get_shape()))\n",
    "    dense11 = tf.layers.dense(dense10,3)\n",
    "    print('Shape of dense11 : '+str(dense11.get_shape()))\n",
    "    dense12 = tf.layers.dense(dense11,3)\n",
    "    print('Shape of dense12 : '+str(dense12.get_shape()))\n",
    "    dense13 = tf.layers.dense(dense12,3)\n",
    "    print('Shape of dense13 : '+str(dense13.get_shape()))\n",
    "    output = tf.layers.dense(dense13,3)\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.square(output - X))\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "        loss_saver = tf.summary.scalar(\"Loss\",loss)\n",
    "        print(os.path.split(X.name))\n",
    "#         weights = tf.get_default_graph().get_tensor_by_name(os.path.split(X.name)[1])\n",
    "#         tf.summary.histogram('weightsX', weights)\n",
    "        summary_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "    with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "        \n",
    "\n",
    "    n_epochs = 300\n",
    "    n_batches_per_epoch = 3\n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op, feed_dict={X: y_batch, y: X_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver.eval(feed_dict={X: y_batch, y: X_batch})\n",
    "                  summary_writer.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train))\n",
    "        summary_writer.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      \n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essai 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Type de couche | Taille | Nb de filtres | Taille du noyau | Pas | Fonction d'activation | Nb de neurones |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Conv0 | - | 3 | 20 | 1 | Relu | \\ |\n",
    "| Conv1 | - | 3 | 15 | 1 | Relu | \\ |\n",
    "| Conv2 | - | 3 | 11 | 1 | Relu | \\ |\n",
    "| Conv3 | - | 3 | 7 | 1 | Relu | \\ |\n",
    "| Conv4 | - | 3 | 3 | 1 | Relu | \\ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recopie pour apprentissage par couche avec convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for epoch in range(n_epochs):\n",
      "            print(\"epoch : \",epoch)\n",
      "            for iteration in range(n_batches_per_epoch):\n",
      "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
      "                sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
      "                if iteration % 5 == 0:\n",
      "                  summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                  summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
      "            loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
      "for epoch in range(n_epochs):\n",
      "            print(\"epoch : \",epoch)\n",
      "            for iteration in range(n_batches_per_epoch):\n",
      "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
      "                sess.run(training_op1, feed_dict={X: X_batch, y: y_batch})\n",
      "                if iteration % 5 == 0:\n",
      "                  summary_str = loss_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                  summary_writer1.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
      "            loss_train1 = loss1.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train1))\n",
      "for epoch in range(n_epochs):\n",
      "            print(\"epoch : \",epoch)\n",
      "            for iteration in range(n_batches_per_epoch):\n",
      "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
      "                sess.run(training_op2, feed_dict={X: X_batch, y: y_batch})\n",
      "                if iteration % 5 == 0:\n",
      "                  summary_str = loss_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                  summary_writer2.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
      "            loss_train2 = loss2.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train2))\n",
      "for epoch in range(n_epochs):\n",
      "            print(\"epoch : \",epoch)\n",
      "            for iteration in range(n_batches_per_epoch):\n",
      "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
      "                sess.run(training_op3, feed_dict={X: X_batch, y: y_batch})\n",
      "                if iteration % 5 == 0:\n",
      "                  summary_str = loss_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                  summary_writer3.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
      "            loss_train3 = loss3.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train3))\n",
      "for epoch in range(n_epochs):\n",
      "            print(\"epoch : \",epoch)\n",
      "            for iteration in range(n_batches_per_epoch):\n",
      "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
      "                sess.run(training_op4, feed_dict={X: X_batch, y: y_batch})\n",
      "                if iteration % 5 == 0:\n",
      "                  summary_str = loss_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "                  summary_writer4.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
      "            loss_train4 = loss4.eval(feed_dict={X: X_batch, y: y_batch})\n",
      "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train4))\n"
     ]
    }
   ],
   "source": [
    "repetition = [\"\"\"for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op\"\"\"+str(i)+\"\"\", feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver\"\"\"+str(i)+\"\"\".eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer\"\"\"+str(i)+\"\"\".add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train\"\"\"+str(i)+\"\"\" = loss\"\"\"+str(i)+\"\"\".eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train\"\"\"+str(i)+\"\"\"))\"\"\" for i in range(5)]\n",
    "for element in repetition:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "nom = 'Essai12'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}{}/\".format(root_logdir,nom,now)\n",
    "taining0 = True\n",
    "taining1 = True\n",
    "taining2 = True\n",
    "taining3 = True\n",
    "taining4 = True\n",
    "    \n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "    print(X.get_shape())\n",
    "    \n",
    "    \n",
    "    conv0 = tf.layers.conv2d(X, filters=3, kernel_size=20,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv0', trainable=taining0)\n",
    "    print('Shape of conv0 : '+str(conv0.get_shape()))\n",
    "    conv1 = tf.layers.conv2d(conv0, filters=3, kernel_size=15,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv1', trainable=taining1)\n",
    "    print('Shape of conv1 : '+str(conv1.get_shape()))\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=3, kernel_size=11,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv2', trainable=taining2)\n",
    "    print('Shape of conv2 : '+str(conv2.get_shape()))\n",
    "    conv3 = tf.layers.conv2d(conv2, filters=3, kernel_size=7,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='conv3', trainable=taining3)\n",
    "    print('Shape of conv3 : '+str(conv3.get_shape()))\n",
    "    output = tf.layers.conv2d(conv3, filters=3, kernel_size=3,\n",
    "           strides=1, padding='SAME',\n",
    "           activation=tf.nn.relu, name='output', trainable=taining4)\n",
    "    print('Shape of output : '+str(output.get_shape()))\n",
    "        \n",
    "      \n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "    with tf.name_scope(\"phase0\"):\n",
    "        loss0 = tf.reduce_mean(tf.square(conv0 - y))\n",
    "        training_op0 = optimizer.minimize(loss0)\n",
    "    with tf.name_scope(\"phase1\"):\n",
    "        loss1 = tf.reduce_mean(tf.square(conv1 - y))\n",
    "        training_op1 = optimizer.minimize(loss1)\n",
    "    with tf.name_scope(\"phase2\"):\n",
    "        loss2 = tf.reduce_mean(tf.square(conv2 - y))\n",
    "        training_op2 = optimizer.minimize(loss2)\n",
    "    with tf.name_scope(\"phase3\"):\n",
    "        loss3 = tf.reduce_mean(tf.square(conv3 - y))\n",
    "        training_op3 = optimizer.minimize(loss3)\n",
    "    with tf.name_scope(\"phase4\"):\n",
    "        loss4 = tf.reduce_mean(tf.square(output - y))\n",
    "        training_op4 = optimizer.minimize(loss4)\n",
    "    \n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"enregistrement\"):\n",
    "        loss_saver0 = tf.summary.scalar(\"Loss0\",loss0)\n",
    "        loss_saver1 = tf.summary.scalar(\"Loss1\",loss1)\n",
    "        loss_saver2 = tf.summary.scalar(\"Loss2\",loss2)\n",
    "        loss_saver3 = tf.summary.scalar(\"Loss3\",loss3)\n",
    "        loss_saver4 = tf.summary.scalar(\"Loss4\",loss4)\n",
    "        kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
    "        kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
    "        kernel2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/kernel')[0]\n",
    "        kernel3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/kernel')[0]\n",
    "        kernel4 = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/kernel')[0]\n",
    "        trainable = tf.trainable_variables()\n",
    "        print(trainable[:2])\n",
    "        \n",
    "        bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
    "        bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "        bias2 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv2/bias')[0]\n",
    "        bias3 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv3/bias')[0]\n",
    "        bias4 = tf.get_collection(tf.GraphKeys.VARIABLES, 'output/bias')[0]\n",
    "        print(os.path.split(X.name))\n",
    "        summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        summary_writer1 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        summary_writer2 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        summary_writer3 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        summary_writer4 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    with tf.name_scope('Avant-Apres_'+nom):\n",
    "        orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "        fin = tf.summary.image('output',tf.cast(output,dtype=tf.uint8))\n",
    "        \n",
    "\n",
    "    n_epochs = 100\n",
    "    n_batches_per_epoch = 3\n",
    "    Lloss = []\n",
    "    def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "        global Lloss\n",
    "        if len(Lloss) < duree:\n",
    "            Lloss.append(lastLoss)\n",
    "            return False\n",
    "        else:\n",
    "            Lloss = Lloss[1:duree] + [lastLoss]\n",
    "            L = Lloss[:]\n",
    "            L.sort(reverse=True)\n",
    "            if L[0] <= seuil:\n",
    "              if decroissance == True:\n",
    "                return True if L == Lloss else False\n",
    "              else:\n",
    "                return True\n",
    "            return False\n",
    "          \n",
    "    import os\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        # Training 0\n",
    "#         training0 = True\n",
    "#         training1 = training2 = training3 = training4 = False\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "            if arret(loss_train0,800,6) == True:\n",
    "              break\n",
    "        # Training 1\n",
    "        training1 = True\n",
    "        training0 = training2 = training3 = training4 = False\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op1, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer1.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train1 = loss1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train1))\n",
    "            if arret(loss_train1,700,4) == True:\n",
    "              break\n",
    "        # Training 2\n",
    "        training2 = True\n",
    "        training1 = training0 = training3 = training4 = False\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op2, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer2.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train2 = loss2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train2))\n",
    "            if arret(loss_train2,500,4) == True:\n",
    "              break\n",
    "        # Training 3\n",
    "        training3 = True\n",
    "        training1 = training2 = training0 = training4 = False\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op3, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer3.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train3 = loss3.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train3))\n",
    "            if arret(loss_train3,500,4) == True:\n",
    "              break\n",
    "        # Training 4\n",
    "        training4 = True\n",
    "        training1 = training2 = training3 = training0 = False\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"epoch : \",epoch)\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                sess.run(training_op4, feed_dict={X: X_batch, y: y_batch})\n",
    "                if iteration % 5 == 0:\n",
    "                  summary_str = loss_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                  summary_writer4.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "            loss_train4 = loss4.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train4))\n",
    "            if arret(loss_train4,200,4) == True:\n",
    "              break\n",
    "        summary_writer4.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        summary_writer4.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "      \n",
    "summary_writer0.close()\n",
    "summary_writer1.close()\n",
    "summary_writer2.close()\n",
    "summary_writer2.close()\n",
    "summary_writer3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essais C et Cdeux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "height = 399\n",
    "width = 399\n",
    "channels = 3\n",
    "\n",
    "batch_size = 7\n",
    "i = 0\n",
    "a,b = 2,2\n",
    "while a<=8:\n",
    "  b = 2\n",
    "  while b <= 8:\n",
    "    print(\"Essai pour conv0 avec \" + str(a) + \" et conv1 avec \" + str(b))\n",
    "    nom = 'EssaiCdeux'+str(i)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    logdir = \"{}/run-{}{}/\".format(root_logdir,nom,now)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
    "        y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
    "        print(X.get_shape())\n",
    "\n",
    "\n",
    "        conv0 = tf.layers.conv2d(X, filters=3, kernel_size=(a,a),\n",
    "               strides=1, padding='SAME',\n",
    "               activation=tf.nn.relu, name='conv0')\n",
    "        print('Shape of conv0 : '+str(conv0.get_shape()))\n",
    "        conv1 = tf.layers.conv2d(X, filters=3, kernel_size=(b,b),\n",
    "               strides=1, padding='SAME',\n",
    "               activation=tf.nn.relu, name='conv1')\n",
    "        print('Shape of conv1 : '+str(conv1.get_shape()))\n",
    "\n",
    "\n",
    "        with tf.name_scope('Optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "        with tf.name_scope(\"phase0\"):\n",
    "            loss0 = tf.reduce_mean(tf.square(conv1 - y))\n",
    "            training_op0 = optimizer.minimize(loss0)\n",
    "\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"enregistrement\"):\n",
    "            loss_saver0 = tf.summary.scalar(\"Loss\",loss0)\n",
    "            kernel0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/kernel')[0]\n",
    "            bias0 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv0/bias')[0]\n",
    "            kernel1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\n",
    "            bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "            kernel_saver0 = tf.summary.histogram(\"Kernel\",kernel0)\n",
    "            bias_saver0 = tf.summary.histogram(\"Bias\",bias0)\n",
    "            kernel_saver1 = tf.summary.histogram(\"Kernel\",kernel1)\n",
    "            bias_saver1 = tf.summary.histogram(\"Bias\",bias1)\n",
    "            print(os.path.split(X.name))\n",
    "            summary_writer0 = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        with tf.name_scope('Avant-Apres_'+nom):\n",
    "            orig = tf.summary.image('input',tf.cast(X,dtype=tf.uint8))\n",
    "            fin = tf.summary.image('output',tf.cast(conv1,dtype=tf.uint8))\n",
    "\n",
    "\n",
    "        n_epochs = 100\n",
    "        n_batches_per_epoch = 3\n",
    "        Lloss = []\n",
    "        def arret(lastLoss, seuil, duree, decroissance = False):\n",
    "            global Lloss\n",
    "            if len(Lloss) < duree:\n",
    "                Lloss.append(lastLoss)\n",
    "                return False\n",
    "            else:\n",
    "                Lloss = Lloss[1:duree] + [lastLoss]\n",
    "                L = Lloss[:]\n",
    "                L.sort(reverse=True)\n",
    "                if L[0] <= seuil:\n",
    "                  if decroissance == True:\n",
    "                    return True if L == Lloss else False\n",
    "                  else:\n",
    "                    return True\n",
    "                return False\n",
    "\n",
    "        import os\n",
    "        with tf.Session() as sess:\n",
    "            init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                print(\"epoch : \",epoch)\n",
    "                for iteration in range(n_batches_per_epoch):\n",
    "                    X_batch, y_batch = next_batch(batch_size, train_dataset_item,train_dataset_label)\n",
    "                    sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
    "                    if iteration % 5 == 0:\n",
    "                      summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                      summary_strKernel = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                      summary_strBias = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                      summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                      summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                      summary_writer0.add_summary(summary_str,epoch*n_batches_per_epoch+iteration)\n",
    "                      summary_writer0.add_summary(summary_strKernel,epoch*n_batches_per_epoch+iteration)\n",
    "                      summary_writer0.add_summary(summary_strBias,epoch*n_batches_per_epoch+iteration)\n",
    "                      summary_writer0.add_summary(summary_strKernel1,epoch*n_batches_per_epoch+iteration)\n",
    "                      summary_writer0.add_summary(summary_strBias1,epoch*n_batches_per_epoch+iteration)\n",
    "\n",
    "                loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
    "                if arret(loss_train0,100,6) == True:\n",
    "                  break\n",
    "            summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "            summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "            saver.save(sess, 'model/model'+nom+'.ckpt')\n",
    "    summary_writer0.close()\n",
    "    b += 2\n",
    "    i += 1\n",
    "  a += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec une couche, il y a apprentissage au début avec les couches de plus petit noyau. </br>\n",
    "Avec 2 couches, de même. </br>\n",
    "Globalement à la fin le système reste imprécis."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
