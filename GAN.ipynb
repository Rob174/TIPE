{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rob174/TIPE/blob/master/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "gjbzLPokPfNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VR_MLSTmPtFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "2b302911-236f-4d3b-bcf7-9ba612d380ac"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a2h-mQi7Pyfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5633212-9ab1-429a-c7d0-5ea6243c1cd9"
      },
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/TIPE'\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TIPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KAK6WTwgP2rW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "55dc9246-a03b-4ef2-b734-1e1ff6fbb4d6"
      },
      "cell_type": "code",
      "source": [
        "print(\"Vérification GPU\")\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "print()\n",
        "print()\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy\n",
        "from PIL import Image\n",
        "\n",
        "# Ajoute images\n",
        "entrainement = 60/100\n",
        "test = 20/100\n",
        "# validation = le reste : 20%\n",
        "tailleImage = 399\n",
        "\n",
        "images = []\n",
        "noises = []\n",
        "\n",
        "i = 1\n",
        "while os.path.isfile(\"Images_source/clean/\"+str(i)+\".jpg\"):\n",
        "    images.append(\"Images_source/clean/\"+str(i)+\".jpg\")\n",
        "    i += 1\n",
        "\n",
        "i = 1\n",
        "while os.path.isfile(\"Images_source/noise/\"+str(i)+\".jpg\"):\n",
        "    noises.append(\"Images_source/noise/\"+str(i)+\".jpg\")\n",
        "    i += 1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vérification GPU\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 708898455517088231\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 5862816696497971332\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ty58ld9kP7jr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "def next_batch(batch_size, noises, images,tailleAttendue):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    print(\"Size of {}\".format(len(noises)))\n",
        "    noiseTensor = []\n",
        "    imageTensor = []\n",
        "#     import matplotlib.pyplot as plt\n",
        "    for i in range(batch_size):\n",
        "      choix = np.random.randint(0,len(images))#choix aléatoire de l'image\n",
        "      image = cv2.imread(images[choix],1)#Ouvre en rgb l'image nettoyée\n",
        "      noise = cv2.imread(noises[choix],1)#Ouvre en rgb l'image bruitée\n",
        "      angle = np.random.randint(0,90)\n",
        "      taille = np.random.randint(int(tailleAttendue*2**0.5), image.shape[0]+1)# Racine de deux pour pouvoir toujours récupérer la taille désirée\n",
        "\n",
        "      resizedImage = cv2.resize(image,(taille,taille))\n",
        "      resizedNoise = cv2.resize(noise,(taille,taille))\n",
        "      \n",
        "      rows,cols = resizedImage.shape[:2]\n",
        "#       print('CentreRow : ')\n",
        "#       print((taille//2,rows-taille//2))\n",
        "#       print('CentreCols : ')\n",
        "#       print((taille//2,cols-taille//2))\n",
        "#       tmp = taille//2\n",
        "      \n",
        "      centre = (np.random.randint(taille//2,rows-taille//2) if taille//2<rows-taille//2 else taille//2,np.random.randint(taille//2,cols-taille//2) if taille//2<cols-taille//2 else taille//2)\n",
        "      M = cv2.getRotationMatrix2D(centre,angle,1)\n",
        "\n",
        "      rotatedImage = cv2.warpAffine(resizedImage,M,(cols,rows))\n",
        "      rotatedNoise = cv2.warpAffine(resizedNoise,M,(cols,rows))\n",
        "#       print('Rotation : '+str(rotatedImage.shape))\n",
        "\n",
        "      resultImage = rotatedImage[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
        "      resultNoise = rotatedNoise[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
        "#       print('Crop : '+str(resultImage.shape))\n",
        "#       plt.imshow(resultImage)\n",
        "\n",
        "      noiseTensor.append(np.array(resultNoise,np.uint8))\n",
        "      imageTensor.append(np.array(resultImage,np.uint8))\n",
        "#       print(imageTensor)\n",
        "#     plt.show()\n",
        "    return np.array(noiseTensor), np.array(imageTensor)\n",
        "# next_batch(3,noises,images,200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B_tBbBzTTYJi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "height = 399\n",
        "width = 399\n",
        "channels = 3\n",
        "\n",
        "batch_size = 7\n",
        "i = 5\n",
        "print(\"Essai avec \"+str(i)+\" couches\")\n",
        "nom = 'EssaiLotRemaniePlusNeurones'+str(i)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "root_logdir = \"tf_logs\"\n",
        "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
        "with tf.Graph().as_default():\n",
        "    X = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"X\")\n",
        "    y = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"y\")\n",
        "    print(X.get_shape())\n",
        "    def denseLayer(input,nbNeurones,nom):\n",
        "      dense = tf.layers.dense(input,nbNeurones,name=nom)\n",
        "      print('Taille de '+nom+' : '+str(dense.get_shape()))\n",
        "      return dense,tf.get_collection(tf.GraphKeys.VARIABLES, nom+'/kernel')[0],tf.get_collection(tf.GraphKeys.VARIABLES, nom+'/bias')[0]\n",
        "    def generator(X,nbCouche,neurones):\n",
        "        kernels = []\n",
        "        bias = []\n",
        "        i = 0\n",
        "        couche,kernel,bias = denseLayer(X,3,'denseGenerator'+str(i))\n",
        "        dense0 = tf.layers.dense(x,3,name='denseGenerator0')\n",
        "        print('Shape of output : '+str(output.get_shape()))\n",
        "        return output\n",
        "\n",
        "    def discriminator(x):\n",
        "        dense0 = tf.layers.dense(x,3,name='denseDiscriminator0')\n",
        "        print('Shape of dense0 : '+str(dense0.get_shape()))\n",
        "        dense1 = tf.layers.dense(dense0,3,name='denseDiscriminator1')\n",
        "        print('Shape of dense1 : '+str(dense1.get_shape()))\n",
        "        dense2 = tf.layers.dense(dense1,100,name='denseDiscriminator2')\n",
        "        print('Shape of dense2 : '+str(dense2.get_shape()))\n",
        "        dense3 = tf.layers.dense(dense2,100,name='denseDiscriminator3')\n",
        "        print('Shape of dense3 : '+str(dense3.get_shape()))\n",
        "        output = tf.layers.dense(dense3,3,name='denseDiscriminator4')\n",
        "        print('Shape of output : '+str(output.get_shape()))\n",
        "        return out_layer\n",
        "\n",
        "    generator = generator(X)\n",
        "    disc_vrai = discriminator()\n",
        "    disc_faux = discriminator()\n",
        "    \n",
        "    gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
        "    disc_loss = -tf.reduce_mean(tf.log(disc_vrai) + tf.log(1. - disc_faux))\n",
        "    \n",
        "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    \n",
        "    weights_gen = []\n",
        "    bias_gen = []\n",
        "    weights_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator0/kernel')[0])\n",
        "    bias_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator0/bias')[0])\n",
        "    weights_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator1/kernel')[0])\n",
        "    bias_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator1/bias')[0])\n",
        "    weights_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator2/kernel')[0])\n",
        "    bias_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator2/bias')[0])\n",
        "    weights_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator3/kernel')[0])\n",
        "    bias_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator3/bias')[0])\n",
        "    weights_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator4/kernel')[0])\n",
        "    bias_gen.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseGenerator4/bias')[0])\n",
        "    \n",
        "    weights_disc = []\n",
        "    bias_disc = []\n",
        "    weights_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator0/kernel')[0])\n",
        "    bias_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator0/bias')[0])\n",
        "    weights_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator1/kernel')[0])\n",
        "    bias_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator1/bias')[0])\n",
        "    weights_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator2/kernel')[0])\n",
        "    bias_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator2/bias')[0])\n",
        "    weights_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator3/kernel')[0])\n",
        "    bias_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator3/bias')[0])\n",
        "    weights_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator4/kernel')[0])\n",
        "    bias_disc.append(tf.get_collection(tf.GraphKeys.VARIABLES, 'denseDiscriminator4/bias')[0])\n",
        "    \n",
        "    gen_vars = [weights_gen,bias_gen]\n",
        "    disc_vars = [weights_disc,bias_disc]\n",
        "    \n",
        "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
        "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    n_batches_per_epoch = 3\n",
        "    Lloss = []\n",
        "    def arret(lastLoss, seuil, duree, decroissance = False):\n",
        "      global Lloss\n",
        "      if len(Lloss) < duree:\n",
        "          Lloss.append(lastLoss)\n",
        "          return False\n",
        "      else:\n",
        "          Lloss = Lloss[1:duree] + [lastLoss]\n",
        "          L = Lloss[:]\n",
        "          L.sort(reverse=True)\n",
        "          if L[0] <= seuil:\n",
        "            if decroissance == True:\n",
        "              return True if L == Lloss else False\n",
        "            else:\n",
        "              return True\n",
        "          return False\n",
        "\n",
        "    import os\n",
        "    with tf.Session() as sess:\n",
        "        init.run()\n",
        "        for i in range(1,301)\n",
        "          X_batch, y_batch = next_batch(batch_size, noises,images,height)\n",
        "          sess.run(training_op0, feed_dict={X: X_batch, y: y_batch})\n",
        "          if i % 5 == 0:\n",
        "            summary_str = loss_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_writer0.add_summary(summary_str,i)\n",
        "\n",
        "            summary_strKernel0 = kernel_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_strBias0 = bias_saver0.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_writer0.add_summary(summary_strKernel0,i)\n",
        "            summary_writer0.add_summary(summary_strBias0,i)\n",
        "\n",
        "            summary_strKernel1 = kernel_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_strBias1 = bias_saver1.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_writer0.add_summary(summary_strKernel1,i)\n",
        "            summary_writer0.add_summary(summary_strBias1,i)\n",
        "\n",
        "            summary_strKernel2 = kernel_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_strBias2 = bias_saver2.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_writer0.add_summary(summary_strKernel2,i)\n",
        "            summary_writer0.add_summary(summary_strBias2,i)\n",
        "\n",
        "            summary_strKernel3 = kernel_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_strBias3 = bias_saver3.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_writer0.add_summary(summary_strKernel3,i)\n",
        "            summary_writer0.add_summary(summary_strBias3,i)\n",
        "\n",
        "            summary_strKernel4 = kernel_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_strBias4 = bias_saver4.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            summary_writer0.add_summary(summary_strKernel4,i)\n",
        "            summary_writer0.add_summary(summary_strBias4,i)\n",
        "                  \n",
        "      loss_train0 = loss0.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "      print(\"Pour l'époque {} la différence vaut {}\".format(epoch,loss_train0))\n",
        "      if arret(loss_train0,100,6) == True:\n",
        "          break\n",
        "  summary_writer0.add_summary(orig.eval(feed_dict={X: X_batch, y: y_batch}))\n",
        "  summary_writer0.add_summary(fin.eval(feed_dict={X: X_batch, y: y_batch}))\n",
        "  saver.save(sess, 'model/model'+nom+'.ckpt')\n",
        "summary_writer0.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}